[
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Some of the software is open source and covered by licenses of type BSD-3 and reusable by anyone, provided copyright and authorship is respected.\nAt the time of writing (Jan 2023) much of the C++ code core to the stack is not yet open source, and cannot be used by third parties without prior discussions and agreement. In principle, access for non-commercial and in particular educational and research purposes are encouraged and you should contact the person(s) listed in the home page to request access.\nSoftware components that are agnostic to the scientific domain itself are all open sourced: build scripts, python and R packaging, software interoperability between R, Python, C++, etc."
  },
  {
    "objectID": "faq.html#can-i-use-these-hydrologic-simulation-tool-for-my-project",
    "href": "faq.html#can-i-use-these-hydrologic-simulation-tool-for-my-project",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Some of the software is open source and covered by licenses of type BSD-3 and reusable by anyone, provided copyright and authorship is respected.\nAt the time of writing (Jan 2023) much of the C++ code core to the stack is not yet open source, and cannot be used by third parties without prior discussions and agreement. In principle, access for non-commercial and in particular educational and research purposes are encouraged and you should contact the person(s) listed in the home page to request access.\nSoftware components that are agnostic to the scientific domain itself are all open sourced: build scripts, python and R packaging, software interoperability between R, Python, C++, etc."
  },
  {
    "objectID": "faq.html#does-it-run-fast",
    "href": "faq.html#does-it-run-fast",
    "title": "Frequently Asked Questions",
    "section": "Does it run fast?",
    "text": "Does it run fast?\nA short answer is: Yes. The choice of C++ for the core engine was made, notably, to foster computational efficiency. Researchers over the years confirm very satisfactory runtime performances, including when compared to several other hydrologic toolsets.\nA longer answer would still be yes, even if there are nuances depending on the hardware, the size of the model and simulation length.\n\nWall time\nA model set up for the South Esk catchment (Tasmania, Australia) comprises 42 subareas and 42 routing links. One simulation run over 1920 time steps completes in ~0.1 seconds on a single core (Intel Core i7-10875H CPU @ 2.30GHz).\nThis figure can be used as a starting point to estimate what a calibration procedure would take to complete. Let’s say you have a calibration procedure that requires 10,000 iterations of the model. On a single core, this would take around 16 minutes. If you run multithreaded on a machine with 4 cores, the same calibration procedure would take around 4 minutes, but assuming some multithreading overhead (see next section), it may take around 6 minutes in practice.\n\n\nScalability with parallelism\nSWIFT2 is designed to use multithreading where feasible, notably for lengthy model calibrations. Measurements done for the paper SWIFT2: High performance software for short-medium term ensemble streamflow forecasting research and operations show a good capability to scale up with multiple threads.\n\n\n\nScalability of runtime performance with respect to system size, simulation length and degree of parallelism. Each point is the median of 10 replicate test runs to reduce measurement uncertainty."
  },
  {
    "objectID": "doc/installation/install_docker.html",
    "href": "doc/installation/install_docker.html",
    "title": "Installation: Docker",
    "section": "",
    "text": "Placeholder: a docker container with python bindings and Jupyter.",
    "crumbs": [
      "Installation",
      "Installation: via Docker"
    ]
  },
  {
    "objectID": "doc/installation/windows_installer.html",
    "href": "doc/installation/windows_installer.html",
    "title": "Introduction",
    "section": "",
    "text": "2023-04-24\nJean-Michel Perraud, CSIRO Environment"
  },
  {
    "objectID": "doc/installation/windows_installer.html#windows-installer",
    "href": "doc/installation/windows_installer.html#windows-installer",
    "title": "Introduction",
    "section": "Windows installer",
    "text": "Windows installer\nExecute the file sf.msi\n\nDo read the license agreement. If you have access to this installer, you should probably already have had prior discussions with the product distributors to agree on the scope of use of the products.\n\nAs a user-level installation you will be prompted with a conventional default location. You may change the destination folder if required.\n\n\nThe installation will typically take a few seconds to proceed."
  },
  {
    "objectID": "doc/installation/windows_installer.html#installing-r-packages",
    "href": "doc/installation/windows_installer.html#installing-r-packages",
    "title": "Introduction",
    "section": "Installing R packages",
    "text": "Installing R packages\nR packages are available in source and precompiled “win.binaries” form. The latter is easy and fast. Installing from source is out of scope of this document. However, Windows binary packages are version dependent. The installer includes packages for R 4.2.x, the current latest stable version of R. If you need another version of the packages, please contact the authors.\nOpen an R terminal.\nCheck with the command `.libPaths()` that the first folder is user specific, so that you will be able to install the packages. It is likely R would likely prompt you if not, and would created one in the process.\nThen you will need the root name of the small R library folder where R packages are, e.g. \"C:\\Users\\xxxyyy\\AppData\\Local\\Programs\\SF\\R\" if you have chosen the default option for an destination folder.\nAdapting the following command should install the streamflow forecasting packages and their third party dependencies\n`install.packages(c('calibragem', 'mhplot', 'efts', 'swift', 'qpp'), repos=c('file:///C:/Users/xxxyyy/AppData/Local/Programs/SF/R', 'https://cran.csiro.au'), type='win.binary')`\nUpon successful installation you should be able to load the `swift` package:\n\nThe command `?swift` should open an HTML page in a browser. At the bottom of the page you can find a shortcut to the package index.\n\nThe package Index page should also include a link to the introductory vignettes:"
  },
  {
    "objectID": "doc/installation/install_macosx.html",
    "href": "doc/installation/install_macosx.html",
    "title": "Installation: MacOSX",
    "section": "",
    "text": "Placeholder - As of Jan 2023, Homebrew binary packages can be built and installed, but this remains experimental.",
    "crumbs": [
      "Installation",
      "Installation: MacOSX"
    ]
  },
  {
    "objectID": "doc/roadmap.html",
    "href": "doc/roadmap.html",
    "title": "Roadmap for ensemble forecasting tools",
    "section": "",
    "text": "As of Jan 2019 the python package pyswift uses CFFI for interop. uchronia started down the same path but while trying to get things at parity with the R packages, we looked at pybind11 which is a serious candidate for our python/c++/c interop needs. Even more so since we are looking at xtensor for multidimensional data handling and xtensor-python also uses pybind11. pybind11 could play a role very similar to Rcpp and indeed is a close conceptual equivalent.\n\nq1 2019 Set up a Testbed with a uchronia_p11 package"
  },
  {
    "objectID": "doc/roadmap.html#software-architecture",
    "href": "doc/roadmap.html#software-architecture",
    "title": "Roadmap for ensemble forecasting tools",
    "section": "",
    "text": "As of Jan 2019 the python package pyswift uses CFFI for interop. uchronia started down the same path but while trying to get things at parity with the R packages, we looked at pybind11 which is a serious candidate for our python/c++/c interop needs. Even more so since we are looking at xtensor for multidimensional data handling and xtensor-python also uses pybind11. pybind11 could play a role very similar to Rcpp and indeed is a close conceptual equivalent.\n\nq1 2019 Set up a Testbed with a uchronia_p11 package"
  },
  {
    "objectID": "doc/build.html",
    "href": "doc/build.html",
    "title": "Build workflows",
    "section": "",
    "text": "This page is an overview of the workflows devised to build the software stack (compile, package, version, document, etc.).",
    "crumbs": [
      "Documentation",
      "Build workflow"
    ]
  },
  {
    "objectID": "doc/build.html#building-installers",
    "href": "doc/build.html#building-installers",
    "title": "Build workflows",
    "section": "Building installers",
    "text": "Building installers\nNot counting third party established libraries such as Boost, a dozen git repositories under our control are used as inputs to the pipelines that package the software stack. Maintaining versions within each of these repositories may be influenced by various factors including unrelated projects, not necessarily solely by these installers. We are using the commit hashsums of this dozen of repositories to record and specify which version of each repositories is used as input to the build pipelines (see Fig. 1)\n\n\n\n\n\n\nflowchart TD\n  A[code freeze each components see Fig. 2] --&gt; D[/git hashsums in sf-stack repo/] \n  D --&gt; E[Linux build]\n  E --&gt; H[/.deb, python wheels \\nand R tarballs/] \n  D --&gt; F[MacOSX build]\n  H-- R tarballs --&gt; G[Win build]\n\n  G --&gt;I[/.dll, R binary \\ninstallers for windows/] \n  F --&gt;J[/.dylib, homebrew packages/] \n\n  H --&gt; K[Fetch artifacts]\n  I --&gt; K\n  J --&gt; K\n  K --&gt; L[copy to shared drive]\n\n  click D \"https://bitbucket.csiro.au/projects/SF/repos/sf-stack/browse\" _blank\n  click K \"https://bitbucket.csiro.au/projects/SF/repos/cruise-control/browse/doc/build_pipelines.md\" _blank\n\n\n\n\n\nFigure 1: Package build workflow.\n\n\n\n\n\nThe source code for Azure Devops pipelines are also as of 2023-01 from github, for information purposes only. Their public availability may change in the future though.\n\nhydro-forecast-windows-pipeline\nhydro-forecast-linux-pipeline",
    "crumbs": [
      "Documentation",
      "Build workflow"
    ]
  },
  {
    "objectID": "doc/build.html#versioning",
    "href": "doc/build.html#versioning",
    "title": "Build workflows",
    "section": "Versioning",
    "text": "Versioning\nThis is a (partly populated) placeholder section to document the rationale for versioning installer bundles, and components at a final granularity.\nThere may be a versioning, semantic or otherwise, used for the bundles of installers produced for various platforms (Fig. 1). The major product is still what used to be refered to as SWIFT2, but there are also other important, related but distinct, products (FoGSS, CHyPP) also.\n\n\n\n\n\n\nflowchart TD\n  A[Testing branch] --&gt; B{testing}\n  B --&gt;|Yes| C[git tag]\n  B --&gt;|No| A\n  C --&gt;D[update CMakeLists.txt versions] \n  D --&gt; E[update debian changelog version]\n  E --&gt; F[update R package versions in\\nDESCRIPTION and R doxygen]\n  E --&gt; G[Update python _version.py file]\n  F --&gt; H[roxygenise R packages]\n  H --&gt; I[commit new or updated .Rd files]\n  I --&gt; J[commit all repo changes]\n  G --&gt; J\n  J --&gt; K[tag repo]\n  K --&gt; L[git push]\n\n\n\n\nFigure 2: Versioning components.",
    "crumbs": [
      "Documentation",
      "Build workflow"
    ]
  },
  {
    "objectID": "doc/build.html#channels-for-software-distribution",
    "href": "doc/build.html#channels-for-software-distribution",
    "title": "Build workflows",
    "section": "Channels for software distribution",
    "text": "Channels for software distribution\nSome software such as the refcount python package are available via conda-forge and pypi. We cannot use these mechanism for closed source packages, but we may consider in the future distribution via:\n\n“private” conda channel\n“private” pypi repository\n“private” debian package repository\n“private” homebrew package repository (MacOSX)",
    "crumbs": [
      "Documentation",
      "Build workflow"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html",
    "href": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html",
    "title": "Linear Muskingum channel routing model - constrained subcatchment calibration",
    "section": "",
    "text": "Linear Muskingum channel routing model - constrained subcatchment calibration ================ Jean-Michel Perraud 2020-01-28",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "calibration"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#setting-up-calibration",
    "href": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#setting-up-calibration",
    "title": "Linear Muskingum channel routing model - constrained subcatchment calibration",
    "section": "Setting up calibration",
    "text": "Setting up calibration\npSpecMaxBounds &lt;- data.frame(\n  Name =  c('X',     'Alpha'),\n  Value = c(1.0E-6, akbounds['alpha_for_max_x']), # IMPORTANT to use these values.\n  Min=    c(1.0E-6, akbounds['min_alpha']),   \n  Max =   c(akbounds['max_x'], 1e6), # Alpha_max can get very large. \n  stringsAsFactors=FALSE\n)\n\npp(pzc(subsim, pSpecMaxBounds))\n##    Name        Min          Max     Value\n## 1     X 0.00000100 3.738204e-01 0.0000010\n## 2 Alpha 0.08143331 4.861449e+04 0.1300477\nIf we were to use another (X, Alpha) point e.g. X=0.1869102, the feasible bounds for Alpha change drastically. If an optimizer samples this for an initial population of points (SCE), this is unnecessarily restrictive for Alpha. Many hydrological calibration schemes were designed without consideration on feasible space that are not hypercubes.\npp(pzc(subsim, pSpecMusk))\n##    Name       Min       Max     Value\n## 1     X 0.0000010 0.3738204 0.1869102\n## 2 Alpha 0.1001528 0.2600954 0.1300477\nWhile calibrating in the (Alpha,X) space is possible, perhaps preferable in some cases, (1/Alpha,X) has a triangular shaped feasibility region that may be easier to handle for optimizers that work with geometric transformation in the parameter space (SCE). Swift can add this on top of the constrained calibration:\n# (X, 1/Alpha) parametrizer with dynamically constrained min/max bounds.\npzer_inv &lt;- function(simulation, pSpecs=pSpecMusk) {\n  p_musk_c &lt;- pzc(simulation, pSpecs)\n  p_musk_inv_a &lt;- wrapTransform(p_musk_c)\n  addTransform(p_musk_inv_a, 'inv_alpha', 'Alpha', '1/x')\n  p_musk_inv_a\n}\n\np &lt;- pzer_inv(subsim, pSpecMaxBounds)\npp(p)\n##        Name       Min        Max    Value\n## 1 inv_alpha 2.057e-05 12.2799877 7.689486\n## 2         X 1.000e-06  0.3738204 0.000001\nWe check that backtransforming to (Alpha-X) works:\npp(backtransform(p))\n##    Name        Min          Max     Value\n## 1     X 0.00000100 3.738204e-01 0.0000010\n## 2 Alpha 0.08143331 4.861449e+04 0.1300477\nobjectiveId &lt;- 'NSE'\nobjective &lt;- createObjective(subsim, varId, observation=someFlow, objectiveId, start(someFlow), end(someFlow))\nscore &lt;- getScore(objective,p)  \nscore\n## $scores\n##       NSE \n## 0.9997748 \n## \n## $sysconfig\n##        Name       Min        Max    Value\n## 1 inv_alpha 2.057e-05 12.2799877 7.689486\n## 2         X 1.000e-06  0.3738204 0.000001\n#termination &lt;- swift::CreateSceMaxRuntimeTerminationWila_R(1/60)\ntermination &lt;- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.001','0.0167'))\nsceParams &lt;- getDefaultSceParameters()\nnpars &lt;- 2\nsceParams &lt;- SCEParameters(npars)\noptimizer &lt;- createSceOptimSwift(objective,terminationCriterion = termination, populationInitializer = p,SCEpars = sceParams)\ncalibLogger &lt;- setCalibrationLogger(optimizer,\"dummy\")\n\noptimStartTime &lt;- lubridate::now();\ncalibResults &lt;- executeOptimization(optimizer)\noptimEndTime &lt;- lubridate::now();\noptimWallClock &lt;- lubridate::as.duration(lubridate::interval(optimStartTime, optimEndTime))\n\noptimWallClock\n## [1] \"0.209973812103271s\"\noptLog &lt;- extractOptimizationLog(optimizer, fitnessName = \"NSE\")\ngeomOps &lt;- optLog$geomOps \n\nshuffleLogs &lt;- mhplot::subsetByCategory(optLog$data, pattern = \"Initial.*|Shuffling.*\") \nmhplot::plotShuffles(shuffleLogs, 'X', 'inv_alpha', objLims = (0:1))\n\nsortedResults &lt;- sortByScore(calibResults, 'NSE')\nhead(scoresAsDataFrame(sortedResults))\n##   NSE inv_alpha         X\n## 1   1  7.688320 0.1871205\n## 2   1  7.687345 0.1867482\n## 3   1  7.686741 0.1869934\n## 4   1  7.693171 0.1868937\n## 5   1  7.693687 0.1868359\n## 6   1  7.693877 0.1869150\nq &lt;- getBestScore(calibResults, 'NSE', FALSE)\nq &lt;- GetSystemConfigurationWila_R(q)\npp(q)\n##        Name      Min       Max     Value\n## 1 inv_alpha 3.849069 9.9821603 7.6883204\n## 2         X 0.000001 0.3737638 0.1871205\npp(backtransform(q))\n##    Name       Min       Max     Value\n## 1     X 0.0000010 0.3737638 0.1871205\n## 2 Alpha 0.1001787 0.2598031 0.1300674",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "calibration"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#seeding-the-optimisation-point-population-with-restrictive-constraint-bounds",
    "href": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#seeding-the-optimisation-point-population-with-restrictive-constraint-bounds",
    "title": "Linear Muskingum channel routing model - constrained subcatchment calibration",
    "section": "Seeding the optimisation point population with restrictive constraint bounds",
    "text": "Seeding the optimisation point population with restrictive constraint bounds\nThis section is a counter-example. Do not do this.\nSay, instead of seeding with alpha set to alpha_for_x_max (0.37382040) we instead use a value cloase to its global minimum, 0.083:\npSpecRestrictiveBounds &lt;- pSpecMaxBounds\npSpecRestrictiveBounds$Value[2] &lt;- 0.083\npSpecRestrictiveBounds\n##                  Name   Value        Min          Max\n##                     X 1.0e-06 0.00000100 3.738204e-01\n## alpha_for_max_x Alpha 8.3e-02 0.08143322 1.000000e+06\np &lt;- pzer_inv(subsim, pSpecRestrictiveBounds)\npp(p)\n##        Name       Min         Max     Value\n## 1 inv_alpha 2.057e-05 12.27998772 12.048193\n## 2         X 1.000e-06  0.01887681  0.000001\nX is now much more constrained in its feasible range, and initializing a population fails to cover large sections of the feasible triangle. If used in the optimizer (uniform random sampling)\ntermination &lt;- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.001','0.0167'))\nsceParams &lt;- getDefaultSceParameters()\nnpars &lt;- 2\nsceParams &lt;- SCEParameters(npars)\noptimizer &lt;- createSceOptimSwift(objective,terminationCriterion = termination, populationInitializer = p,SCEpars = sceParams)\ncalibLogger &lt;- setCalibrationLogger(optimizer,\"dummy\")\ncalibResults &lt;- executeOptimization(optimizer)\nlibrary(mhplot)\noptLog &lt;- extractOptimizationLog(optimizer, fitnessName = \"NSE\")\ngeomOps &lt;- optLog$geomOps \n\nshuffleLogs &lt;- mhplot::subsetByCategory(optLog$data, pattern = \"Initial.*|Shuffling.*\") \nmhplot::plotShuffles(shuffleLogs, 'X', 'inv_alpha', objLims = (0:1))\n SCE does manage to converge towards the optimum, but it takes a larger number of iterations. Anecdotally, we observed cases where the calibration does fail to go near the optimum, when interplaying with a convergence criterion configured for “leniency”.",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "calibration"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#provenance",
    "href": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#provenance",
    "title": "Linear Muskingum channel routing model - constrained subcatchment calibration",
    "section": "Provenance",
    "text": "Provenance\nThis document was generated from an R markdown file on 2020-01-28 10:56:27\nsessionInfo()\n## R version 3.6.1 (2019-07-05)\n## Platform: x86_64-w64-mingw32/x64 (64-bit)\n## Running under: Windows 10 x64 (build 17763)\n## \n## Matrix products: default\n## \n## locale:\n## [1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252   \n## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C                      \n## [5] LC_TIME=English_Australia.1252    \n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mhplot_0.5-1     stringr_1.4.0    readr_1.3.1      dplyr_0.8.3     \n##  [5] tidyr_1.0.0      ggplot2_3.2.1    lubridate_1.7.4  DiagrammeR_1.0.1\n##  [9] swift_2.1.2      Rcpp_1.0.2       knitr_1.25       rmarkdown_1.16  \n## \n## loaded via a namespace (and not attached):\n##  [1] mvtnorm_1.0-11     lattice_0.20-38    visNetwork_2.0.8  \n##  [4] msvs_0.3.1         zoo_1.8-6          utf8_1.1.4        \n##  [7] assertthat_0.2.1   zeallot_0.1.0      digest_0.6.21     \n## [10] R6_2.4.0           backports_1.1.4    evaluate_0.14     \n## [13] pillar_1.4.2       rlang_0.4.0        lazyeval_0.2.2    \n## [16] rstudioapi_0.10    labeling_0.3       webshot_0.5.1     \n## [19] downloader_0.4     htmlwidgets_1.3    igraph_1.2.4.1    \n## [22] munsell_0.5.0      compiler_3.6.1     influenceR_0.1.0  \n## [25] rgexf_0.15.3       xfun_0.9           pkgconfig_2.0.2   \n## [28] htmltools_0.3.6    tidyselect_0.2.5   tibble_2.1.3      \n## [31] gridExtra_2.3      XML_3.98-1.20      fansi_0.4.0       \n## [34] joki_0.4.0         viridisLite_0.3.0  crayon_1.3.4      \n## [37] withr_2.1.2        grid_3.6.1         jsonlite_1.6      \n## [40] gtable_0.3.0       lifecycle_0.1.0    magrittr_1.5      \n## [43] scales_1.0.0       cli_1.1.0          stringi_1.4.3     \n## [46] viridis_0.5.1      calibragem_0.8-0   ellipsis_0.3.0    \n## [49] brew_1.0-6         xts_0.11-2         vctrs_0.2.0       \n## [52] RColorBrewer_1.1-2 tools_3.6.1        glue_1.3.1        \n## [55] purrr_0.3.2        hms_0.5.1          Rook_1.1-1        \n## [58] yaml_2.2.0         colorspace_1.4-1   cinterop_0.3.0    \n## [61] uchronia_2.1.2",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "calibration"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/log_likelihood/log_likelihood.html",
    "href": "doc/samples/R/vignettes/log_likelihood/log_likelihood.html",
    "title": "Sample code for log-likelihood calibration",
    "section": "",
    "text": "Sample code for log-likelihood calibration\nJean-Michel Perraud 2020-01-28\n\n\nSample code for log-likelihood calibration\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:54:01. It illustrates how to set up a calibration with a log-likelihood objective.\n\n\nSetting up a calibration on daily data\nWe will use some sample data from (MMH - Minamata?) included in the package\nlibrary(lubridate)\nlibrary(swift)\ndata(swift_sample_data)\nsSpan &lt;- '1990-01-01/2005-12-31'\nrain &lt;- sampleSeries('MMH', 'rain')[sSpan]\nevap &lt;- sampleSeries('MMH', 'evap')[sSpan]\nflow &lt;- sampleSeries('MMH', 'flow')[sSpan]\nWe need to adjust the observed flow, as the SWIFTv1 legacy missing value code is not consistent with default handling in SAK.\nflow[flow&lt;0] &lt;- NA\nLet’s create a single catchment setup, using daily data. We need so specify the simulation time step to be consistent with the daily input data.\nms &lt;- createSubarea('GR4J', 1.0)\ns &lt;- start(rain)\ne &lt;- end(rain)\nsetSimulationSpan(ms, s, e)\nsetSimulationTimeStep(ms, 'daily')\nAssign input time series\nsubAreaName &lt;- getSubareaNames(ms)[1]\nplaySubareaInput(ms, input=rain, subAreaName, \"P\")\nplaySubareaInput(ms, input=evap, subAreaName, \"E\")\nModel variables identifiers are hierarchical, with separators ‘.’ and ‘|’ supported. The “dot” notation should now be preferred, as some R functions producing data frames may change the variable names and replace some characters with ‘.’.\nsubareaId &lt;- paste0(\"subarea.\", subAreaName)\nrootId &lt;- paste0(subareaId, \".\")\nprint(getVariableIds(ms, subareaId))\n##  [1] \"areaKm2\"       \"P\"             \"E\"             \"runoff\"       \n##  [5] \"S\"             \"R\"             \"Ps\"            \"Es\"           \n##  [9] \"Pr\"            \"ech1\"          \"ech2\"          \"Perc\"         \n## [13] \"x1\"            \"x2\"            \"x3\"            \"x4\"           \n## [17] \"UHExponent\"    \"PercFactor\"    \"OutflowVolume\" \"OutflowRate\"\ngr4StateNames &lt;- paste0(rootId, c('runoff', 'S', 'R', 'Perc'))\nfor (name in gr4StateNames) { recordState(ms, name) }\nLet’s check that one simulation runs fine, before we build a calibration definition.\nexecSimulation(ms)\nsState &lt;- getRecorded(ms, gr4StateNames[2])\nnames(sState) &lt;- shortVarId(names(sState))\nzoo::plot.zoo(sState)\n\nLet’s build the objective calculator that will guide the calibration process:\nw &lt;- uchronia::mkDate(1992, 01, 01)\nrunoffDepthVarname &lt;- gr4StateNames[1]\nmodRunoff &lt;- getRecorded(ms, runoffDepthVarname)\nzoo::index(flow) &lt;- zoo::index(modRunoff)\nobjective &lt;- createObjective(ms, runoffDepthVarname, flow, 'log-likelihood', w, e)\nDefine the feasible parameter space, using a generic parameter set for the model parameters. This is ‘wrapped’ by a log-likelihood parameter set with the extra parameters used in the log likelihood calculation, but which exposes all the parameters as 8 independent degrees of freedom to the optimizer.\n(pSpecGr4j &lt;- joki::getFreeParams('GR4J'))\n##   Name      Value Min  Max\n## 1   x1 650.488000   1 3000\n## 2   x2  -0.280648 -27   27\n## 3   x3   7.891230   1  660\n## 4   x4  18.917200   1  240\npSpecGr4j$Value &lt;- c(542.1981111, -0.4127542, 7.7403390, 1.2388548)\npSpecGr4j$Min &lt;- c(1,-30, 1,1)\npSpecGr4j$Max &lt;- c(3000, 30, 1000, 240)\npSpecGr4j$Name &lt;- paste0(rootId, pSpecGr4j$Name)\n\n\nmaxobs &lt;- max(flow, na.rm=TRUE)\np &lt;- createParameterizer(type='Generic', specs=pSpecGr4j)\nsetLogLikParamKeys(a='a', b='b', m='m', s='s', ct=\"ct\", censopt='censopt')\ncensorThreshold &lt;- maxobs / 100 # TBC\ncensopt &lt;- 0.0\n\nloglik &lt;- createParameterizer(type='no apply')\naddToHyperCube(loglik, \n          data.frame( Name=c('b','m','s','a','maxobs','ct', 'censopt'),\n          Min   = c(-30, 0, 1,    -30, maxobs, censorThreshold, censopt),\n          Max   = c(0,   0, 1000, 1, maxobs, censorThreshold, censopt),\n          Value = c(-7,  0, 100,  -10, maxobs, censorThreshold, censopt),\n          stringsAsFactors=FALSE) )\np &lt;- concatenateParameterizers(p, loglik)\nparameterizerAsDataFrame(p)\n##                  Name         Min          Max       Value\n## 1  subarea.Subarea.x1   1.0000000 3000.0000000 542.1981111\n## 2  subarea.Subarea.x2 -30.0000000   30.0000000  -0.4127542\n## 3  subarea.Subarea.x3   1.0000000 1000.0000000   7.7403390\n## 4  subarea.Subarea.x4   1.0000000  240.0000000   1.2388548\n## 5                   b -30.0000000    0.0000000  -7.0000000\n## 6                   m   0.0000000    0.0000000   0.0000000\n## 7                   s   1.0000000 1000.0000000 100.0000000\n## 8                   a -30.0000000    1.0000000 -10.0000000\n## 9              maxobs  17.2211304   17.2211304  17.2211304\n## 10                 ct   0.1722113    0.1722113   0.1722113\n## 11            censopt   0.0000000    0.0000000   0.0000000\nCheck that the objective calculator works, at least with the default values in the feasible parameter space:\nscore &lt;- getScore(objective, p)\nprint(score)\n## $scores\n## Log-likelihood \n##      -463759.9 \n## \n## $sysconfig\n##                  Name         Min          Max       Value\n## 1  subarea.Subarea.x1   1.0000000 3000.0000000 542.1981111\n## 2  subarea.Subarea.x2 -30.0000000   30.0000000  -0.4127542\n## 3  subarea.Subarea.x3   1.0000000 1000.0000000   7.7403390\n## 4  subarea.Subarea.x4   1.0000000  240.0000000   1.2388548\n## 5                   b -30.0000000    0.0000000  -7.0000000\n## 6                   m   0.0000000    0.0000000   0.0000000\n## 7                   s   1.0000000 1000.0000000 100.0000000\n## 8                   a -30.0000000    1.0000000 -10.0000000\n## 9              maxobs  17.2211304   17.2211304  17.2211304\n## 10                 ct   0.1722113    0.1722113   0.1722113\n## 11            censopt   0.0000000    0.0000000   0.0000000\nmodRunoff &lt;- getRecorded(ms, runoffDepthVarname)\njoki::plotTwoSeries(flow, modRunoff, ylab=\"obs/mod runoff\", startTime = start(flow), endTime = end(flow))\n\nBuild the optimiser definition, instrument with a logger.\n# term &lt;- getMaxRuntimeTermination(maxHours = 0.3/60)  # ~20 second appears enough with SWIFT binaries in Release mode\n# term &lt;- getMarginalTermination(tolerance = 1e-06, cutoffNoImprovement = 10, maxHours = 0.3/60) \nterm &lt;- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.05','0.0167'))\n\nsceParams &lt;- getDefaultSceParameters()\nurs &lt;- createParameterSampler(0, p, 'urs')\noptimizer &lt;- createSceOptimSwift(objective, term, SCEpars=sceParams, urs)\ncalibLogger &lt;- setCalibrationLogger(optimizer, '')\nstartTime &lt;- lubridate::now()\ncalibResults &lt;- executeOptimization(optimizer)\nendTime &lt;- lubridate::now()\ncalibWallTime &lt;- endTime-startTime\nprint(paste( 'Optimization completed in ', calibWallTime, attr(calibWallTime, 'units')))\n## [1] \"Optimization completed in  50.4119968414307 secs\"\nd &lt;- getLoggerContent(optimizer)\nd$PointNumber = 1:nrow(d)\nlogMh &lt;- mhplot::mkOptimLog(d, fitness = 'Log.likelihood', messages = \"Message\", categories = \"Category\") \ngeomOps &lt;- mhplot::subsetByMessage(logMh)\nstr(geomOps@data)\n## 'data.frame':    2575 obs. of  16 variables:\n##  $ Category          : Factor w/ 7 levels \"Complex No 0\",..: 7 7 7 7 7 7 7 7 7 7 ...\n##  $ CurrentShuffle    : Factor w/ 37 levels \"\",\"0\",\"1\",\"10\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Message           : Factor w/ 6 levels \"Adding a random point in hypercube\",..: 4 4 4 4 4 4 4 4 4 4 ...\n##  $ Log.likelihood    : num  -1.00e+20 -1.00e+20 -1.00e+20 -7.71e+05 -1.00e+20 ...\n##  $ a                 : num  -17.63 -26.11 -3.56 -28.56 -14.85 ...\n##  $ b                 : num  -24.79 -4.05 -5.19 -4.51 -7.93 ...\n##  $ censopt           : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ ct                : num  0.172 0.172 0.172 0.172 0.172 ...\n##  $ m                 : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ maxobs            : num  17.2 17.2 17.2 17.2 17.2 ...\n##  $ s                 : num  710 977 784 196 354 ...\n##  $ subarea.Subarea.x1: num  2843 227 1651 2993 224 ...\n##  $ subarea.Subarea.x2: num  23 13.5 -13.6 26.2 -26.1 ...\n##  $ subarea.Subarea.x3: num  62.2 611.4 425.9 246.5 658.5 ...\n##  $ subarea.Subarea.x4: num  161.9 223.4 120 190.2 65.7 ...\n##  $ PointNumber       : int  1 2 3 4 5 6 7 8 9 10 ...\npVarIds &lt;- (parameterizerAsDataFrame(p))$Name\nfor (pVar in pVarIds) {\n    print(mhplot::plotParamEvolution(geomOps, pVar))\n}\n\nFinally, get a visual of the runoff time series with the best known parameter set (the penultimate entry in the data frame with the log of the calibration process).\nsortedResults &lt;- sortByScore(calibResults, 'Log-likelihood')\nhead(scoresAsDataFrame(sortedResults))\n##   Log.likelihood subarea.Subarea.x1 subarea.Subarea.x2 subarea.Subarea.x3\n## 1       2924.671           266.1925          -29.60785           773.6771\n## 2       2923.107           191.3387          -29.28017           824.4254\n## 3       2920.818           176.7221          -27.53492           866.5888\n## 4       2911.518           177.2337          -28.48846           789.2143\n## 5       2911.046           280.0083          -28.70505           771.3981\n## 6       2902.444           231.8093          -26.57367           809.7063\n##   subarea.Subarea.x4         b m        s         a   maxobs        ct\n## 1           1.033368 -1.850320 0 1.099840 -4.939078 17.22113 0.1722113\n## 2           1.147338 -2.099422 0 1.242173 -5.096023 17.22113 0.1722113\n## 3           1.032679 -2.056026 0 1.189237 -5.143367 17.22113 0.1722113\n## 4           1.165675 -2.045539 0 1.146694 -4.928836 17.22113 0.1722113\n## 5           1.082458 -2.118719 0 1.143031 -4.991464 17.22113 0.1722113\n## 6           1.035202 -2.024382 0 1.165068 -5.007636 17.22113 0.1722113\n##   censopt\n## 1       0\n## 2       0\n## 3       0\n## 4       0\n## 5       0\n## 6       0\nbestPset &lt;- getScoreAtIndex(sortedResults, 1)\napplySysConfig(bestPset, ms)\nexecSimulation(ms)\nmodRunoff &lt;- getRecorded(ms, runoffDepthVarname)\njoki::plotTwoSeries(flow, modRunoff, ylab=\"obs/mod runoff\", startTime = start(flow), endTime = end(flow))"
  },
  {
    "objectID": "doc/samples/R/vignettes/getting_started/getting_started.html",
    "href": "doc/samples/R/vignettes/getting_started/getting_started.html",
    "title": "Getting started with the swift R package",
    "section": "",
    "text": "Getting started with the swift R package\nJean-Michel Perraud 2020-01-28\n\n\nGetting started with the SWIFT R package\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:53:51. It is the introduction ‘vignette’ to an R package for interacting with SWIFT.\nIt shows one of the most basic usage, running a single model simulation. While basic, it is realistic and uses data from a study catchment.\n\n\nGetting started\nLoad the R package the usual way…\nlibrary(swift)\nIt includes a fair level of documentation, that should be accessible using the ‘?’ command, shortcut for help(swift).\n?swift\n?getRecorded\nThe package contains some sample data for a few Australian catchments. Note that these sample data are for documentation only and not to be used for real world applications.\ndata('swift_sample_data')\nnames(swiftSampleData)\n## [1] \"MMH\"       \"Ovens\"     \"Abbeyard\"  \"07378500\"  \"South_Esk\" \"Adelaide\"\nswift now has some functions to create a single subarea simulation for testing purposes. While is it perfectly possible to manually build your own model simulation from scratch, for the sake of getting started quickly let’s use pre-defined functions to get a model simulation ready to run. The parameters of the function should be fairly self-explanatory to you:\nms &lt;- createSubareaSimulation(dataId='MMH', simulStart='1990-01-01', simulEnd='2005-12-31', \n    modelId='GR4J', tstep='daily', varNameRain='P', varNamePet='E')\nstr(ms)\n## Formal class 'ExternalObjRef' [package \"cinterop\"] with 2 slots\n##   ..@ obj :&lt;externalptr&gt; \n##   ..@ type: chr \"MODEL_SIMULATION_PTR\"\nThe R object ‘ms’ may appear unusual to many R users. This is basically a handle to the SWIFT simulation written in C++. It is passed as an argument to R functions, and you do not need to know more details than this.\nThe simulation we just created is ready to execute, which means it already has some input data defined (site ‘MMH’). We can inspect it for instance with:\nhead(getPlayed(ms))\n## Warning: timezone of object (UTC) is different than current timezone ().\n\n##            subcatchment.Subarea.E subarea.Subarea.P\n## 1990-01-01               5.542243               0.000000\n## 1990-01-02               5.552225               0.000000\n## 1990-01-03               5.562208               0.000000\n## 1990-01-04               5.572191               0.000000\n## 1990-01-05               5.582174               0.000000\n## 1990-01-06               5.592156               0.526757\nzoo::plot.zoo(getPlayed(ms), main='MMH daily climate inputs')\n\nThe simulation has no output to record to time series defined yet. SWIFT is designed to record model variables on demand in a highly flexible manner. First, we can query the system to find out known models, and the model variable names that we can record.\ngetRecordedVarnames(ms)\n## character(0)\nrunoffModelIds()\n##  [1] \"NetRainfall\"   \"GR4J\"          \"GR4J_SG\"       \"GR5H\"         \n##  [5] \"GR6J\"          \"GR5J\"          \"PDM\"           \"AWBM\"         \n##  [9] \"SACSMA\"        \"const_outflow\" \"external\"      \"GRKAL\"\ngr4jModelVars &lt;- runoffModelVarIds('GR4J')\nprint(gr4jModelVars)\n##  [1] \"P\"          \"E\"          \"runoff\"     \"S\"          \"R\"         \n##  [6] \"Ps\"         \"Es\"         \"Pr\"         \"ech1\"       \"ech2\"      \n## [11] \"Perc\"       \"x1\"         \"x2\"         \"x3\"         \"x4\"        \n## [16] \"UHExponent\" \"PercFactor\"\nThese are the variable names for a single GR4J model instance; since SWIFT is for semi-distributed models, we need to use a hierarchical naming scheme to uniquely identify model variables (even when in this case we do have only one subarea). Using unique keys allow to inspect the model states in great details if needed.\ngetSubareaIds(ms)\n## [1] \"Subarea\"\ngetStateValue(ms, 'subarea.Subarea.x4')\n## subarea.Subarea.x4 \n##                0.5\ntoRecord &lt;- c('runoff', 'S', 'R', 'Ps', 'Es', 'Pr', 'ech1', 'ech2', 'Perc')\nstopifnot(all( toRecord %in% gr4jModelVars)) # just a check in case something changes in the future\nLet’s record to time series all the storage and flux states of GR4J (no need to record model parameters which will be flat lines here)\nrecordState(ms, paste0('subarea.Subarea.', toRecord))\ngetRecordedVarnames(ms)\n## [1] \"subarea.Subarea.Es\"     \"subarea.Subarea.Perc\"  \n## [3] \"subarea.Subarea.Pr\"     \"subarea.Subarea.Ps\"    \n## [5] \"subarea.Subarea.R\"      \"subarea.Subarea.S\"     \n## [7] \"subarea.Subarea.ech1\"   \"subarea.Subarea.ech2\"  \n## [9] \"subarea.Subarea.runoff\"\nAnd let’s run\nexecSimulation(ms)\nWe do a bit of text preprocessing to shorten the default names of the time series (‘xts’ objects in R), but otherwise default time series plots are straightforward.\nvarSeries = getRecorded(ms)\nstr(varSeries)\n## An 'xts' object on 1990-01-01/2005-12-31 containing:\n##   Data: num [1:5844, 1:9] 0 0 0 0 0 ...\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : NULL\n##   ..$ : chr [1:9] \"subarea.Subarea.Es\" \"subarea.Subarea.Perc\" \"subarea.Subarea.Pr\" \"subarea.Subarea.Ps\" ...\n##   Indexed by objects of class: [POSIXct,POSIXt] TZ: UTC\n##   xts Attributes:  \n##  NULL\nnames(varSeries) &lt;- gsub(names(varSeries), pattern=\"subarea\\\\.Subarea\\\\.\", replacement='')\nzoo::plot.zoo(varSeries, main = 'Default GR4J output on MMH data')\n\nLet’s look at a shorter period of the output; we can demonstrate the use of the lubridate package for convenient date-time arithmetic, and the window function. We define a couple of functions to slice and plot the last three years of the time series.\nlibrary(lubridate)\n\nlastThreeYears &lt;- function(tts) {\n    window(tts, start=end(tts)-lubridate::years(3), end=end(tts)) \n}\nobsVsCalc &lt;- function(obs, calc, ylab=\"runoff (mm)\") {\n    obs &lt;- lastThreeYears(obs)\n    joki::plotTwoSeries(obs, calc, ylab=ylab, startTime = start(obs), endTime = end(obs))\n}\nzoo::plot.zoo(lastThreeYears(varSeries), main = 'Default GR4J output on MMH data')\n\n\n\nExploring the model interactively\nAs mentioned earlier, it is possibly to define the model simulation definition directly and interactively. The following shows how a to assign another input time series, with a somewhat contrived example of a scaled up precipitation input series.\nprecipId &lt;- 'subarea.Subarea.P'\nrunoffId &lt;- 'subarea.Subarea.runoff'\nprecip &lt;- getPlayed(ms, precipId)\nbaselineRunoff &lt;- getRecorded(ms, runoffId)\nprecipScaled &lt;- precip * 1.1\nplayInput(ms, precipScaled, precipId)\nexecSimulation(ms)\nrunoffDiff &lt;- getRecorded(ms, runoffId) - baselineRunoff\nzoo::plot.zoo(lastThreeYears(runoffDiff), main = 'Additional runoff with scaled up precipitation', ylab='runoff depth change (mm)')\n\nLet’s get back to the initial input settings, and demonstrate one way to change parameters interactively. Note that this is not necessarily the recommended way to handle model parameterisation, as will be made clear later in this document when setting up calibration. You can already see how the code required for just one parameter is more length.\nplayInput(ms, precip, precipId)\nx4Id &lt;- 'subarea.Subarea.x4'\n(x4 &lt;- getStateValue(ms, x4Id))\n## subarea.Subarea.x4 \n##                0.5\nsetStateValue(ms, x4Id, x4*1.1)\nexecSimulation(ms)\nrunoffDiff &lt;- getRecorded(ms, runoffId) - baselineRunoff\nzoo::plot.zoo(lastThreeYears(runoffDiff), main = 'Change in runoff with x4 scaled up by 10%', ylab='runoff depth change')\n\nsetStateValue(ms, x4Id, x4)\n\n\nCalibration\nLet’s st up a calibration against the observed runoff depth for ‘MMH’, included as sample data in the package, and view it along the current default model runoff output.\nobsRunoff &lt;- sampleSeries('MMH', 'flow') #actually, runoff depth\nobsRunoff[which(obsRunoff &lt; -1)] &lt;- NA\nobsVsCalc(obsRunoff, baselineRunoff)\n\nFirst let’s define the objective of the calibration, the Nash-Sutcliffe efficiency (NSE) for the runoff depth. We’ll use two years of data as warmup.\ns &lt;- start(obsRunoff)\nw &lt;- s + lubridate::years(2)\ne &lt;- end(obsRunoff)\nsetSimulationSpan(ms, s, e)\nobjective &lt;- createObjective(ms, runoffId, obsRunoff, 'NSE', w, e)\nWhile we can tweak model parameters directly from R one by one as shown in the previous paragraph, there are tools available in the core SWIFT library for catchment parameterization. These are more scalable to catchments with a large number of sub-areas. In this section we’ll use a generic parameterizer. Other forms available are parameterizers applying scaled “meta-parameters” to subareas, based for instance on the surface of each subarea.\nStarting from an arbitrary template data frame for GR4J parameters:\n(pSpecGr4j &lt;- joki::getFreeParams('GR4J'))\n##   Name      Value Min  Max\n## 1   x1 650.488000   1 3000\n## 2   x2  -0.280648 -27   27\n## 3   x3   7.891230   1  660\n## 4   x4  18.917200   1  240\npSpecGr4j$Value &lt;- c(542.1981111,  -0.4127542,   7.7403390 ,  1.2388548)\npSpecGr4j$Min &lt;- c(1,-30, 1,1)\npSpecGr4j$Max &lt;- c(1000, 30, 1000, 240)\nThe template is using the short names of the GR4J parameters; our SWIFT parameterizer here is generic and needs the full hierarchical name of the model variable identifiers\npSpecGr4j$Name &lt;- paste0('subarea.Subarea.', pSpecGr4j$Name)\np &lt;- createParameterizer(type='Generic', pSpecGr4j)\nparameterizerAsDataFrame(p)\n##                 Name Min  Max       Value\n## 1 subarea.Subarea.x1   1 1000 542.1981111\n## 2 subarea.Subarea.x2 -30   30  -0.4127542\n## 3 subarea.Subarea.x3   1 1000   7.7403390\n## 4 subarea.Subarea.x4   1  240   1.2388548\nLet’s see the default goodness of fit for these parameters. Unsurprisingly, fairly poor. The purpose of the following lines is to show how to get manually objective score values.\nscore &lt;- getScore(objective, p)\nprint(score)\n## $scores\n##       NSE \n## -2.338177 \n## \n## $sysconfig\n##                 Name Min  Max       Value\n## 1 subarea.Subarea.x1   1 1000 542.1981111\n## 2 subarea.Subarea.x2 -30   30  -0.4127542\n## 3 subarea.Subarea.x3   1 1000   7.7403390\n## 4 subarea.Subarea.x4   1  240   1.2388548\nWe have our objectives defined, and the parameter space ‘p’ in which to search. Let’s create an optimizer and we are ready to go. While the optimizer can be created in one line, we show how to choose one custom termination criterion and how to configure the optimizer to capture a detailed log of the process.\nThere are several options for defining a calibration termination criterion\nterm &lt;- getMarginalTermination(tolerance = 1e-06, cutoffNoImprovement = 100, maxHours = 0.05) \nterm &lt;- getMaxRuntimeTermination(maxHours = 0.0015) \nFor this vignette we will use a criterion using the maximum standard deviation of parameters in a population (of 0.2%). Note that most termination criteria have a maximum wallclock runtime as a fallback to set an upper bound for reasonable runtime.\nterm &lt;- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.002','0.0167'))\n\nsceParams &lt;- getDefaultSceParameters()\nurs &lt;- createParameterSampler(0, p, 'urs')\noptimizer &lt;- createSceOptimSwift(objective, term, SCEpars=sceParams, urs)\n\ncalibLogger &lt;- setCalibrationLogger(optimizer, '')\nstartTime &lt;- lubridate::now()\ncalibResults &lt;- executeOptimization(optimizer)\nendTime &lt;- lubridate::now()\ncalibWallTime &lt;- endTime-startTime\nprint(paste( 'Optimization completed in ', calibWallTime, attr(calibWallTime, 'units')))\n## [1] \"Optimization completed in  2.04213500022888 secs\"\nswift uses optimization tools that will parallelize model simulation runs if possible (i.e. if supported by the model).\nThere are high level functions in the packages swift and mhplot to import the optimisation log information into R data structures\noptLog &lt;- extractOptimizationLog(optimizer, fitnessName = \"NSE\")\ngeomOps &lt;- optLog$geomOps \nstr(geomOps@data)\n## 'data.frame':    1848 obs. of  9 variables:\n##  $ Category          : Factor w/ 7 levels \"Complex No 0\",..: 7 7 7 7 7 7 7 7 7 7 ...\n##  $ CurrentShuffle    : Factor w/ 29 levels \"\",\"0\",\"1\",\"10\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Message           : Factor w/ 5 levels \"Adding a random point in hypercube\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ NSE               : num  -1339.286 -0.891 -0.327 -2.367 -913.047 ...\n##  $ subarea.Subarea.x1: num  948 175 215 725 874 ...\n##  $ subarea.Subarea.x2: num  23.01 -7.28 -7.07 6.66 28.64 ...\n##  $ subarea.Subarea.x3: num  62.2 710.5 124.6 930.6 126.4 ...\n##  $ subarea.Subarea.x4: num  161.9 96.4 19 207.7 83.7 ...\n##  $ PointNumber       : int  1 2 3 4 5 6 7 8 9 10 ...\nWe can see that at least one of the parameters settled at one of its boundaries:\npVarIds &lt;- parameterizerAsDataFrame(p)$Name\npVarIds\n## [1] \"subarea.Subarea.x1\" \"subarea.Subarea.x2\" \"subarea.Subarea.x3\"\n## [4] \"subarea.Subarea.x4\"\nprint(mhplot::plotParamEvolution(geomOps, pVarIds[1], objLims=c(0,1)))\n\nNote that the parameter x4 also seems to have settled at its lower bound. x4 influences the unit hydrograph, and the meaning of this parameter depends on the time step of the input series. It may be justified in this case to go below 1 for its lower bound.\nprint(mhplot::plotParamEvolution(geomOps, pVarIds[4], objLims=c(0,1)))\n\nSo let’s restart, with a larger upper bound for the x1 parameter:\npSpecGr4j$Max &lt;- c(2500, 30, 1000, 240)\npSpecGr4j$Min &lt;- c(1,-30, 1,0.2)\np &lt;- createParameterizer(type='Generic', pSpecGr4j)\nurs &lt;- createParameterSampler(0, p, 'urs')\noptimizer &lt;- createSceOptimSwift(objective, term, SCEpars=sceParams, urs)\ncalibLogger &lt;- setCalibrationLogger(optimizer, '')\ncalibResults &lt;- executeOptimization(optimizer)\noptLog &lt;- extractOptimizationLog(optimizer, fitnessName = \"NSE\")\ngeomOps &lt;- optLog$geomOps \nLet’s check that the parameter does not settle at the boundary anymore:\nd &lt;- mhplot::plotParamEvolution(geomOps, pVarIds[1], objLims=c(0,1))\nprint(d)\n\nNote that there are further options in mhplot and ggplot2 to assess the behavior of the optimization process. Explore these packages’ documentation to find these options.\nlibrary(ggplot2)\nd + facet_wrap( as.formula(paste(\"~\", geomOps@messages, sep=' ')) )\n\nLet’s retrieve the parameter set with the best NSE, and see the resulting runoff time series.\n(bestPset &lt;- getBestScore(calibResults, 'NSE'))\n## An object of class \"ExternalObjRef\"\n## Slot \"obj\":\n## &lt;pointer: 0x0000000016f07c10&gt;\n## \n## Slot \"type\":\n## [1] \"OBJECTIVE_SCORES_WILA_PTR\"\nTo get a visual on the information of this external pointer, we can use:\nasRStructure(bestPset)\n## $scores\n##       NSE \n## 0.7832226 \n## \n## $sysconfig\n##                 Name   Min  Max       Value\n## 1 subarea.Subarea.x1   1.0 2500 1141.326083\n## 2 subarea.Subarea.x2 -30.0   30   -5.443462\n## 3 subarea.Subarea.x3   1.0 1000   97.374236\n## 4 subarea.Subarea.x4   0.2  240    0.430444\nNote, as an aside, that we see only the last 3 years of time series, while the NSE score is calculated over several more years. As it happens, the runoff prediction has a systematic negative bias.\napplySysConfig(bestPset, ms)\nexecSimulation(ms)\nobsVsCalc(obsRunoff, getRecorded(ms, runoffId))",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Getting started with the swift R package"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/calibration_initial_states/calibration_initial_states.html",
    "href": "doc/samples/R/vignettes/calibration_initial_states/calibration_initial_states.html",
    "title": "Calibration with initial model memory states as parameters",
    "section": "",
    "text": "Calibration with initial model memory states as parameters\nJean-Michel Perraud 2020-01-28\n\n\nCalibration with initial model memory states as parameters\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:52:32. It is a vignette to demonstrate features in SWIFT to calibrate a model with initial model states as a parameter.\n\n\nEssentials of setting up a calibration of initial states\nThis vignette will illustrate how to define two meta-parameters, S0 and R0, controlling the initial level of stores in the GR4J model, as fraction of the store capacities.\nWe’ll load a simple catchment with one subarea only; the feature applies equally to catchment with multiple sub-areas\nlibrary(swift)\n\nmodelId &lt;- 'GR4J'\nms &lt;- createSubareaSimulation(dataId='MMH', simulStart='1990-01-01', simulEnd='2005-12-31', \n    modelId=modelId, tstep='daily', varNameRain='P', varNamePet='E')\nWe define a few model state identifiers, and set them to be recorded to time series.\ngr4jModelVars &lt;- runoffModelVarIds(modelId)\nprint(gr4jModelVars)\n##  [1] \"P\"          \"E\"          \"runoff\"     \"S\"          \"R\"         \n##  [6] \"Ps\"         \"Es\"         \"Pr\"         \"ech1\"       \"ech2\"      \n## [11] \"Perc\"       \"x1\"         \"x2\"         \"x3\"         \"x4\"        \n## [16] \"UHExponent\" \"PercFactor\"\nelementId &lt;- 'subarea.Subarea'\nmkVarId &lt;- function (shortName) { paste0(elementId, '.', shortName) }\nrunoffId &lt;- mkVarId('runoff')\nsVarId &lt;- mkVarId('S')\nrVarId &lt;- mkVarId('R')\nrecordState(ms, c(runoffId, sVarId, rVarId))\nWe’ll set up a short runtime span, so that we illustrate the state initialisation feature.\nobsRunoff &lt;- sampleSeries('MMH', 'flow') #actually, this is a time series of runoff depth, not streamflow rate\nobsRunoff[which(obsRunoff &lt; -1)] &lt;- NA\ns &lt;- start(obsRunoff)\nw &lt;- s\ne &lt;- s + lubridate::days(90)\nsetSimulationSpan(ms, s, e)\nLet’s apply some default model parameters to the model:\n(pSpecGr4j &lt;- joki::getFreeParams(modelId))\n##   Name      Value Min  Max\n## 1   x1 650.488000   1 3000\n## 2   x2  -0.280648 -27   27\n## 3   x3   7.891230   1  660\n## 4   x4  18.917200   1  240\npSpecGr4j$Name &lt;- mkVarId(pSpecGr4j$Name)\n# TODO : a print function for native parameterizers.\np &lt;- createParameterizer(type='Generic', pSpecGr4j)\nparameterizerAsDataFrame(p)\n##                 Name Min  Max      Value\n## 1 subarea.Subarea.x1   1 3000 650.488000\n## 2 subarea.Subarea.x2 -27   27  -0.280648\n## 3 subarea.Subarea.x3   1  660   7.891230\n## 4 subarea.Subarea.x4   1  240  18.917200\napplySysConfig(p, ms)\nWe get a time series of S if we run it at this point; the starting value is zero.\nexecSimulation(ms)\nplot(getRecorded(ms, sVarId), main='GR4J S store. No state initializer')\n\nLet’s define S0 and R0 parameters such that for each GR4J model instance, S = S0 * x1 and R = R0 * x3\npStates &lt;- linearParameterizer(\n                      c(\"S0\",\"R0\"), \n                      c(\"S\",\"R\"), \n                      c(\"x1\",\"x3\"),\n                      c(0.0,0.0), \n                      c(1.0,1.0), \n                      c(0.9,0.9), \n                      'each subarea')\nIf one applies this parameterizer pState to the system, the the S store is set to the expected value relative to x1.\napplySysConfig(pStates, ms)\ngetStateValue(ms, sVarId)\n## subarea.Subarea.S \n##          585.4392\nHowever this is not enough to define a parameterizer as an initial state. If executing the simulation, the time series of S still starts at zero, because the resetting the model overrides the state S:\nexecSimulation(ms)\nplot(getRecorded(ms, sVarId), main='GR4J S store; incomplete store initialization')\n\nYou need to define a new parameterizer, that makes sure that the model is reset to the expected initial value.\ninitParameterizer &lt;- makeStateInitParameterizer(pStates)\napplySysConfig(initParameterizer, ms)\nexecSimulation(ms)\nplot(getRecorded(ms, sVarId), main='GR4J S store, with a proper state initializer')\n\nThere is logic in keeping the two previous steps in defining a parameterizer as separate, hence this present vignette emphasizes the importance of these two steps.\nOnce you have defined this parameterizer using {r eval=FALSE} makeStateInitParameterizer, you can define a calibration objective the usual way. This vignette does not include calibration steps; please refer to other vignettes.\np &lt;- concatenateParameterizers(p, initParameterizer)\nparameterizerAsDataFrame(p)\n##                 Name Min  Max      Value\n## 1 subarea.Subarea.x1   1 3000 650.488000\n## 2 subarea.Subarea.x2 -27   27  -0.280648\n## 3 subarea.Subarea.x3   1  660   7.891230\n## 4 subarea.Subarea.x4   1  240  18.917200\n## 5                 R0   0    1   0.900000\n## 6                 S0   0    1   0.900000\nobjective &lt;- createObjective(ms, runoffId, obsRunoff, 'NSE', w, e)\nscore &lt;- getScore(objective, p)\nprint(score)\n## $scores\n##       NSE \n## -5.894663 \n## \n## $sysconfig\n##                 Name Min  Max      Value\n## 1 subarea.Subarea.x1   1 3000 650.488000\n## 2 subarea.Subarea.x2 -27   27  -0.280648\n## 3 subarea.Subarea.x3   1  660   7.891230\n## 4 subarea.Subarea.x4   1  240  18.917200\n## 5                 R0   0    1   0.900000\n## 6                 S0   0    1   0.900000",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Calibration with initial model memory states as parameters"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/meta_parameters/meta_parameters.html",
    "href": "doc/samples/R/vignettes/meta_parameters/meta_parameters.html",
    "title": "Calibrating tied meta parameters",
    "section": "",
    "text": "Calibrating tied meta parameters\nJean-Michel Perraud 2020-01-28\n\n\nSample code to define meta parameter sets over a catchment\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:54:59. It illustrates how to set up a calibration where a global parameterization is set at the catchment level, with scaled values for each subareas. This method helps to keep the degrees of freedom of an optimisation to a minimum.\n\n\nGetting started\nlibrary(swift)\nWe need to adjust the observed flow, as the SWIFTv1 legacy missing value code is not consistent with default handling in SAK. Note that I think the flow included in the sample data is the Ovens catchment outlet, but I am not sure!\nflow &lt;- sampleSeries('Abbeyard', 'flow')\nflow[which(flow &lt; -1)] &lt;- NA\nWe create a system with areas similar to the real use case, but do note that test catchment structure is just an arbitrary default one, suitable for this example, but probably not a valid model.\nareasKm2 &lt;- c(91.2627, 95.8716, 6.5610, 128.4822, 93.0042)\nmss &lt;- createTestCatchmentStructure(areasKm2 = areasKm2)\nms &lt;- mss$model\nms &lt;- swapModel(ms, 'Muskingum', 'channel_routing')\nWe will run over a few years and calibrate with a warmup of two years.\ne &lt;- end(flow) - lubridate::ddays(2)\nw &lt;- e - lubridate::dyears(10)\ns &lt;- w - lubridate::dyears(2)\nms &lt;- configureTestSimulation(ms, dataId = \"Ovens\", simulStart = s, \n    simulEnd = e, tstep = \"hourly\", \n    varNameRain = \"P\", varNamePet = \"E\",\n    varNameDataRain = 'rain', varNameDataPet = 'evap') \nThe package includes a function that flags possible inconsistencies prior to running a model (inconsistent time steps, etc.)\ncheckSimulation(ms)\n## $errors\n## character(0)\nWe need to adjust a couple of parameters for proper operation on hourly data for the GR4 model structure.\npGr4jHourly &lt;- createGr4jhParameters()\nparameterizerAsDataFrame(pGr4jHourly)\n##         Name  Min  Max Value\n## 1 PercFactor 4.00 4.00  4.00\n## 2 UHExponent 1.25 1.25  1.25\napplySysConfig(pGr4jHourly, ms)\nWe now define a meta parameter set with area scaling applied to x4 and time scaling applied to x2 and x3.\nrefArea &lt;- 250\ntimeSpan &lt;- as.integer(lubridate::dhours(1))\np &lt;- gr4jScaledParameterizer(refArea, timeSpan)\n(pSpecGr4j &lt;- joki::getFreeParams('GR4J'))\n##   Name      Value Min  Max\n## 1   x1 650.488000   1 3000\n## 2   x2  -0.280648 -27   27\n## 3   x3   7.891230   1  660\n## 4   x4  18.917200   1  240\npSpecGr4j$Min &lt;- c(1.0E+00, -2.70E+01, 1.0E+00, 1.0E+00)\npSpecGr4j$Max &lt;- c(5.0E+03,  2.70E+01, 6.6E+02, 2.4E+02)\nsetHyperCube(p, pSpecGr4j)\n\np &lt;- wrapTransform(p)\naddTransform(p, 'log_x4', 'x4', 'log10')\nWe can inspect the values of one of the subareas to check that the parameter values applied are indeed scaled. For instance x4 is scaled based on the area\nparameterizerAsDataFrame(p)\n##     Name Min         Max      Value\n## 1 log_x4   0    2.380211   1.276857\n## 2     x2 -27   27.000000  -0.280648\n## 3     x3   1  660.000000   7.891230\n## 4     x1   1 5000.000000 650.488000\nsubareaIds &lt;- paste('subarea', getSubareaIds(ms), sep='.')\nareas &lt;- getStateValue(ms, paste(subareaIds, 'areaKm2', sep='.') )\nx4ParamIds &lt;- paste(subareaIds, 'x4', sep='.')\ngetStateValue(ms, x4ParamIds)\n## subarea.lnk1.x4 subarea.lnk2.x4 subarea.lnk3.x4 subarea.lnk4.x4 \n##             0.5             0.5             0.5             0.5 \n## subarea.lnk5.x4 \n##             0.5\napplySysConfig(p, ms)\ngetStateValue(ms, x4ParamIds)\n## subarea.lnk1.x4 subarea.lnk2.x4 subarea.lnk3.x4 subarea.lnk4.x4 \n##       11.429665       11.714718        3.064586       13.561519 \n## subarea.lnk5.x4 \n##       11.538202\nBuild the definition of the optimisation task. TODO: improve ways to search for element keys by element names.\noutflowVarname &lt;- \"Catchment.StreamflowRate\"\nrecordState(ms, outflowVarname)\nexecSimulation(ms)\nlibrary(lubridate)\ncalc &lt;- getRecorded(ms, outflowVarname)\njoki::plotTwoSeries(flow, calc, startTime=end(flow)-lubridate::years(3), endTime=end(flow))\n\nobjective &lt;- createObjective(ms, outflowVarname, flow, 'NSE', w, e)\nscore &lt;- getScore(objective, p)\nprint(score)\n## $scores\n##       NSE \n## -3.536795 \n## \n## $sysconfig\n##     Name Min         Max      Value\n## 1 log_x4   0    2.380211   1.276857\n## 2     x2 -27   27.000000  -0.280648\n## 3     x3   1  660.000000   7.891230\n## 4     x1   1 5000.000000 650.488000\nWe have our objectives defined, and the parameter space ‘p’ in which to search. Let’s create an optimizer and we are ready to go. While the optimizer can be created in one line, we show how to choose one custom termination criterion and how to configure the optimizer to capture a detailed log of the process.\nif(Sys.getenv('SWIFT_FULL') != \"\") {\n  maxHours = 0.2 \n} else {\n  maxHours = 0.02\n}\n# term &lt;- getMarginalTermination(tolerance = 1e-05, cutoffNoImprovement = 30, maxHours = maxHours) \nterm &lt;- getMaxRuntimeTermination(maxHours = maxHours) \nsceParams &lt;- getDefaultSceParameters()\nurs &lt;- createParameterSampler(0, p, 'urs')\noptimizer &lt;- createSceOptimSwift(objective, term, SCEpars=sceParams, urs)\ncalibLogger &lt;- setCalibrationLogger(optimizer, '')\nAt this point you may want to specify the maximum number of cores that can be used by the optimizer, for instance if you wish to keep one core free to work in parallel on something else.\n# TODO add an API entry point for SetMaxDegreeOfParallelismHardwareMinus\nstartTime &lt;- lubridate::now()\ncalibResults &lt;- executeOptimization(optimizer)\nendTime &lt;- lubridate::now()\ncalibWallTime &lt;- endTime-startTime\nprint(paste( 'Optimization completed in ', calibWallTime, attr(calibWallTime, 'units')))\n## [1] \"Optimization completed in  1.20513858397802 mins\"\nProcessing the calibration log:\nd &lt;- getLoggerContent(optimizer)\nd$PointNumber = 1:nrow(d)\nlogMh &lt;- mhplot::mkOptimLog(d, fitness = \"NSE\", messages = \"Message\", categories = \"Category\") \ngeomOps &lt;- mhplot::subsetByMessage(logMh)\nstr(geomOps@data)\n## 'data.frame':    1198 obs. of  9 variables:\n##  $ Category      : Factor w/ 7 levels \"Complex No 0\",..: 7 7 7 7 7 7 7 7 7 7 ...\n##  $ CurrentShuffle: Factor w/ 16 levels \"\",\"0\",\"1\",\"10\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Message       : Factor w/ 5 levels \"Adding a random point in hypercube\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ NSE           : num  -1.20e+04 1.31e-01 -1.29e-04 -1.02 -3.22e+03 ...\n##  $ log_x4        : num  2.256 0.414 0.509 1.724 2.08 ...\n##  $ x1            : num  3366 1996 377 4325 1731 ...\n##  $ x2            : num  20.7 -6.55 -6.36 5.99 25.78 ...\n##  $ x3            : num  41.4 469 82.6 614.2 83.7 ...\n##  $ PointNumber   : int  1 2 3 4 5 6 7 8 9 10 ...\nWe can then visualize how the calibration evolved. There are several types of visualisations included in the mhplot package, and numerous customizations possible, but starting with the overall population evolution:\npVarIds &lt;- (parameterizerAsDataFrame(p))$Name\nfor (pVarId in pVarIds) {\n    print(mhplot::plotParamEvolution(geomOps, pVarId, objLims=c(0,1)))\n}\n\nsortedResults &lt;- sortByScore(calibResults, 'NSE')\nbestPset &lt;- getScoreAtIndex(sortedResults, 1)\nbestPset &lt;- GetSystemConfigurationWila_R(bestPset)\nswift can back-transform a parameters to obtain the untransformed parameter set(s):\nuntfPset &lt;- backtransform(bestPset)\n(score &lt;- getScore(objective, bestPset))\n## $scores\n##       NSE \n## 0.6221243 \n## \n## $sysconfig\n##     Name Min         Max      Value\n## 1 log_x4   0    2.380211   1.488763\n## 2     x2 -27   27.000000 -12.748777\n## 3     x3   1  660.000000 147.210803\n## 4     x1   1 5000.000000 548.429824\n(score &lt;- getScore(objective, untfPset))\n## $scores\n##       NSE \n## 0.6221243 \n## \n## $sysconfig\n##   Name Min  Max     Value\n## 1   x2 -27   27 -12.74878\n## 2   x3   1  660 147.21080\n## 3   x4   1  240  30.81507\n## 4   x1   1 5000 548.42982\nFinally, let’s have a visual of the fitted streamflow data at Abbeyard:\napplySysConfig(bestPset, ms)\nexecSimulation(ms)\nmodRunoff &lt;- getRecorded(ms, outflowVarname)\njoki::plotTwoSeries(flow, modRunoff, startTime=end(modRunoff)-lubridate::years(3), endTime=end(modRunoff))",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Calibrating tied meta parameters"
    ]
  },
  {
    "objectID": "doc/samples/sample_workflows_python.html",
    "href": "doc/samples/sample_workflows_python.html",
    "title": "Sample modelling workflows - Python",
    "section": "",
    "text": "Sample modelling workflows - Python\nThe documentation for the Python package swift2 includes jupyter notebooks (external site).",
    "crumbs": [
      "Documentation",
      "Examples in Python"
    ]
  },
  {
    "objectID": "doc/notes.html",
    "href": "doc/notes.html",
    "title": "Howto",
    "section": "",
    "text": "# cd ~/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes\n# R\n\nsetwd('C:/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes')\n\nlibrary(rmarkdown)\n\ninfn &lt;- c(\n\"calibrate_multisite\"         ,    \"calibrate_subcatchments\"        ,\n\"calibration_initial_states\"  ,    \"ensemble_forecast_model_runs\"   ,\n\"ensemble_model_runs\"         ,    \"error_correction_four_stages\"   ,\n\"getting_started\"             ,    \"log_likelihood\"                 ,\n\"meta_parameters\"             ,    \"muskingum_multilink_calibration\"\n)\n\n\nf &lt;- function(fn)\n{\n    input &lt;- paste(fn, 'Rmd', sep='.')\n    output_format &lt;- 'github_document'\n\n    output_dir &lt;- fn\n\n    rmarkdown::render(input, output_format, output_file = NULL, output_dir,\n                output_options = NULL, intermediates_dir = NULL,\n                knit_root_dir = NULL,\n                runtime = c(\"auto\", \"static\", \"shiny\", \"shiny_prerendered\"),\n                clean = TRUE, params = NULL, knit_meta = NULL,\n                run_pandoc = TRUE, quiet = FALSE)\n}\n\nlapply(infn, FUN=f)\n\n\nfile.remove(list.files('.', pattern=\"*.html\", full.names=TRUE, recursive=TRUE))\nfile.copy(infn, '~/src/github_jm/streamflow-forecasting-tools-onboard/doc/vignettes/', recursive=TRUE)\n\nfile.copy(infn, 'c:/src/github_jm/streamflow-forecasting-tools-onboard/doc/vignettes/', recursive=TRUE)\ngiven what I get from this issue I am not sure it is possible to get relative paths to figures in the markdown documents. Have to use full text search/replace to correct.\nNote the regex pattern to use:\n/home/xxxxxx/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes/[a-z_]*/\nC:/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes/[a-z_]*/\nand needs to be replaced with ./"
  },
  {
    "objectID": "doc/notes.html#generating-vignettes-output-as-markdown",
    "href": "doc/notes.html#generating-vignettes-output-as-markdown",
    "title": "Howto",
    "section": "",
    "text": "# cd ~/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes\n# R\n\nsetwd('C:/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes')\n\nlibrary(rmarkdown)\n\ninfn &lt;- c(\n\"calibrate_multisite\"         ,    \"calibrate_subcatchments\"        ,\n\"calibration_initial_states\"  ,    \"ensemble_forecast_model_runs\"   ,\n\"ensemble_model_runs\"         ,    \"error_correction_four_stages\"   ,\n\"getting_started\"             ,    \"log_likelihood\"                 ,\n\"meta_parameters\"             ,    \"muskingum_multilink_calibration\"\n)\n\n\nf &lt;- function(fn)\n{\n    input &lt;- paste(fn, 'Rmd', sep='.')\n    output_format &lt;- 'github_document'\n\n    output_dir &lt;- fn\n\n    rmarkdown::render(input, output_format, output_file = NULL, output_dir,\n                output_options = NULL, intermediates_dir = NULL,\n                knit_root_dir = NULL,\n                runtime = c(\"auto\", \"static\", \"shiny\", \"shiny_prerendered\"),\n                clean = TRUE, params = NULL, knit_meta = NULL,\n                run_pandoc = TRUE, quiet = FALSE)\n}\n\nlapply(infn, FUN=f)\n\n\nfile.remove(list.files('.', pattern=\"*.html\", full.names=TRUE, recursive=TRUE))\nfile.copy(infn, '~/src/github_jm/streamflow-forecasting-tools-onboard/doc/vignettes/', recursive=TRUE)\n\nfile.copy(infn, 'c:/src/github_jm/streamflow-forecasting-tools-onboard/doc/vignettes/', recursive=TRUE)\ngiven what I get from this issue I am not sure it is possible to get relative paths to figures in the markdown documents. Have to use full text search/replace to correct.\nNote the regex pattern to use:\n/home/xxxxxx/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes/[a-z_]*/\nC:/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes/[a-z_]*/\nand needs to be replaced with ./"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "Software packages are not all publicly downloadable, but access for evaluation or research purposes can easily be arranged upon request. Contact jean-michel.perraud@csiro.au and/or david.robertson@csiro.au. Once you have received a link to downloadable packages and files, follow the instructions for your platform:\n\nInstalling on Window\nInstalling on Linux\nInstalling on MacOSX\n\nWe intend to also provide in the future a docker container",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Streamflow Forecasting",
    "section": "",
    "text": "This is a public entry point for a suite of hydrologic ensemble forecasting modelling tools developed in Australia over the past decade. Most scientific modelling features were authored by the CSIRO. While designed with unique features for ensemble streamflow forecasting, these tools can equally be used in non-ensemble simulation mode for other, “traditional” semi-distributed hydrologic modelling.\n\nApplications\nSome of the modelling tools are used by the Australian Bureau of Meteorology (the Bureau) to provide the 7-day Ensemble Streamflow Forecasts service. An overview of the research and outcomes in streamflow forecasting of the research alliance between the Bureau and CSIRO is available at this page.\nMore recent versions of the tools are also used for further research, development and applications. The core modelling features are usually native (C++) libraries, accessed seamlessly by user from Python, R and Matlab.\n\n\n\nEnsemble forecasting simulation run from an R script\n\n\nThe documentation page has links to sample modelling workflows in R and Python that hydrologic modellers can browse through to get an idea of the features.\n\n\nInstallation\nInstallation packages are available for Windows, Debian/Ubuntu Linux, and MacOSX. They currently cannot all be made publicly downloadable, but access for evaluation or research purposes can be arranged. Contact David Robertson at david.robertson@csiro.au and/or jean-michel.perraud@csiro.au. Then follow the instructions at the Installation page.\n\n\nArchitecture\nThe core of the toolset is written in C++ for execution speed, but in practice users mostly access the features via packages in R, python, or Matlab.\n\n\n\nHigh-level software stack\n\n\nBesides building upon established third party open source software, some components of our stack are also open source. See csiro-hydroinformatics on GitHub, for instance the uchronia time series handling module.\nThe developer page also contains more detailed technical information on the software stack.\n\n\nAcknowledgements\nMost of the tools covered by this documentation is the output of research supported by the Water Information Research and Development Alliance between CSIRO and the Australian Bureau of Meteorology.\n\n\nPublications\nSWIFT2: High performance software for short-medium term ensemble streamflow forecasting research and operations\nSWIFT2: Advanced software for continuous ensemble short-term streamflow forecasting"
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Pointers to further technical documentation for using or developing features.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation.html#packages-documentations",
    "href": "documentation.html#packages-documentations",
    "title": "Documentation",
    "section": "Packages documentations",
    "text": "Packages documentations\nThe python package swift2 is the latest (from ~2020) implementation of python access arrangements to the “swift2” engine for semi-distributed streamflow modelling and forecasting. The uchronia time series API documentation combines the documentation of the python package uchronia with its C++ core. Other packages include “docstrings” available to users for reference, but are not yet published on the Web.\nThe R packages in the software distribution contain function documentation, example code snippets and so-called “vignettes”. These are not yet published via web sites. You can access vignettes in the packages for instance with the following statements in an R terminal:\nlibrary(swift)\nlibrary(uchronia)\nbrowseVignettes('swift')\nYou can access the wider package documentation using the command ?swift, and see an index of the features by navigating down the page to the footer [Package swift version 2.4.x Index]. Click on the Index hyperlink of that footer. Note that you will find a very long list of functions, but note that functions postfixed _R should not be used as primary building blocks in your scripts.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "documentation.html#build-pipelines",
    "href": "documentation.html#build-pipelines",
    "title": "Documentation",
    "section": "Build pipelines",
    "text": "Build pipelines\nSoftware build pipelines have been devised to streamline the compilation, testing and packaging of software artifacts. See build workflows for details.",
    "crumbs": [
      "Documentation"
    ]
  },
  {
    "objectID": "doc/samples/sample_workflows_matlab.html",
    "href": "doc/samples/sample_workflows_matlab.html",
    "title": "Sample modelling workflows - MATLAB",
    "section": "",
    "text": "Note\n\n\n\nThis is only a basic workflow for illustrative purposes, not an extensive reference document, nor even a polished “teaser”. Much work was done by the Matlab user base, but not (yet) published for an external audience.",
    "crumbs": [
      "Documentation",
      "Examples in Matlab"
    ]
  },
  {
    "objectID": "doc/samples/sample_workflows_matlab.html#path-setup",
    "href": "doc/samples/sample_workflows_matlab.html#path-setup",
    "title": "Sample modelling workflows - MATLAB",
    "section": "Path setup",
    "text": "Path setup\n% Matlab SWIFT getting started vignette (last updated March 2018)\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% Define the paths to the directories containing the SWIFT-Matlab functions\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n% We need to define a few locations \n% In order to load the swift native library, matlab needs a C/C++ compiler. \n% Matlab probably locates Visual Studio C/C++ compilers if you have installed them. \n% if using mingw on windows you may need to set up an environment variable such as:\n\n% On a Windows machine:\n% Mapping drive from my laptop to bragg-w for convenience:\nsetenv('MW_MINGW64_LOC', 'C:/Users/username/prog/mingw-w64/x86_64-5.3.0-posix-seh-rt_v4-rev0/mingw64');\nbindingsLocation  = 'C:/Users/username/local/matlab'\nlocalDir = 'C:/Users/username/local';\nlibs_dir = fullfile(localDir, 'libs/64');\nFor illustration, there are a few steps that would be expected on compute clusters, for instance Linux based:\n%%%%%%%%%\n% % Linux cluster additional steps\n%%%%%%%%%\n% % on a linux machine such as a Linux cluster:\n% bindingsLocation = '/data/username/src/stash/swift/bindings/matlab';\n% libs_dir = '/data/projects/hydroforecast/usr/local/lib';\n% % on a Linux cluster due to Matlab coming coming with its own netcdf.so, which is problematic:\n% prevenv = getenv('LD_LIBRARY_PATH')\n% newenv = strcat('/apps/netcdf/4.3.3.1/lib', ':', prevenv);\n% setenv('LD_LIBRARY_PATH', newenv);\n% % And we also need the following to really get rid of matlabs' own:\n% loadlibrary('libnetcdf', '/apps/netcdf/4.3.3.1/include/netcdf.h')\nFinally, specifyint to Matlab where matlab functions (refered to as bindings earlier) can be found:\naddpath(fullfile(bindingsLocation, 'datatypes'));\naddpath(fullfile(bindingsLocation, 'native'));\naddpath(fullfile(bindingsLocation, 'interop'));\naddpath(fullfile(bindingsLocation, 'external'));",
    "crumbs": [
      "Documentation",
      "Examples in Matlab"
    ]
  },
  {
    "objectID": "doc/samples/sample_workflows_matlab.html#modelling-workflow",
    "href": "doc/samples/sample_workflows_matlab.html#modelling-workflow",
    "title": "Sample modelling workflows - MATLAB",
    "section": "Modelling workflow",
    "text": "Modelling workflow\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% Basic modelling workflow\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n%% Load the native libraries for 'swift'  and 'datatypes'\ninitSwift(libs_dir)\n% initSwift('C:/Users/username/local/libs/64');\n\n%% SIMULATION\ncd('C:/Users/username/tmp');\ncd('/data/username/tmp');\n% Define the simulation period\nsimulationStart = datetime(1990, 1, 1);\nsimulationEnd = datetime(2005, 12, 31);\n\n% Read in time series data from disk for rain and evap\n\ntsRainData = transpose(csvread(strcat(bindingsLocation, '/tutorials/getting_started/data/rain.csv')));\ntsRain = SimpleTimeSeries(simulationStart, 'daily', tsRainData, 'rain');\n\ntsEvapData = transpose(csvread(strcat(bindingsLocation, '/tutorials/getting_started/data/evap.csv')));\ntsEvap = SimpleTimeSeries(simulationStart, 'daily', tsEvapData, 'evap');\n\n% Create a simple one subarea model simulation using GR4J\n\nms = createCatchment('GR4J', {'node1','node2'},{'node 1','node 2'},{'link1'},{'link 1'},{'node1'},{'node2'},1.0);\n\nsetSimulationTimeStep(ms, 'daily');\nsetSimulationSpan(ms, simulationStart, simulationEnd);\n\n% Play rain and evap into model simulation\n\nplayInput(ms, {'subarea.link1.P'}, tsRainData, simulationStart, 'daily');\nplayInput(ms, {'subarea.link1.E'}, tsEvapData, simulationStart, 'daily');\n\n% Query the model simulation\n\nrunoffModelIds = runoffModelIds();\n\ngr4jModelVars = runoffModelVarIds('GR4J');\n\nsubareaIds = getSubareaIds(ms);\n\nx4 = getStateValue(ms, {'subarea.link1.x4'});\n\n% Record runoff\n\nrecordState(ms, {'subarea.link1.runoff'});\n\nrecordedVarNames = getRecordedVarNames(ms);\n\n% Execute the model simulation\n\nexecSimulation(ms);\n\n% Retrieve the recorded time series data\n\ntsRunoffData = getRecorded(ms, 'subarea.link1.runoff');\ntsRunoff = SimpleTimeSeries(simulationStart, 'daily', tsRunoffData, 'runoff');\n\n% CALIBRATION\n\n% Read in time series data from disk for flow\n\ntsFlowData = transpose(csvread(strcat(bindingsLocation, '/tutorials/getting_started/data/flow.csv')));\ntsFlow = SimpleTimeSeries(simulationStart, 'daily', tsFlowData, 'flow');\n\n% Replace no value flag values with nan\n\ntsFlowData(tsFlowData &lt; -1) = nan;\n\ncalibrationStart = datetime(1992, 1, 1);\n\nsetSimulationSpan(ms, calibrationStart, simulationEnd);\n\nrunoffId = 'subarea.link1.runoff';\n\nobjective = createObjective(ms, runoffId, tsFlow, 'NSE', calibrationStart, simulationEnd);\n\n% Note that as of March 2018, the matlab bindings will retrieve error\n% messages from the native library (instead of crashing...) on incorrect\n% calls such as:\n% execSimulation(objective)\n\n% Configure parameter bounds for our GR4J calibration\n\npSpecGr4j = ParameterSet();\n\npSpecGr4j.AddParameter('subarea.link1.x1', 1, 1000, 542.1981111);\npSpecGr4j.AddParameter('subarea.link1.x2', -30, 30, -0.4127542);\npSpecGr4j.AddParameter('subarea.link1.x3', 1, 1000, 7.7403390);\npSpecGr4j.AddParameter('subarea.link1.x4', 1, 240, 1.2388548);\n\n% Create a parameterizer\n\np = createParameterizer('generic', pSpecGr4j);\n\nscore = getScore(objective, p);\n\n% Create a max runtime termination criteria\n\nterm = getMaxRuntimeTermination(0.005);\n\n% Create a default SCE parameter structure\n\nsceParams = getDefaultSceParameters();\n\n% Create a population initializer\n\nurs = createParameterSampler(0, p, 'urs');\n\n% Construct an optimizer\n\noptimizer = createSceOptimSwift(objective, term, sceParams, urs);\n\n% Setup the calibration logger\n\nsetCalibrationLogger(optimizer, '');\n\n% Calibrate the model\n\ncalibResults = executeOptimization(optimizer);\n\n% Display the best objective score\n\nbestResult = calibResults{1,2};\ndisp(bestResult);\n\n% Load the calibration log data in matlab\n\nlogData = loadCalibrationLogger(optimizer);\n\n% Plot the evolution of all parameters\n\nplotAllParameterEvolutions(logData);",
    "crumbs": [
      "Documentation",
      "Examples in Matlab"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html",
    "href": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html",
    "title": "Error correction - ERRIS",
    "section": "",
    "text": "Jean-Michel Perraud 2020-01-28",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Error correction - ERRIS"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html#about-this-document",
    "href": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html#about-this-document",
    "title": "Error correction - ERRIS",
    "section": "About this document",
    "text": "About this document\nThis document was generated from an R markdown file on 2020-01-28 10:52:38.\nLi, Ming; Wang, QJ; Bennett, James; Robertson, David. Error reduction and representation in stages (ERRIS) in hydrological modelling for ensemble streamflow forecasting. Hydrology and Earth System Sciences. 2016; 20:3561-3579. https://doi.org/10.5194/hess-20-3561-2016",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Error correction - ERRIS"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html#calibrating-erris",
    "href": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html#calibrating-erris",
    "title": "Error correction - ERRIS",
    "section": "Calibrating ERRIS",
    "text": "Calibrating ERRIS\n\nModel structure\nWe use sample hourly data from the Adelaide catchment this catchment in the Northern Territory, TBC. The catchment model set up is not the key point of this vignette so we do not comment on that section:\nlibrary(swift)\ncatchmentStructure &lt;- sampleCatchmentModel(siteId = \"Adelaide\", configId = \"catchment\")\n\nhydromodel &lt;- \"GR4J\";\nchannel_routing &lt;- 'MuskingumNonLinear';\nhydroModelRainfallId &lt;-'P'\nhydroModelEvapId &lt;-'E'\n\n# set models\ninsimulation &lt;- swapModel(catchmentStructure, modelId = hydromodel ,what = \"runoff\")\nsimulation &lt;- swapModel(insimulation, modelId = channel_routing ,what = \"channel_routing\")\n\nsaId &lt;- getSubareaIds(simulation)\nstopifnot(length(saId) == 1)\n\nprecipTs &lt;- sampleSeries(siteId = \"Adelaide\", varName = \"rain\")\nevapTs &lt;- sampleSeries(siteId = \"Adelaide\", varName = \"evap\")\nflowRateTs &lt;- sampleSeries(siteId = \"Adelaide\", varName = \"flow\")\n\nplayInput(simulation, precipTs, mkFullDataId('subarea', saId, hydroModelRainfallId))\nplayInput(simulation, evapTs, mkFullDataId('subarea', saId, hydroModelEvapId))\nconfigureHourlyGr4j(simulation)\nsetSimulationTimeStep(simulation, 'hourly')\n\n# Small time interval only, to reduce runtimes in this vignette\nsimstart &lt;- uchronia::mkDate(2010,12,1)  \nsimend &lt;- uchronia::mkDate(2011,6,30,23)  \nsimwarmup &lt;- simstart\nsetSimulationSpan(simulation, simstart, simend)\ntemplateHydroParameterizer &lt;- function(simulation) {\n  calibragem::defineParameterizerGr4jMuskingum(refArea=250, timeSpan=3600L, simulation=simulation, paramNameK=\"Alpha\", objfun='NSE', deltaT=1L)\n}\n\nnodeId &lt;- 'node.2'\nflowId &lt;- mkFullDataId(nodeId, 'OutflowRate')\n\nrecordState(simulation, flowId)\nWe use pre-calibrated hydrologic parameters (reproducible with doc/error_correction_doc_preparation.r in this package structure)\np &lt;- templateHydroParameterizer(simulation)\nsetMinParameterValue(p, 'R0', 0.0)\nsetMaxParameterValue(p, 'R0', 1.0)\nsetMinParameterValue(p, 'S0', 0.0)\nsetMaxParameterValue(p, 'S0', 1.0)\nSetParameterValue_R( p, 'log_x4', 1.017730e+00)\nSetParameterValue_R( p, 'log_x1', 2.071974e+00  )\nSetParameterValue_R( p, 'log_x3', 1.797909e+00  )\nSetParameterValue_R( p, 'asinh_x2', -1.653842e+00)  \nSetParameterValue_R( p, 'R0', 2.201930e-11  )\nSetParameterValue_R( p, 'S0', 3.104968e-11  )\nSetParameterValue_R( p, 'X', 6.595537e-03   ) # Gotcha: needs to be set before alpha is changed.\nSetParameterValue_R( p, 'Alpha', 6.670534e-01   )\nparameterizerAsDataFrame(p)\n##       Name         Min        Max         Value\n## 1   log_x4  0.00000000 2.38021124  1.017730e+00\n## 2   log_x1  0.00000000 3.77815125  2.071974e+00\n## 3   log_x3  0.00000000 3.00000000  1.797909e+00\n## 4 asinh_x2 -3.98932681 3.98932681 -1.653842e+00\n## 5       R0  0.00000000 1.00000000  2.201930e-11\n## 6       S0  0.00000000 1.00000000  3.104968e-11\n## 7        X  0.00100000 0.01662228  6.595537e-03\n## 8    Alpha  0.01116157 1.68112917  6.670534e-01\nsViz &lt;- uchronia::mkDate(2010,12,1)\neViz &lt;- uchronia::mkDate(2011,4,30,23)\n\noneWetSeason &lt;- function(tts) {\n    window(tts, start=sViz, end=eViz) \n}\n\nobsVsCalc &lt;- function(obs, calc, ylab=\"flow (m3/s)\") {\n  obs &lt;- oneWetSeason(obs)\n  calc &lt;- oneWetSeason(calc)\n    joki::plotTwoSeries(obs, calc, ylab=ylab, startTime = start(calc), endTime = end(calc))\n}\n\napplySysConfig(p, simulation)\nexecSimulation(simulation)\nobsVsCalc(flowRateTs, getRecorded(simulation, flowId))\n\n\n\nSet up the error correction model\nlist(getNodeIds(simulation), \ngetNodeNames(simulation))\n## [[1]]\n## [1] \"2\" \"1\"\n## \n## [[2]]\n## [1] \"Outlet\" \"Node_1\"\nerrorModelElementId &lt;- 'node.2';\nsetErrorCorrectionModel(simulation, 'ERRIS', errorModelElementId, length=-1, seed=0)\n\nflowRateTsGapped &lt;- flowRateTs\nflowRateTsGapped['2011-02'] &lt;- NA\n\n# plot(flowRateTsGapped)\n\nplayInput(simulation,flowRateTsGapped,varIds=mkFullDataId(errorModelElementId,\"ec\",\"Observation\"))\nNow, prepare a model with error correction, and set up for generation\necs &lt;- swift::CloneModel_R(simulation)\n\nsetStateValue(ecs,mkFullDataId(nodeId,\"ec\",\"Generating\"),FALSE)\nupdatedFlowVarID &lt;- mkFullDataId(nodeId,\"ec\",\"Updated\")\ninputFlowVarID &lt;- mkFullDataId(nodeId,\"ec\",\"Input\")\nrecordState(ecs,varIds=c(updatedFlowVarID, inputFlowVarID))\n\n\nERRIS calibration in stages\n#termination &lt;- getMaxRuntimeTermination(0.005)\ntermination &lt;- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.05','0.0167'))\nWe could set up a four-stages estimation in one go, but we will instead work in each stages for didactic purposes.\ncensOpt = 0.0\nestimator &lt;- createERRISParameterEstimator (simulation, flowRateTs, errorModelElementId,\n                                            estimationStart = simstart, estimationEnd=simend, censThr=0.0, censOpt,\n                                            termination, restrictionOn=TRUE, weightedLeastSquare=FALSE)\n\nstageOnePset = CalibrateERRISStageOne_R(estimator)\nprint(parameterizerAsDataFrame(stageOnePset))\n##              Name    Min    Max       Value\n## 1         Epsilon  -20.0    0.0   -8.676488\n## 2          Lambda  -30.0    5.0   -1.451523\n## 3               D    0.0    0.0    0.000000\n## 4              Mu    0.0    0.0    0.000000\n## 5             Rho    0.0    0.0    0.000000\n## 6  Sigma1_Falling    0.0    0.0    0.000000\n## 7   Sigma1_Rising    0.0    0.0    0.000000\n## 8  Sigma2_Falling    0.0    0.0    0.000000\n## 9   Sigma2_Rising    0.0    0.0    0.000000\n## 10 Weight_Falling    1.0    1.0    1.000000\n## 11  Weight_Rising    1.0    1.0    1.000000\n## 12        CensThr    0.0    0.0    0.000000\n## 13        CensOpt    0.0    0.0    0.000000\n## 14         MaxObs 1126.3 1126.3 1126.300000\n\nStage 2\nStage two can be logged:\nSetERRISVerboseCalibration_R(estimator, TRUE)\n\nstageTwoPset = CalibrateERRISStageTwo_R(estimator, stageOnePset)\nprint(parameterizerAsDataFrame(stageTwoPset))\n##              Name         Min         Max        Value\n## 1               D    0.000000    2.000000    0.7386187\n## 2              Mu -100.000000  100.000000   -3.5225392\n## 3   Sigma1_Rising   -6.907755    6.907755    0.9109061\n## 4         CensOpt    0.000000    0.000000    0.0000000\n## 5         CensThr    0.000000    0.000000    0.0000000\n## 6         Epsilon   -8.676488   -8.676488   -8.6764883\n## 7          Lambda   -1.451523   -1.451523   -1.4515233\n## 8          MaxObs 1126.300000 1126.300000 1126.3000000\n## 9             Rho    0.000000    0.000000    0.0000000\n## 10 Sigma1_Falling    0.000000    0.000000    0.0000000\n## 11 Sigma2_Falling    0.000000    0.000000    0.0000000\n## 12  Sigma2_Rising    0.000000    0.000000    0.0000000\n## 13 Weight_Falling    1.000000    1.000000    1.0000000\n## 14  Weight_Rising    1.000000    1.000000    1.0000000\nmkEcIds &lt;- function(p) {\n  df &lt;- parameterizerAsDataFrame(p)\n  df$Name &lt;- mkFullDataId(nodeId, 'ec', df$Name)\n  createParameterizer('Generic',df)\n}\n\napplySysConfig(mkEcIds(stageTwoPset), ecs)\nexecSimulation(ecs)\nobsVsCalc(flowRateTsGapped, getRecorded(ecs, updatedFlowVarID))\n\nA helper function to process the calibration log:\nprepOptimLog &lt;- function(estimator, fitnessName = \"Log.likelihood\")\n{\n  optimLog = getLoggerContent(estimator)\n  # head(optimLog)\n  optimLog$PointNumber = 1:nrow(optimLog)   \n  logMh &lt;- mhplot::mkOptimLog(optimLog, fitness = fitnessName, messages = \"Message\", categories = \"Category\") \n  geomOps &lt;- mhplot::subsetByMessage(logMh)\n  d &lt;- list(data=logMh, geomOps=geomOps)\n}\nd &lt;- prepOptimLog(estimator, fitnessName = \"Log.likelihood\")\nprint(mhplot::plotParamEvolution(d$geomOps, 'Sigma1_Rising', c(0, max(d$data@data$Log.likelihood))))\n\n\n\nStage 3\nstageThreePset = CalibrateERRISStageThree_R(estimator, stageTwoPset)\nprint(parameterizerAsDataFrame(stageThreePset))\n##              Name          Min          Max        Value\n## 1             Rho    0.0000000    1.0000000    0.9880881\n## 2   Sigma1_Rising   -6.9077553    6.9077553   -0.9134793\n## 3         CensOpt    0.0000000    0.0000000    0.0000000\n## 4         CensThr    0.0000000    0.0000000    0.0000000\n## 5               D    0.7386187    0.7386187    0.7386187\n## 6         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 7          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 8          MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 9              Mu   -3.5225392   -3.5225392   -3.5225392\n## 10 Sigma1_Falling    0.0000000    0.0000000    0.0000000\n## 11 Sigma2_Falling    0.0000000    0.0000000    0.0000000\n## 12  Sigma2_Rising    0.0000000    0.0000000    0.0000000\n## 13 Weight_Falling    1.0000000    1.0000000    1.0000000\n## 14  Weight_Rising    1.0000000    1.0000000    1.0000000\nd &lt;- prepOptimLog(estimator, fitnessName = \"Log.likelihood\")\nprint(mhplot::plotParamEvolution(d$geomOps, 'Rho', c(0, max(d$data@data$Log.likelihood))))\n\n\n\nStage 3a, generating and fitting M and S if free\nstageThreePsetMS = CalibrateERRISStageThreeMS_R(estimator, stageThreePset)\nprint(parameterizerAsDataFrame(stageThreePsetMS))\n##              Name          Min          Max        Value\n## 1             Rho    0.0000000    1.0000000    0.9880881\n## 2   Sigma1_Rising   -6.9077553    6.9077553   -0.9134793\n## 3         CensOpt    0.0000000    0.0000000    0.0000000\n## 4         CensThr    0.0000000    0.0000000    0.0000000\n## 5               D    0.7386187    0.7386187    0.7386187\n## 6         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 7          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 8          MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 9              Mu   -3.5225392   -3.5225392   -3.5225392\n## 10 Sigma1_Falling    0.0000000    0.0000000    0.0000000\n## 11 Sigma2_Falling    0.0000000    0.0000000    0.0000000\n## 12  Sigma2_Rising    0.0000000    0.0000000    0.0000000\n## 13 Weight_Falling    1.0000000    1.0000000    1.0000000\n## 14  Weight_Rising    1.0000000    1.0000000    1.0000000\n## 15         MNoise -100.0000000  100.0000000   -2.5833943\n## 16         SNoise  -10.0000000   10.0000000    1.8858371\napplySysConfig(mkEcIds(stageThreePsetMS), ecs)\nexecSimulation(ecs)\nobsVsCalc(flowRateTsGapped, getRecorded(ecs, updatedFlowVarID))\n\n\n\nStage 4, rising limb\nstageFourPsetRising = CalibrateERRISStageFour_R(estimator, stageThreePsetMS, useRising = TRUE)\nprint(parameterizerAsDataFrame(stageFourPsetRising))\n##              Name          Min          Max        Value\n## 1   Sigma1_Rising   -6.9077553    6.9077553   -1.0822514\n## 2   Sigma2_Rising   -6.9077553    6.9077553    0.6662236\n## 3   Weight_Rising    0.5000000    1.0000000    0.8770708\n## 4         CensOpt    0.0000000    0.0000000    0.0000000\n## 5         CensThr    0.0000000    0.0000000    0.0000000\n## 6               D    0.7386187    0.7386187    0.7386187\n## 7         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 8          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 9          MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 10             Mu   -3.5225392   -3.5225392   -3.5225392\n## 11            Rho    0.9880881    0.9880881    0.9880881\n## 12 Sigma1_Falling    0.0000000    0.0000000    0.0000000\n## 13 Sigma2_Falling    0.0000000    0.0000000    0.0000000\n## 14 Weight_Falling    1.0000000    1.0000000    1.0000000\nd &lt;- prepOptimLog(estimator, fitnessName = \"Log.likelihood\")\nprint(mhplot::plotParamEvolution(d$geomOps, 'Weight_Rising', c(0, max(d$data@data$Log.likelihood))))\n\napplySysConfig(mkEcIds(stageFourPsetRising), ecs)\nexecSimulation(ecs)\nobsVsCalc(flowRateTsGapped, getRecorded(ecs, updatedFlowVarID))\n\n\n\nStage 4, falling limbs\nstageFourPsetFalling = CalibrateERRISStageFour_R(estimator, stageThreePsetMS, useRising = FALSE)\nprint(parameterizerAsDataFrame(stageFourPsetFalling))\n##              Name          Min          Max        Value\n## 1   Sigma1_Rising   -6.9077553    6.9077553   -3.1021521\n## 2   Sigma2_Rising   -6.9077553    6.9077553   -0.6290198\n## 3   Weight_Rising    0.5000000    1.0000000    0.8131939\n## 4         CensOpt    0.0000000    0.0000000    0.0000000\n## 5         CensThr    0.0000000    0.0000000    0.0000000\n## 6               D    0.7386187    0.7386187    0.7386187\n## 7         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 8          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 9          MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 10             Mu   -3.5225392   -3.5225392   -3.5225392\n## 11            Rho    0.9880881    0.9880881    0.9880881\n## 12 Sigma1_Falling    0.0000000    0.0000000    0.0000000\n## 13 Sigma2_Falling    0.0000000    0.0000000    0.0000000\n## 14 Weight_Falling    1.0000000    1.0000000    1.0000000\nNd &lt;- prepOptimLog(estimator, fitnessName = \"Log.likelihood\")\nprint(mhplot::plotParamEvolution(d$geomOps, 'Weight_Rising', c(0, max(d$data@data$Log.likelihood))))\n\n\n\nFinal consolidated parameter set\nfinalPset = ConcatenateERRISStagesParameters_R(estimator, hydroParams = createParameterizer(), stage1_result =  stageOnePset, stage2_result = stageTwoPset, \n                                   stage3_result = stageThreePsetMS, stage4a_result = stageFourPsetRising, stage4b_result = stageFourPsetFalling, toLongParameterName = FALSE)\n\nprint(parameterizerAsDataFrame(finalPset))\n##              Name          Min          Max        Value\n## 1         CensThr    0.0000000    0.0000000    0.0000000\n## 2         CensOpt    0.0000000    0.0000000    0.0000000\n## 3          MNoise -100.0000000  100.0000000   -2.5833943\n## 4          SNoise  -10.0000000   10.0000000    1.8858371\n## 5          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 6         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 7              Mu   -3.5225392   -3.5225392   -3.5225392\n## 8               D    0.7386187    0.7386187    0.7386187\n## 9             Rho    0.9880881    0.9880881    0.9880881\n## 10         MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 11  Sigma1_Rising   -6.9077553    6.9077553   -1.0822514\n## 12  Sigma2_Rising   -6.9077553    6.9077553    0.6662236\n## 13  Weight_Rising    0.5000000    1.0000000    0.8770708\n## 14 Sigma1_Falling   -6.9077553    6.9077553   -3.1021521\n## 15 Sigma2_Falling   -6.9077553    6.9077553   -0.6290198\n## 16 Weight_Falling    0.5000000    1.0000000    0.8131939\n\n\n\nLegacy call\nCheck that the previous “one stop shop” call gives the same results.\ndummyDate &lt;- simstart\n\npsetFullEstimate &lt;- estimateERRISParameters(simulation, flowRateTs, errorModelElementId,\n  warmupStart=dummyDate, warmupEnd=dummyDate, warmup=FALSE, estimationStart = simstart, estimationEnd=simend, censThr=0.0,\n censOpt = censOpt, exclusionStart=dummyDate, exclusionEnd=dummyDate, exclusion=FALSE, terminationCondition = termination,\n  hydroParams = NULL, errisParams = NULL, restrictionOn = TRUE,\n  weightedLeastSquare = FALSE)\n\nprint(parameterizerAsDataFrame(psetFullEstimate))\n##                        Name          Min          Max        Value\n## 1         node.2.ec.CensThr    0.0000000    0.0000000    0.0000000\n## 2         node.2.ec.CensOpt    0.0000000    0.0000000    0.0000000\n## 3          node.2.ec.MNoise -100.0000000  100.0000000   -2.5833943\n## 4          node.2.ec.SNoise  -10.0000000   10.0000000    1.8858371\n## 5          node.2.ec.Lambda   -1.4515233   -1.4515233   -1.4515233\n## 6         node.2.ec.Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 7              node.2.ec.Mu   -3.5225392   -3.5225392   -3.5225392\n## 8               node.2.ec.D    0.7386187    0.7386187    0.7386187\n## 9             node.2.ec.Rho    0.9880881    0.9880881    0.9880881\n## 10         node.2.ec.MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 11  node.2.ec.Sigma1_Rising   -6.9077553    6.9077553   -1.0822514\n## 12  node.2.ec.Sigma2_Rising   -6.9077553    6.9077553    0.6662236\n## 13  node.2.ec.Weight_Rising    0.5000000    1.0000000    0.8770708\n## 14 node.2.ec.Sigma1_Falling   -6.9077553    6.9077553   -3.1021521\n## 15 node.2.ec.Sigma2_Falling   -6.9077553    6.9077553   -0.6290198\n## 16 node.2.ec.Weight_Falling    0.5000000    1.0000000    0.8131939",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Error correction - ERRIS"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/ensemble_forecast_model_runs/ensemble_forecast_model_runs.html",
    "href": "doc/samples/R/vignettes/ensemble_forecast_model_runs/ensemble_forecast_model_runs.html",
    "title": "Ensemble Forecasting SWIFT model runs",
    "section": "",
    "text": "Ensemble Forecasting SWIFT model runs\nJean-Michel Perraud 2020-01-28\n\n\nEnsemble SWIFT model runs\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:52:34.\n\n\nElaboration\nlibrary(swift)\nhas_data &lt;- swift::hasSampleData()\nif (!has_data) {Sys.setenv(SWIFT_SAMPLE_DATA_DIR='c:/data/stsf/documentation')}\nLet’s create a test catchment with a few subareas\nrunoffModel='GR4J'\n\nnodeIds=paste0('n', 1:6)\nlinkIds = paste0('lnk', 1:5)\ndefn &lt;- list(\n    nodeIds=nodeIds,\n    nodeNames = paste0(nodeIds, '_name'),\n    linkIds=linkIds,\n    linkNames = paste0(linkIds, '_name'),\n    fromNode = paste0('n', c(2,5,4,3,1)),\n    toNode = paste0('n', c(6,2,2,4,4)),\n    areasKm2 = c(1.2, 2.3, 4.4, 2.2, 1.5),\n    runoffModel = runoffModel\n)\nsimulation &lt;- createCatchment(defn$nodeIds, defn$nodeNames, defn$linkIds, defn$linkNames, defn$fromNode, defn$toNode, defn$runoffModel, defn$areasKm2)\nthe package uchronia includes facilities to access time series from a “library”, akin to what you would do to manage books.\ndataLibrary &lt;- uchronia::sampleTimeSeriesLibrary('upper murray')\ndataIds &lt;- uchronia::GetEnsembleDatasetDataIdentifiers_R(dataLibrary)\nprint(uchronia::GetEnsembleDatasetDataSummaries_R(dataLibrary))\n## [1] \"variable name: pet_der, identifier: 1, start: 1989-12-31T00:00:00, end: 2012-12-30T00:00:00, time length: 8401, time step: daily\"                            \n## [2] \"variable name: pet_der, identifier: 1, start: 1988-12-31T00:00:00, end: 2012-12-30T00:00:00, time length: 8766, time step: daily\"                            \n## [3] \"variable name: rain_der, identifier: 1, start: 1989-12-31T13:00:00, end: 2012-10-31T12:00:00, time length: 200160, time step: hourly\"                        \n## [4] \"variable name: rain_fcast_ens, identifier: 1, index: 0, start: 2010-08-01T21:00:00, end: 2010-08-06T21:00:00, time length: 5, time step: &lt;not yet supported&gt;\"\nThe sample catchment structure is obviously not the real “Upper Murray”. For the sake of a didactic example, let’s set the same inputs across all the subareas.\nprecipIds &lt;- paste( 'subarea', getSubareaIds(simulation), 'P', sep='.')\nevapIds &lt;- paste( 'subarea', getSubareaIds(simulation), 'E', sep='.')\nplayInputs(simulation, dataLibrary, precipIds, rep('rain_obs', length(precipIds)))\nplayInputs(simulation, dataLibrary, evapIds, rep('pet_obs', length(evapIds)), 'daily_to_hourly')\n## Warning in playInputs(simulation, dataLibrary, evapIds, rep(\"pet_obs\",\n## length(evapIds)), : Reusing argument `resample` to match the length of\n## `modelVarId`\n# And the flow rate we will record\noutflowId &lt;- 'Catchment.StreamflowRate'\nGiven the information from the input data, let’s define a suitable simulation time span. NOTE and TODO: hourly information may not have been shown above yet.\ns &lt;- uchronia::mkDate(2007, 1, 1)\ne &lt;- uchronia::mkDate(2010, 8, 1, 20)\nsHot &lt;- uchronia::mkDate(2010, 8, 1, 21)\neHot &lt;- uchronia::mkDate(2010, 8, 5, 21)\nFirst, before demonstrating ensemble forecasting simulations, let’s demonstrate how we can get a snapshot of the model states at a point in time and restore it later on, hot-starting further simulation.\nsetSimulationSpan(simulation, start=s, end=eHot)\nrecordState(simulation, outflowId)\nexecSimulation(simulation)\nbaseline &lt;- getRecorded(simulation, outflowId)\nintv &lt;- joki::makeTextTimeInterval(sHot,eHot)\nbaseline &lt;- baseline[intv]\n\nsetSimulationSpan(simulation, start=s, end=e)\nexecSimulation(simulation)\nsnapshot &lt;- snapshotState(simulation)\nWe can execute a simulation over the new time span, but requesting model states to NOT be reset. If we compare with a simulation where, as per default, the states are reset before the first time step, we notice a difference:\nsetSimulationSpan(simulation, start=sHot, end=eHot)\nexecSimulation(simulation, resetInitialStates = FALSE)\nnoReset &lt;- getRecorded(simulation, outflowId)\nexecSimulation(simulation, resetInitialStates = TRUE)\nwithReset &lt;- getRecorded(simulation, outflowId)\nx &lt;- merge(noReset,withReset)\nzoo::plot.zoo(x, plot.type='single', col=c('blue','red'), ylab=\"Outflow m3/s\", main=\"Outflows with/without state resets\")\n\nNow let’d ready the simulation to do ensemble forecasts. We define a list inputMap such that keys are the names of ensemble forecast time series found in dataLibrary and the values is one or more of the model properties found in the simulation. In this instance we use the same series for all model precipitation inputs in precipIds\ninputMap &lt;- list(rain_fcast_ens=precipIds)\nresetModelStates(simulation)\nsetStates(simulation, snapshot)\nems &lt;- createEnsembleForecastSimulation(simulation, dataLibrary, start=sHot, end=eHot, inputMap=inputMap, leadTime=as.integer(24*2 + 23), ensembleSize=100, nTimeStepsBetweenForecasts=24)\nGetSimulationSpan_Pkg_R(ems)\n## $Start\n## [1] \"2010-08-01 21:00:00 UTC\"\n## \n## $End\n## [1] \"2010-08-04 21:00:00 UTC\"\n## \n## $TimeStep\n## [1] \"hourly\"\nrecordState(ems, outflowId)\nexecSimulation(ems)\nforecasts &lt;- getRecordedEnsembleForecast(ems, outflowId)\nstrSwiftRef(forecasts)\n## ensemble forecast time series:\n##  2010-08-01 21:00:00 UTC\n##  time step 86400S\n##  size 4\nflowFc &lt;- uchronia::getItem(forecasts, 1)\nuchronia::plotXtsQuantiles(flowFc)",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Ensemble Forecasting SWIFT model runs"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/calibrate_multisite/calibrate_multisite.html",
    "href": "doc/samples/R/vignettes/calibrate_multisite/calibrate_multisite.html",
    "title": "Calibration of a catchment using multisite multiobjective composition",
    "section": "",
    "text": "Calibration of a catchment using multisite multiobjective composition\nJean-Michel Perraud 2020-01-28\n\n\nCalibration of a catchment using multisite multiobjective composition\n\n\nUse case\nThis vignette demonstrates how one can calibrate a catchment using multiple gauging points available within this catchment.\nThis is a joint calibration weighing multiple objectives, possibly sourced at different modelling elements, thus a whole-of-catchment calibration technique. Staged, cascading calibration is supported and described in another vignette.\n\n\nData\nThe sample data that comes with the package contains a model definition for the South Esk catchment, including a short subset of the climate and flow record data.\n# swiftr_dev()   \n\nlibrary(swift)\n\nmodelId &lt;- 'GR4J'\nsiteId &lt;- 'South_Esk'\nsimulation &lt;- sampleCatchmentModel(siteId=siteId, configId='catchment')\n# simulation &lt;- swapModel(simulation, 'MuskingumNonLinear', 'channel_routing')\nsimulation &lt;- swapModel(simulation, 'LagAndRoute', 'channel_routing')\nseClimate &lt;- sampleSeries(siteId=siteId, varName='climate')\nseFlows &lt;- sampleSeries(siteId=siteId, varName='flow')\nThe names of the climate series is already set to the climate input identifiers of the model simulation, so setting them as inputs is easy:\nplayInput(simulation, seClimate)\nsetSimulationSpan(simulation, start(seClimate), end(seClimate))\nsetSimulationTimeStep(simulation, 'hourly')\nMoving on to define the parameters, free or fixed. We will use (for now - may change) the package calibragem, companion to SWIFT.\nconfigureHourlyGr4j(simulation)\nWe define a function creating a realistic feasible parameter space. This is not the main object of this vignette, so we do not describe in details.\ncreateMetaParameterizer &lt;- function(simulation, refArea=250, timeSpan=3600L) {\n  timeSpan &lt;- as.integer(timeSpan)\n  parameterizer &lt;- defineGr4jScaledParameter(refArea, timeSpan)\n  \n  # Let's define _S0_ and _R0_ parameters such that for each GR4J model instance, _S = S0 * x1_ and _R = R0 * x3_\n  pStates &lt;- linearParameterizer(\n                      paramName=c(\"S0\",\"R0\"), \n                      stateName=c(\"S\",\"R\"), \n                      scalingVarName=c(\"x1\",\"x3\"),\n                      minPval=c(0.0,0.0), \n                      maxPval=c(1.0,1.0), \n                      value=c(0.9,0.9), \n                      selectorType='each subarea')\n  \n  initParameterizer &lt;- makeStateInitParameterizer(pStates)\n  parameterizer &lt;- concatenateParameterizers(parameterizer, initParameterizer)\n  \n  lagAndRouteParameterizer &lt;- function() {\n    p &lt;- data.frame(Name = c('alpha', 'inverse_velocity'),\n        Value = c(1, 1),\n        Min = c(1e-3, 1e-3),\n        Max = c(1e2, 1e2),\n        stringsAsFactors = FALSE)\n    p &lt;- createParameterizer('Generic links', p)\n    return(p)\n  }\n\n  # Lag and route has several discrete storage type modes. One way to set up the modeP\n  setupStorageType &lt;- function(simulation) {\n    p &lt;- data.frame(Name = c('storage_type'), \n        Value = 1, \n        Min = 1,\n        Max = 1,\n        stringsAsFactors = FALSE)\n    p &lt;- createParameterizer('Generic links', p)\n    applySysConfig(p, simulation)\n  }\n  setupStorageType(simulation)\n\n  # transfer reach lengths to the Lag and route model\n  linkIds &lt;- getLinkIds(simulation)\n  reachLengths &lt;- getStateValue(simulation, paste0('link.', linkIds, '.Length'))\n  setStateValue(simulation, paste0('link.', linkIds, '.reach_length'), reachLengths) \n\n  lnrp &lt;- lagAndRouteParameterizer()\n  parameterizer &lt;- concatenateParameterizers(parameterizer, lnrp)  \n  return(parameterizer)\n}\nparameterizer &lt;- createMetaParameterizer(simulation)\nsetParameterValue(parameterizer, 'asinh_x2', 0)\napplySysConfig(parameterizer, simulation)\nexecSimulation(simulation)\nWe are now ready to enter the main topic of this vignette, setting up a weighted multi-objective for calibration purposes.\nThe sample gauge data flow contains identifiers that are of course distinct from the network node identifiers. We create a map between them (note - this information used to be in the NodeLink file in swiftv1).\ngauges &lt;- as.character(c( 92106, 592002, 18311, 93044,    25,   181))\nnodeIds &lt;- paste0('node.', as.character( c(7,   12,   25,   30,   40,   43) ))   \nnames(gauges) &lt;- nodeIds\nFirst, let us try the Nash Sutcliffe efficiency, for simplicity (no additional parameters needed). We will set up NSE calculations at two points (nodes) in the catchments. Any model state from a link, node or subarea could be a point for statistics calculation.\ns &lt;- GetSimulationSpan_Pkg_R(simulation)\n\ncalibrationPoints &lt;- nodeIds[1:2]\nmvids &lt;- mkFullDataId(calibrationPoints, 'OutflowRate')\n\nw &lt;- c(1.0, 2.0) # weights (internally normalised to a total of 1.0)\nnames(w) &lt;- mvids\n\nstatspec &lt;- multiStatisticDefinition( \n  modelVarIds = mvids, \n  statisticIds = rep('nse', 2), \n  objectiveIds=calibrationPoints, \n  objectiveNames = paste0(\"NSE-\", calibrationPoints), \n  starts = rep(s$Start, 2), \n  ends = rep(s$End, 2) )\n\nobservations &lt;- list(\n  seFlows[,gauges[1]],\n  seFlows[,gauges[2]]\n)\n\nmoe &lt;- createMultisiteObjective(simulation, statspec, observations, w)\ngetScore(moe, parameterizer)\n## $scores\n## MultisiteObjectives \n##          0.03281224 \n## \n## $sysconfig\n##               Name       Min        Max     Value\n## 1           log_x4  0.000000   2.380211 0.3054223\n## 2           log_x1  0.000000   3.778151 0.5066903\n## 3           log_x3  0.000000   3.000000 0.3154245\n## 4         asinh_x2 -3.989327   3.989327 0.0000000\n## 5               R0  0.000000   1.000000 0.9000000\n## 6               S0  0.000000   1.000000 0.9000000\n## 7            alpha  0.001000 100.000000 1.0000000\n## 8 inverse_velocity  0.001000 100.000000 1.0000000\nWe now build a parameterizer that is suited to multisite objective functions. Note that for now there is no effect changing the result - just scaffloding and structural test.\np &lt;- createParameterizer('no apply')\nfuncParam &lt;- list(p, clone(p)) # for the two calib points, NSE has no param here\ncatchmentPzer &lt;- parameterizer\nfp &lt;- createMultisiteObjParameterizer(funcParam, calibrationPoints, prefixes=paste0(calibrationPoints, '.'), mixFuncParam=NULL, hydroParam=catchmentPzer)\n(parameterizerAsDataFrame(fp))\n##               Name       Min        Max     Value\n## 1           log_x4  0.000000   2.380211 0.3054223\n## 2           log_x1  0.000000   3.778151 0.5066903\n## 3           log_x3  0.000000   3.000000 0.3154245\n## 4         asinh_x2 -3.989327   3.989327 0.0000000\n## 5               R0  0.000000   1.000000 0.9000000\n## 6               S0  0.000000   1.000000 0.9000000\n## 7            alpha  0.001000 100.000000 1.0000000\n## 8 inverse_velocity  0.001000 100.000000 1.0000000\nEvaluateScoreForParameters(moe, fp)\n## $scores\n## MultisiteObjectives \n##          0.03281224 \n## \n## $sysconfig\n##               Name       Min        Max     Value\n## 1           log_x4  0.000000   2.380211 0.3054223\n## 2           log_x1  0.000000   3.778151 0.5066903\n## 3           log_x3  0.000000   3.000000 0.3154245\n## 4         asinh_x2 -3.989327   3.989327 0.0000000\n## 5               R0  0.000000   1.000000 0.9000000\n## 6               S0  0.000000   1.000000 0.9000000\n## 7            alpha  0.001000 100.000000 1.0000000\n## 8 inverse_velocity  0.001000 100.000000 1.0000000\nWe can get the value of each objective. The two NSE scores below are negative. Note above that the composite objective is positive, i.e. the opposite of the weighted average. This is because the composite objective is always minimisable (as of writing anyway this is a design choice.)\nEvaluateScoresForParametersWila_Pkg_R(moe, fp)\n##     node.12      node.7 \n## -0.03819707 -0.02204260\n\nlog-likelihood multiple objective\nNow, let’s move on to use log-likelihood instead of NSE.\nstatspec &lt;- multiStatisticDefinition(  \n  modelVarIds = mvids, \n  statisticIds = rep('log-likelihood', 2), \n  objectiveIds=calibrationPoints, \n  objectiveNames = paste0(\"LL-\", calibrationPoints), \n  starts = rep(s$Start, 2), \n  ends = rep(s$End, 2) )\n\nobj &lt;- createMultisiteObjective(simulation, statspec, observations, w)\nFor this to work we need to include parameters\nmaxobs &lt;- max(observations[[1]], na.rm=TRUE)\ncensorThreshold &lt;- 0.01\ncensopt &lt;- 0.0\ncalcMAndS &lt;- 1.0 # i.e. TRUE\n\n#       const string LogLikelihoodKeys::KeyAugmentSimulation = \"augment_simulation\";\n#       const string LogLikelihoodKeys::KeyExcludeAtTMinusOne = \"exclude_t_min_one\";\n#       const string LogLikelihoodKeys::KeyCalculateModelledMAndS = \"calc_mod_m_s\";\n#       const string LogLikelihoodKeys::KeyMParMod = \"m_par_mod\";\n#       const string LogLikelihoodKeys::KeySParMod = \"s_par_mod\";\n\np &lt;- createParameterizer('no apply')\n# Note: exampleParameterizer is also available\naddToHyperCube(p, \n          data.frame( Name=c('b','m','s','a','maxobs','ct', 'censopt', 'calc_mod_m_s'),\n          Min   = c(-30, 0, 1,    -30, maxobs, censorThreshold, censopt, calcMAndS),\n          Max   = c(0,   0, 1000, 1, maxobs, censorThreshold, censopt, calcMAndS),\n          Value = c(-7,  0, 100,  -10, maxobs, censorThreshold, censopt, calcMAndS),\n          stringsAsFactors=FALSE) )\n\nfuncParam &lt;- list(p, clone(p)) # for the two calib points, NSE has no param here\ncatchmentPzer &lt;- parameterizer\nfp &lt;- createMultisiteObjParameterizer(funcParam, calibrationPoints, prefixes=paste0(calibrationPoints, '.'), mixFuncParam=NULL, hydroParam=catchmentPzer)\n(parameterizerAsDataFrame(fp))\n##                    Name        Min         Max       Value\n## 1                log_x4   0.000000    2.380211   0.3054223\n## 2                log_x1   0.000000    3.778151   0.5066903\n## 3                log_x3   0.000000    3.000000   0.3154245\n## 4              asinh_x2  -3.989327    3.989327   0.0000000\n## 5                    R0   0.000000    1.000000   0.9000000\n## 6                    S0   0.000000    1.000000   0.9000000\n## 7                 alpha   0.001000  100.000000   1.0000000\n## 8      inverse_velocity   0.001000  100.000000   1.0000000\n## 9              node.7.b -30.000000    0.000000  -7.0000000\n## 10             node.7.m   0.000000    0.000000   0.0000000\n## 11             node.7.s   1.000000 1000.000000 100.0000000\n## 12             node.7.a -30.000000    1.000000 -10.0000000\n## 13        node.7.maxobs  22.000000   22.000000  22.0000000\n## 14            node.7.ct   0.010000    0.010000   0.0100000\n## 15       node.7.censopt   0.000000    0.000000   0.0000000\n## 16  node.7.calc_mod_m_s   1.000000    1.000000   1.0000000\n## 17            node.12.b -30.000000    0.000000  -7.0000000\n## 18            node.12.m   0.000000    0.000000   0.0000000\n## 19            node.12.s   1.000000 1000.000000 100.0000000\n## 20            node.12.a -30.000000    1.000000 -10.0000000\n## 21       node.12.maxobs  22.000000   22.000000  22.0000000\n## 22           node.12.ct   0.010000    0.010000   0.0100000\n## 23      node.12.censopt   0.000000    0.000000   0.0000000\n## 24 node.12.calc_mod_m_s   1.000000    1.000000   1.0000000\nmoe &lt;- obj\ngetScore(moe, fp)\n## $scores\n## MultisiteObjectives \n##             44779.5 \n## \n## $sysconfig\n##                    Name        Min         Max       Value\n## 1                log_x4   0.000000    2.380211   0.3054223\n## 2                log_x1   0.000000    3.778151   0.5066903\n## 3                log_x3   0.000000    3.000000   0.3154245\n## 4              asinh_x2  -3.989327    3.989327   0.0000000\n## 5                    R0   0.000000    1.000000   0.9000000\n## 6                    S0   0.000000    1.000000   0.9000000\n## 7                 alpha   0.001000  100.000000   1.0000000\n## 8      inverse_velocity   0.001000  100.000000   1.0000000\n## 9              node.7.b -30.000000    0.000000  -7.0000000\n## 10             node.7.m   0.000000    0.000000   0.0000000\n## 11             node.7.s   1.000000 1000.000000 100.0000000\n## 12             node.7.a -30.000000    1.000000 -10.0000000\n## 13        node.7.maxobs  22.000000   22.000000  22.0000000\n## 14            node.7.ct   0.010000    0.010000   0.0100000\n## 15       node.7.censopt   0.000000    0.000000   0.0000000\n## 16  node.7.calc_mod_m_s   1.000000    1.000000   1.0000000\n## 17            node.12.b -30.000000    0.000000  -7.0000000\n## 18            node.12.m   0.000000    0.000000   0.0000000\n## 19            node.12.s   1.000000 1000.000000 100.0000000\n## 20            node.12.a -30.000000    1.000000 -10.0000000\n## 21       node.12.maxobs  22.000000   22.000000  22.0000000\n## 22           node.12.ct   0.010000    0.010000   0.0100000\n## 23      node.12.censopt   0.000000    0.000000   0.0000000\n## 24 node.12.calc_mod_m_s   1.000000    1.000000   1.0000000\nEvaluateScoresForParametersWila_Pkg_R(moe, fp)\n##   node.12    node.7 \n## -44617.76 -45102.99",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Calibration of a catchment using multisite multiobjective composition"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/calibrate_subcatchments/calibrate_subcatchments.html",
    "href": "doc/samples/R/vignettes/calibrate_subcatchments/calibrate_subcatchments.html",
    "title": "Calibration of subcatchments defined by multiple gauges in a catchment",
    "section": "",
    "text": "Jean-Michel Perraud 2020-01-28",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Calibration of subcatchments defined by multiple gauges in a catchment"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/calibrate_subcatchments/calibrate_subcatchments.html#whole-of-catchment-calibration-combining-point-gauges",
    "href": "doc/samples/R/vignettes/calibrate_subcatchments/calibrate_subcatchments.html#whole-of-catchment-calibration-combining-point-gauges",
    "title": "Calibration of subcatchments defined by multiple gauges in a catchment",
    "section": "Whole of catchment calibration combining point gauges",
    "text": "Whole of catchment calibration combining point gauges\ngauges &lt;- as.character(c( 92106, 592002, 18311, 93044,    25,   181))\nnames(gauges) &lt;- paste0('node.', as.character( c(7,   12,   25,   30,   40,   43) ))\ncalibNodes &lt;- paste0('node.', as.character( c(7,   12) ))\n\nelementId &lt;- names(subCats)[1]\n\ngaugeId &lt;- gauges[calibNodes]\ngaugeFlow &lt;- seFlows[,gaugeId]\n\nvarId &lt;- paste0(calibNodes, '.OutflowRate')\nrecordState(simulation,varId)\nobjectiveId &lt;- 'NSE'\nobjective &lt;- createObjective(simulation, varId[1], observation=gaugeFlow[,1], objectiveId, start(seFlows), end(seFlows))\n\naddToCompositeObjective &lt;- function(compositeObj, objective, weight, name) {\n  obj &lt;- objective\n  if (cinterop::isExternalObjRef(objective, 'OBJECTIVE_EVALUATOR_WILA_PTR')) {\n    obj &lt;- UnwrapObjectiveEvaluatorWila_R(objective)\n  }\n  AddSingleObservationObjectiveEvaluator_R(compositeObj, obj, weight, name)\n}\n\nco &lt;- CreateEmptyCompositeObjectiveEvaluator_R()\naddToCompositeObjective(co, objective, 1.0, varId[1])\nobjective &lt;- createObjective(simulation, varId[2], observation=gaugeFlow[,2], objectiveId, start(seFlows), end(seFlows))\naddToCompositeObjective(co, objective, 1.0, varId[2])\n\nco &lt;- swift::WrapObjectiveEvaluatorWila_R(co, clone=TRUE)\n\nscore &lt;- getScore(co,parameterizer) \n# scoresAsDataFrame(score)",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Calibration of subcatchments defined by multiple gauges in a catchment"
    ]
  },
  {
    "objectID": "doc/samples/R/vignettes/ensemble_model_runs/ensemble_model_runs.html",
    "href": "doc/samples/R/vignettes/ensemble_model_runs/ensemble_model_runs.html",
    "title": "Ensemble SWIFT model runs",
    "section": "",
    "text": "Ensemble SWIFT model runs\nJean-Michel Perraud 2020-01-28\n\n\nEnsemble SWIFT model runs\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:52:36.\n\n\nElaboration\nlibrary(swift)\nhas_data &lt;- swift::hasSampleData()\nLet’s create a test catchment with a few subareas\nrunoffModel='GR4J'\n\nnodeIds=paste0('n', 1:6)\nlinkIds = paste0('lnk', 1:5)\ndefn &lt;- list(\n    nodeIds=nodeIds,\n    nodeNames = paste0(nodeIds, '_name'),\n    linkIds=linkIds,\n    linkNames = paste0(linkIds, '_name'),\n    fromNode = paste0('n', c(2,5,4,3,1)),\n    toNode = paste0('n', c(6,2,2,4,4)),\n    areasKm2 = c(1.2, 2.3, 4.4, 2.2, 1.5),\n    runoffModel = runoffModel\n)\nsimulation &lt;- createCatchment(defn$nodeIds, defn$nodeNames, defn$linkIds, defn$linkNames, defn$fromNode, defn$toNode, defn$runoffModel, defn$areasKm2)\nthe package uchronia includes facilities to access time series from a “library”, akin to what you would do to manage books.\ndataLibrary &lt;- uchronia::sampleTimeSeriesLibrary('upper murray')\ndataIds &lt;- uchronia::GetEnsembleDatasetDataIdentifiers_R(dataLibrary)\nprint(uchronia::GetEnsembleDatasetDataSummaries_R(dataLibrary))\n## [1] \"variable name: pet_der, identifier: 1, start: 1989-12-31T00:00:00, end: 2012-12-30T00:00:00, time length: 8401, time step: daily\"                            \n## [2] \"variable name: pet_der, identifier: 1, start: 1988-12-31T00:00:00, end: 2012-12-30T00:00:00, time length: 8766, time step: daily\"                            \n## [3] \"variable name: rain_der, identifier: 1, start: 1989-12-31T13:00:00, end: 2012-10-31T12:00:00, time length: 200160, time step: hourly\"                        \n## [4] \"variable name: rain_fcast_ens, identifier: 1, index: 0, start: 2010-08-01T21:00:00, end: 2010-08-06T21:00:00, time length: 5, time step: &lt;not yet supported&gt;\"\nThe sample catchment structure is obviously not the real “Upper Murray”. For the sake of a didactic example, let’s set the same inputs across all the subareas.\nprecipIds &lt;- paste( 'subarea', getSubareaIds(simulation), 'P', sep='.')\nevapIds &lt;- paste( 'subarea', getSubareaIds(simulation), 'E', sep='.')\nplayInputs(simulation, dataLibrary, precipIds, rep('rain_obs', length(precipIds)))\nplayInputs(simulation, dataLibrary, evapIds, rep('pet_obs', length(evapIds)), 'daily_to_hourly')\n## Warning in playInputs(simulation, dataLibrary, evapIds, rep(\"pet_obs\",\n## length(evapIds)), : Reusing argument `resample` to match the length of\n## `modelVarId`\n# And the flow rate we will record\noutflowId &lt;- 'Catchment.StreamflowRate'\nGiven the information from the input data, let’s define a suitable simulation time span. NOTE and TODO: hourly information may not have been shown above yet.\ns &lt;- joki::asPOSIXct('2007-01-01')\ne &lt;- joki::asPOSIXct('2010-08-01 20')\nsHot &lt;- joki::asPOSIXct('2010-08-01 21')\neHot &lt;- joki::asPOSIXct('2010-08-05 21')\nFirst, before demonstrating ensemble forecasting simulations, let’s demonstrate how we can get a snapshot of the model states at a point in time and restore it later on, hot-starting further simulation.\nsetSimulationSpan(simulation, start=s, end=eHot)\nrecordState(simulation, outflowId)\nexecSimulation(simulation)\nbaseline &lt;- getRecorded(simulation, outflowId)\nintv &lt;- joki::makeTextTimeInterval(sHot,eHot)\nbaseline &lt;- baseline[intv]\n\nsetSimulationSpan(simulation, start=s, end=e)\nexecSimulation(simulation)\nsnapshot &lt;- snapshotState(simulation)\nWe can execute a simulation over the new time span, but requesting model states to NOT be reset. If we compare with a simulation where, as per default, the states are reset before the first time step, we notice a difference:\nsetSimulationSpan(simulation, start=sHot, end=eHot)\nexecSimulation(simulation, resetInitialStates = FALSE)\nnoReset &lt;- getRecorded(simulation, outflowId)\nexecSimulation(simulation, resetInitialStates = TRUE)\nwithReset &lt;- getRecorded(simulation, outflowId)\nx &lt;- merge(noReset,withReset)\nzoo::plot.zoo(x, plot.type='single', col=c('blue','red'), ylab=\"Outflow m3/s\", main=\"Outflows with/without state resets\")\n\nNow let’d ready the simulation to do ensemble forecasts. We define a list inputMap such that keys are the names of ensemble forecast time series found in dataLibrary and the values is one or more of the model properties found in the simulation. In this instance we use the same series for all model precipitation inputs in precipIds\ninputMap &lt;- list(rain_fcast_ens=precipIds)\nresetModelStates(simulation)\nsetStates(simulation, snapshot)\nems &lt;- createEnsembleForecastSimulation(simulation, dataLibrary, start=sHot, end=eHot, inputMap=inputMap, leadTime=as.integer(24*2 + 23), ensembleSize=100, nTimeStepsBetweenForecasts=24)\nGetSimulationSpan_Pkg_R(ems)\n## $Start\n## [1] \"2010-08-01 21:00:00 UTC\"\n## \n## $End\n## [1] \"2010-08-04 21:00:00 UTC\"\n## \n## $TimeStep\n## [1] \"hourly\"\nrecordState(ems, outflowId)\nexecSimulation(ems)\nforecasts &lt;- getRecordedEnsembleForecast(ems, outflowId)\nstrSwiftRef(forecasts)\n## ensemble forecast time series:\n##  2010-08-01 21:00:00 UTC\n##  time step 86400S\n##  size 4\nflowFc &lt;- uchronia::getItem(forecasts, 1)\nuchronia::plotXtsQuantiles(flowFc)",
    "crumbs": [
      "Documentation",
      "Examples in R",
      "Ensemble SWIFT model runs"
    ]
  },
  {
    "objectID": "doc/samples/sample_workflows_r.html",
    "href": "doc/samples/sample_workflows_r.html",
    "title": "Sample modelling workflows - R",
    "section": "",
    "text": "Sample modelling workflows - R\nThe following documents are extracted from the vignettes in the R package swift\n\nGetting started with the swift R package\nEnsemble SWIFT model runs\nEnsemble forecast model runs\nCalibrating tied meta parameters\nMultisite multiobjective calibration\nCalibration of subcatchments defined by multiple gauges in a catchment\nCalibration with initial model memory states as parameters\n\nError correction models - Elaboration\nLinear Muskingum channel routing model - constrained subcatchment calibration",
    "crumbs": [
      "Documentation",
      "Examples in R"
    ]
  },
  {
    "objectID": "doc/developer.html",
    "href": "doc/developer.html",
    "title": "Developer documentation",
    "section": "",
    "text": "“cruise-control” (restricted link) contains detailed instructions for building and maintaining the software stack, and deploying it on CSIRO organisational infrastructure. Research partners can naturally access that documentation upon request: the content is not sensitive but kept private for logistical reasons.\n\nTechnical blog posts\nA few posts that are related to this hydrologic simulation software stack:\n\nPresenting doxygen C++ API documentation via MkDocs with doxybook2\nGenerate programming language bindings to a C API\nAzure devops CI pipeline for hydrologic forecasting software\nCompiling C++ libraries to WebAssembly - Part 1\nWindows installer with WiX 4 - part 1\n\n\n\nRelease and versioning\nThe streamflow forecasting software stack comprise a dozen or so public or private git repositories. Co-evolving these repositories is not trivial. Each component may have a versioning rationale that is orthogonal to others. However for reproducibility we need to have “meta-releases” that are aggregates of components with a clear, traceable lineage.\nAs of 2023-01:\n\nDiagram that explains the overarching system to release/tag.",
    "crumbs": [
      "Documentation",
      "Developer documentation"
    ]
  },
  {
    "objectID": "doc/installation/Readme.html",
    "href": "doc/installation/Readme.html",
    "title": "Notes",
    "section": "",
    "text": "Notes\nOn Windows (not sure whether it works on Linux)\npandoc -f docx -t markdown windows_installer.docx -o windows_installer.qmd  --extract-media ."
  },
  {
    "objectID": "doc/installation/install_source.html",
    "href": "doc/installation/install_source.html",
    "title": "Installation: From source code",
    "section": "",
    "text": "“cruise-control” (restricted link) contains detailed instructions for building and maintaining the software stack from source.\nFew people have this need in practice, and within this pool of people, most will have a particular platform and preference in development tools, so it is moot to document prescriptive steps here for now. That said, the build pipelines that exist for various operating systems may be a basis for documenting this in the future. Pending user needs.",
    "crumbs": [
      "Installation",
      "Installation: From source code"
    ]
  },
  {
    "objectID": "doc/installation/install_source.html#installing-hydroforecast-packages-in-dev-mode",
    "href": "doc/installation/install_source.html#installing-hydroforecast-packages-in-dev-mode",
    "title": "Installation: From source code",
    "section": "Installing hydroforecast packages in dev mode",
    "text": "Installing hydroforecast packages in dev mode\nTo install instead in development mode, for some or all of the 4 packages:\nset GITHUB_REPOS=c:\\src\nset CSIRO_BITBUCKET=c:\\src\ncd %GITHUB_REPOS%\\pyrefcount\npython setup.py develop\ncd %GITHUB_REPOS%\\c-interop\\bindings\\python\\cinterop\npython setup.py develop\n\ncd %CSIRO_BITBUCKET%\\datatypes\\bindings\\python\\uchronia\\\npython setup.py develop\ncd %CSIRO_BITBUCKET%\\swift\\bindings\\python\\swift2\\\npython setup.py develop\n:: fogss...\nNow to explore sample notebooks\nmamba install -c conda-forge seaborn\ncd %userprofile%\\Documents\nmkdir notebooks\nxcopy %CSIRO_BITBUCKET%\\swift\\bindings\\python\\swift2\\notebooks\\* .\\\n\njupyter-lab .\nstart with getting_started.ipynb\nTODO: recommend using nbstripout and jupytext",
    "crumbs": [
      "Installation",
      "Installation: From source code"
    ]
  },
  {
    "objectID": "doc/installation/install_linux.html",
    "href": "doc/installation/install_linux.html",
    "title": "Installation: Debian-based Linux",
    "section": "",
    "text": "As of Jan 2020, Debian .deb binary packages can be built and installed, instead of installing from source code. Ubuntu-flavoured linux distributions use debian packages to manage software installation. This has been done practically a few times to set up servers, but this remains a novel installation medium as of 2023 and you should feel free to seek advice and help as need be.\nYou should have received an archive (“zip file” or similar) that contains the following folders:\n\ndeb_pkgs: installable Debian packages\npy_pkgs: installable Python packages (wheels)\nr_pkgs: installable R packages (R source packages tarballs)\n\ncd deb_pkgs\nThere are various user arrangements to install packages; one command line application is dpkg. You may (probably) need administrative rights to install software though. Adapt the following, or a subset, to the package version you have:\nsudo dpkg -i libmoirai_1.0-1_amd64.deb\nsudo dpkg -i libuchronia_2.3-7_amd64.deb\nsudo dpkg -i libswift_2.3-7_amd64.deb\nsudo dpkg -i libqppcore_2.3-7_amd64.deb\nsudo dpkg -i libqpp_2.3-7_amd64.deb\nIf you are planning to develop applications in C++, you will need to install header files:\nsudo dpkg -i libboost-threadpool-dev_0.2-6_amd64.deb\nsudo dpkg -i libcinterop-dev_1.1-1_amd64.deb\nsudo dpkg -i libwila-dev_0.7-2_amd64.deb\nsudo dpkg -i libmoirai-dev_1.0-1_amd64.deb\nsudo dpkg -i libqppcore-dev_2.3-7_amd64.deb\nsudo dpkg -i libqpp-dev_2.3-7_amd64.deb\nsudo dpkg -i libsfsl-dev_2.3-1_amd64.deb\nsudo dpkg -i libswift-dev_2.3-7_amd64.deb\nsudo dpkg -i libuchronia-dev_2.3-7_amd64.deb\nAn advanced use case is to install debug symbols, but this is hypothetical as most core developpers so far have access to the source code of the software stack and work from this.\nsudo dpkg -i libmoirai-dbgsym_1.0-1_amd64.deb\nsudo dpkg -i libqppcore-dbgsym_2.3-7_amd64.deb\nsudo dpkg -i libqpp-dbgsym_2.3-7_amd64.deb\nsudo dpkg -i libuchronia-dbgsym_2.3-7_amd64.deb\nsudo dpkg -i libswift-dbgsym_2.3-7_amd64.deb",
    "crumbs": [
      "Installation",
      "Installation: Debian based Linux"
    ]
  },
  {
    "objectID": "doc/installation/install_windows.html",
    "href": "doc/installation/install_windows.html",
    "title": "Installation: Windows",
    "section": "",
    "text": "This document is a short guide to install on Windows the CSIRO ensemble streamflow forecasting software stack that includes tools known as SWIFT2, FoGSS, CHyPP.\nAs of 2023-04-24 it is recommended to use a Windows Installer (MSI file). The manual setup is kept in this document but is deprecated.",
    "crumbs": [
      "Installation",
      "Installation: Windows"
    ]
  },
  {
    "objectID": "doc/installation/install_windows.html#windows-installer",
    "href": "doc/installation/install_windows.html#windows-installer",
    "title": "Installation: Windows",
    "section": "Windows installer",
    "text": "Windows installer\nExecute the file sf.msi (or sf-x.y.z.msi)\n\nDo read the license agreement. If you have access to this installer, you should already have had prior discussions with the product distributors to agree on the scope of use of the products.\n\nAs a user-level installation you will be prompted with a conventional default location. You may change the destination folder if required.\n\n\nThe installation will typically take a few seconds to proceed.",
    "crumbs": [
      "Installation",
      "Installation: Windows"
    ]
  },
  {
    "objectID": "doc/installation/install_windows.html#installing-shared-dynamic-libraries",
    "href": "doc/installation/install_windows.html#installing-shared-dynamic-libraries",
    "title": "Installation: Windows",
    "section": "Installing shared (dynamic) libraries",
    "text": "Installing shared (dynamic) libraries\nThis documentation is fairly prescriptive in terms of the location where you would install dynamic libraries. You can still choose the top level folder where you would install these libraries. This document uses c:\\local, and you are encourage to adhere to this convention if you can logistically can.\nYou may have 7z in your command line prompt, otherwise use Windows Explorer to extract the zip file from the 7Zip graphical user interface. You may replace 7z with the full path, typically \"c:\\Program Files\\7-Zip\\7z.exe\" with quotation marks.\nif not defined local_dir set local_dir=c:\\local\nif not exist %local_dir% mkdir %local_dir%\\libs\\64\n\ncd %local_dir%\n7z x -y libs.7z\n:: or:\n\"c:\\Program Files\\7-Zip\\7z.exe\" x -y libs.7z\nThis should have created the folder C:\\local\\libs\\64. A 32 bits folder may be present but is deprecated since 2017-12: operating systems are now largely 64 bits and we strongly recommend you use 64 bits version of the products.\n\nAdding environment variable LIBRARY_PATH\nThe packages in R, python, (etc.) need a way to search for these native libraries. To do so, you should set up a User- or System-level environmental variable LIBRARY_PATH. The approach borrows from what is customary (albeit variable) on most Linux system for locating software libraries.\nGo to the Control Panel, System and Security, Search for the string “environment”, and it should give you the link to Edit environment variables for your account.\n\nfor the field “Variable name:” use LIBRARY_PATH\nfor the field “Variable value:” use c:\\local\\libs (if you unzipped libs.7z into c:\\local in the previous section)",
    "crumbs": [
      "Installation",
      "Installation: Windows"
    ]
  },
  {
    "objectID": "doc/installation/install_windows.html#r-packages",
    "href": "doc/installation/install_windows.html#r-packages",
    "title": "Installation: Windows",
    "section": "R packages",
    "text": "R packages\nR packages are available in source and precompiled “win.binaries” form. The latter is easy and fast. Installing from source is out of scope of this document. However, Windows binary packages are version dependent. The installer includes packages for R 4.2.x, the current latest stable version of R. If you need another version of the packages, please contact the authors.\nThere are multiple ways to install R. If you have a corporate software management system, chances are the latest stable release of R (currently 4.2.x) is readily available.\nOpen an R terminal.\nCheck with the command .libPaths() that the first folder is user specific, so that you will be able to install the packages.\n[1] \"C:/Users/xxxyyy/AppData/Local/R/win-library/4.2\"\n[2] \"C:/Program Files/R/R-4.2.1/library\"\nIt is likely R would likely prompt you if do not already have a personal library, and would created one in the process; for instance when adding a package:\n\n\nThen after that:\n\nAn alternate possibility is to specify a custom library directory via the R_LIBS environment variable.\n\nAdding an R_LIBS environment variable\nThis step is optional, and possibly deprecated.\nWhile not required, you can set up an additional R library location, specified via an environment variable R_LIBS at the machine or user level. This can facilitate access to R packages for all users and upgrades to newer version of R down the track. you can install the packages in the other library folders that R creates (user-specific if you do not have admin rights)\n\n\nInstalling an R package\nThen you will need the root name of the small R library folder where R packages are, e.g. “C:\\Users\\xxxyyy\\AppData\\Local\\Programs\\SF\\R” if you have chosen the default option for an destination folder. You can also get the path via the shortcut named “Open installation folder” (start to type open in the start menu will likely get you there:)\n\nAdapt the following command with the folder name and it should install the streamflow forecasting packages and their third party dependencies\ninstall.packages(c('calibragem', 'mhplot', 'efts', 'swift',\n'qpp'),\nrepos=c('file:///C:/Users/xxxyyy/AppData/Local/Programs/SF/R',\n'https://cran.csiro.au'), type='win.binary')\nUpon successful installation you should be able to load the swift package:\n\nThe command ?swift should open an HTML page in a browser. At the bottom of the page you can find a shortcut to the package index.\n\nThe package Index page should also include a link to the introductory vignettes:",
    "crumbs": [
      "Installation",
      "Installation: Windows"
    ]
  },
  {
    "objectID": "doc/installation/install_windows.html#python-packages",
    "href": "doc/installation/install_windows.html#python-packages",
    "title": "Installation: Windows",
    "section": "Python packages",
    "text": "Python packages\n\nPython environments\nWe strongly advise you to use python virtual environments to execute these tools via python.\nThere are various methods for these, but typically venv (virtual environments) or conda\nThis section documents the process using conda.\nThere are various options to set up a suitable python environment to run. This document outlines using conda or its replacement mamba. If you do not have any pre-existing environments, we recommend you use mambaforge, but you may have already it installed on your Windows box by e.g. your IT department installation management software.\nThis document assumes you start from the base environment, for instance if you installed miniconda:\n(base) C:\\Users\\xxxyyy&gt;conda env list\n# conda environments:\n#\nbase                  *  C:\\Users\\xxxyyy\\Miniconda3\nIf you have installed Mambaforge, look for “miniforge” in the Windows start menu:\n\n\nWe recommend you use the program mamba, a newer and faster drop-in replacement for conda. This is optional.\nYou may already have mamba installed (e.g. if you installed mambaforge). You can check the command where mamba returns paths to mamba.bat and/or mamba.exe files:\n\nOtherwise you can install mamba in the base environment with:\nconda install -c conda-forge mamba\n\n\nInstalling Python packages\nBelow remember to replace mamba by conda if you have not installed mamba.\nset env_name=\"hydrofc\"\nmamba create -n %env_name% -c conda-forge python=3.9 xarray cffi pandas numpy matplotlib ipykernel jsonpickle netcdf4 seaborn\nREM note to self seaborn used in swift2 sample notebooks, so somewhat optional.\n\n\n\n\n\n\nTip\n\n\n\nIt was once observed on some machines that downloading the package metadata for a conda channel was painfully slow:\n\nHopefully you will not encounter this issue, but if you do there may be a workaround. If there are already cached package metadata, it is possible to skip downloading newer package metadata with the -C option. For instance:\nmamba install -c conda-forge -C xarray\n\n\nRegister the new conda environment as a “kernel” for jupyter notebooks\nconda activate %env_name%\npython -m ipykernel install --user --name %env_name% --display-name \"HFC\"\nFrom here on all commands are done from within this new conda environment\nYou may already have jupyter-lab installed in another conda environment. You may use it to run ‘hydrofc’ notebooks. If not, install jupyter-lab in this new environment with:\nmamba install -c conda-forge jupyterlab\nYou may need to install some additional conda packages depending on the notebooks you are using.\nWe can now install “our” packages. we can install from ‘wheels’, or from source code (for developers)\nTwo dependencies refcount and cinterop are on pypi, but should also be in the zip archive as wheels as well; prefer the latter.\nIf installed from the Windows installer, you should have the wheels in a folder such as “C:\\Users\\xxxyyy\\AppData\\Local\\Programs\\SF\\python” if you have chosen the default option for an destination folder.\nTo install “wheels”, if you got them from a zip file:\ncd  C:\\tmp\\sf\n7z x python.7z\nor\ncd C:\\Users\\xxxyyy\\AppData\\Local\\Programs\\SF\\python\nIn that folder, there may be a batch file installpkgs.bat that can be called to install the streamflow forecasting packages in your new conda environment.\nOtherwise, a more manual approach is to adapt the following lines:\n:: Adapt the following to the versions you have.\npip install --force-reinstall --no-deps refcount-0.9.3-py2.py3-none-any.whl\npip install --force-reinstall --no-deps cinterop-0.9.0-py2.py3-none-any.whl\npip install --force-reinstall --no-deps uchronia-2.3.7-py2.py3-none-any.whl\npip install --force-reinstall --no-deps swift2-2.3.7-py2.py3-none-any.whl\npip install --force-reinstall --no-deps fogss-0.3-py2.py3-none-any.whl",
    "crumbs": [
      "Installation",
      "Installation: Windows"
    ]
  },
  {
    "objectID": "doc/installation/install_windows.html#matlab-functions",
    "href": "doc/installation/install_windows.html#matlab-functions",
    "title": "Installation: Windows",
    "section": "Matlab functions",
    "text": "Matlab functions\n\n\n\n\n\n\nWarning\n\n\n\nThis section is a draft, indicative only.\nOur MATLAB bindings are known to work very well, but the requirements from MATLAB for C/C++ compilers can be fiddly to figure out and set up, and this section has not been fully validated recently.\n\n\nIf installed from the Windows installer, you should have the MATLAB functions in a folder such as “C:\\Users\\xxxyyy\\AppData\\Local\\Programs\\SF\\matlab” if you have chosen the default option for an destination folder.\nYou will find bindings for SWIFT and FoGSS.\n\nC/C++ compiler Visual Studio\nIf you have Visual Studio with the C++ compiler installed on your machine, chances are that MATLAB can detect and use it. Otherwise using the MinGW64 toolchain has also been successfully tested.\n\n\nInstalling Mingw64\nRead the MATLAB Support for MinGW-w64 C/C++ Compiler page. You will find some information on versions of GCC you need depending on the version of MATLAB you are using.\nHead to Mingw-w64 and the download section for “MingW-W64-builds”\nOlder versions of MATLAB may require versions of GCC that are more difficult to obtain. Around 2017 MATLAB required GCC version 5.3 which appears harder to obtain as of 2023. Users with older versions of MATLAB may find useful material in this MinGW-w64 - for 32 and 64 bit Windows Files on Sourceforge. This is indicative only and download and use is for you to judge. As a hint, for matlab 2017 an installer version x86_64-5.3.0-posix-seh-rt_v4-rev0 was needed.\n\n\nCompiling the MEX code for swift\nIn order to capture error messages from the swift library the matlab code needs a bit of MEX code. At the matlab prompt:\nsetenv('MW_MINGW64_LOC', 'C:\\Users\\xxxyyy\\prog\\mingw-w64\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\mingw64')\ncd('C:/src/csiro/stash/swift/bindings/matlab/interop')\nmex -L\"C:/localdev/libs/64\" -lswift registerSwiftExceptionCallback.cpp\n\n%% I seemed to need the addpath calls for  createSwiftPrototype to work\nbindingsLocation = 'C:/src/csiro/stash/swift/bindings/matlab';\naddpath(fullfile(bindingsLocation, 'datatypes'));\naddpath(fullfile(bindingsLocation, 'native'));\naddpath(fullfile(bindingsLocation, 'interop'));\naddpath(fullfile(bindingsLocation, 'external'));\naddpath('c:/localdev/libs/64');\naddpath('c:/local/libs/64');\ncd('C:/src/csiro/stash/swift/bindings/matlab/native')\ncreateSwiftPrototype()",
    "crumbs": [
      "Installation",
      "Installation: Windows"
    ]
  }
]