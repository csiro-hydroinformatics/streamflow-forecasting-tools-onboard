[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Streamflow Forecasting",
    "section": "",
    "text": "This site is a public entry point for people interested in a suite of hydrologic ensemble forecasting modelling tools developped by the CSIRO over the past decade. While featuring unique features for ensemble streamflow forecasting, they can equally be used in non-ensemble simulation mode for other semi-distributed hydrologic modelling.\n\nApplications\nA subset of the tools is used by the Australian Bureau of Meteorology to provide the 7-day Ensemble Streamflow Forecasts service.\nMore recent versions of the tools are also used for research and development. The core modelling features are usually native (C++) libraries, accessed seamlessly by user from R, Python and Matlab.\n\n\n\n\n\nEnsemble forecasting simulation\n\n\nHands-on hydrologic modellers can browse through some sample modelling workflows in R package vignettes to get an idea of the features.\n\n\nInstallation\nPackages are available for installation on Windows, Debian/Ubuntu Linux, and MacOSX. They currently cannot all be made publicly downloadable, but access for evaluation or research purposes can usually easily be arranged upon request. Contact David Robertson at david.robertson@csiro.au and/or jean-michel.perraud@csiro.au. Then follow the instructions at the Installation page.\n\n\nArchitecture\n\n\n\n\n\nSoftware stack\n\n\nThis documentation introduces a set of software tools for ensemble streamflow forecasting techniques. The core is written in C++, but in practice users access the features via packages in R, python, or Matlab.\nAn overview of the research and outcomes is available at this page.\nA high level description of the software stack with the main components follows. This stack is built mostly on open source software; see csiro-hydroinformatics, notably uchronia-time-series. We also intend to open source the semi-distributed modelling engine.\n\n\nGetting started\nIn R you can access samples via vignettes in the packages:\nlibrary(swift)\nlibrary(uchronia)\nbrowseVignettes('swift')\nYou can access the wider package documentation using the command ?swift, and navigate down the page to the footer [Package swift version 0.7.7 Index] (yes, admitedly there should be an easier way…). Click on the Index hyperlink of that footer. Note that you will find a very long list of functions but functions postfixed _R should not be used as primary building blocks in your scripts.\n\n\nTroubleshooting\n\n\nFAQ\n\n\nAcknowledgements\nMost of the tools covered by this documentation is the output of research supported by the Water Information Research and Development Alliance between CSIRO and the Australian Bureau of Meteorology.\n\n\nPublications\nSWIFT2: High performance software for short-medium term ensemble streamflow forecasting research and operations\nSWIFT2: Advanced software for continuous ensemble short-term streamflow forecasting"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "Software packages are not all publicly downloadable, but access for evaluation or research purposes can easily be arranged upon request. Contact jean-michel.perraud@csiro.au and/or david.robertson@csiro.au. Once you have received a link to downloadable packages and files, follow the instructions for your platform:\n\nInstalling on Window\nInstalling on Linux\nInstalling on MacOSX\n\nWe intend to also provide in the future a docker container"
  },
  {
    "objectID": "documentation.html",
    "href": "documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Pointers to further technical documentation for using or developing features."
  },
  {
    "objectID": "documentation.html#build-pipelines",
    "href": "documentation.html#build-pipelines",
    "title": "Documentation",
    "section": "Build pipelines",
    "text": "Build pipelines\nThe source code for Azure Devops pipelines are also available as of 2023-01 from github. Their public availability may change in the future though.\n\nhydro-forecast-windows-pipeline\nhydro-forecast-linux-pipeline"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "doc/notes.html",
    "href": "doc/notes.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "# cd ~/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes\n# R\n\nsetwd('C:/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes')\n\nlibrary(rmarkdown)\n\ninfn <- c(\n\"calibrate_multisite\"         ,    \"calibrate_subcatchments\"        ,\n\"calibration_initial_states\"  ,    \"ensemble_forecast_model_runs\"   ,\n\"ensemble_model_runs\"         ,    \"error_correction_four_stages\"   ,\n\"getting_started\"             ,    \"log_likelihood\"                 ,\n\"meta_parameters\"             ,    \"muskingum_multilink_calibration\"\n)\n\n\nf <- function(fn)\n{\n    input <- paste(fn, 'Rmd', sep='.')\n    output_format <- 'github_document'\n\n    output_dir <- fn\n\n    rmarkdown::render(input, output_format, output_file = NULL, output_dir,\n                output_options = NULL, intermediates_dir = NULL,\n                knit_root_dir = NULL,\n                runtime = c(\"auto\", \"static\", \"shiny\", \"shiny_prerendered\"),\n                clean = TRUE, params = NULL, knit_meta = NULL,\n                run_pandoc = TRUE, quiet = FALSE)\n}\n\nlapply(infn, FUN=f)\n\n\nfile.remove(list.files('.', pattern=\"*.html\", full.names=TRUE, recursive=TRUE))\nfile.copy(infn, '~/src/github_jm/streamflow-forecasting-tools-onboard/doc/vignettes/', recursive=TRUE)\n\nfile.copy(infn, 'c:/src/github_jm/streamflow-forecasting-tools-onboard/doc/vignettes/', recursive=TRUE)\ngiven what I get from this issue I am not sure it is possible to get relative paths to figures in the markdown documents. Have to use full text search/replace to correct.\nNote the regex pattern to use:\n/home/xxxxxx/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes/[a-z_]*/\nC:/src/csiro/stash/swift/bindings/R/pkgs/swift/vignettes/[a-z_]*/\nand needs to be replaced with ./"
  },
  {
    "objectID": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html",
    "href": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Jean-Michel Perraud 2020-01-28"
  },
  {
    "objectID": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html#about-this-document",
    "href": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html#about-this-document",
    "title": "Streamflow Forecasting Onboarding",
    "section": "About this document",
    "text": "About this document\nThis document was generated from an R markdown file on 2020-01-28 10:52:38.\nLi, Ming; Wang, QJ; Bennett, James; Robertson, David. Error reduction and representation in stages (ERRIS) in hydrological modelling for ensemble streamflow forecasting. Hydrology and Earth System Sciences. 2016; 20:3561-3579. https://doi.org/10.5194/hess-20-3561-2016"
  },
  {
    "objectID": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html#calibrating-erris",
    "href": "doc/samples/R/vignettes/error_correction_four_stages/error_correction_four_stages.html#calibrating-erris",
    "title": "Streamflow Forecasting Onboarding",
    "section": "Calibrating ERRIS",
    "text": "Calibrating ERRIS\n\nModel structure\nWe use sample hourly data from the Adelaide catchment this catchment in the Northern Territory, TBC. The catchment model set up is not the key point of this vignette so we do not comment on that section:\nlibrary(swift)\ncatchmentStructure <- sampleCatchmentModel(siteId = \"Adelaide\", configId = \"catchment\")\n\nhydromodel <- \"GR4J\";\nchannel_routing <- 'MuskingumNonLinear';\nhydroModelRainfallId <-'P'\nhydroModelEvapId <-'E'\n\n# set models\ninsimulation <- swapModel(catchmentStructure, modelId = hydromodel ,what = \"runoff\")\nsimulation <- swapModel(insimulation, modelId = channel_routing ,what = \"channel_routing\")\n\nsaId <- getSubareaIds(simulation)\nstopifnot(length(saId) == 1)\n\nprecipTs <- sampleSeries(siteId = \"Adelaide\", varName = \"rain\")\nevapTs <- sampleSeries(siteId = \"Adelaide\", varName = \"evap\")\nflowRateTs <- sampleSeries(siteId = \"Adelaide\", varName = \"flow\")\n\nplayInput(simulation, precipTs, mkFullDataId('subarea', saId, hydroModelRainfallId))\nplayInput(simulation, evapTs, mkFullDataId('subarea', saId, hydroModelEvapId))\nconfigureHourlyGr4j(simulation)\nsetSimulationTimeStep(simulation, 'hourly')\n\n# Small time interval only, to reduce runtimes in this vignette\nsimstart <- uchronia::mkDate(2010,12,1)  \nsimend <- uchronia::mkDate(2011,6,30,23)  \nsimwarmup <- simstart\nsetSimulationSpan(simulation, simstart, simend)\ntemplateHydroParameterizer <- function(simulation) {\n  calibragem::defineParameterizerGr4jMuskingum(refArea=250, timeSpan=3600L, simulation=simulation, paramNameK=\"Alpha\", objfun='NSE', deltaT=1L)\n}\n\nnodeId <- 'node.2'\nflowId <- mkFullDataId(nodeId, 'OutflowRate')\n\nrecordState(simulation, flowId)\nWe use pre-calibrated hydrologic parameters (reproducible with doc/error_correction_doc_preparation.r in this package structure)\np <- templateHydroParameterizer(simulation)\nsetMinParameterValue(p, 'R0', 0.0)\nsetMaxParameterValue(p, 'R0', 1.0)\nsetMinParameterValue(p, 'S0', 0.0)\nsetMaxParameterValue(p, 'S0', 1.0)\nSetParameterValue_R( p, 'log_x4', 1.017730e+00)\nSetParameterValue_R( p, 'log_x1', 2.071974e+00  )\nSetParameterValue_R( p, 'log_x3', 1.797909e+00  )\nSetParameterValue_R( p, 'asinh_x2', -1.653842e+00)  \nSetParameterValue_R( p, 'R0', 2.201930e-11  )\nSetParameterValue_R( p, 'S0', 3.104968e-11  )\nSetParameterValue_R( p, 'X', 6.595537e-03   ) # Gotcha: needs to be set before alpha is changed.\nSetParameterValue_R( p, 'Alpha', 6.670534e-01   )\nparameterizerAsDataFrame(p)\n##       Name         Min        Max         Value\n## 1   log_x4  0.00000000 2.38021124  1.017730e+00\n## 2   log_x1  0.00000000 3.77815125  2.071974e+00\n## 3   log_x3  0.00000000 3.00000000  1.797909e+00\n## 4 asinh_x2 -3.98932681 3.98932681 -1.653842e+00\n## 5       R0  0.00000000 1.00000000  2.201930e-11\n## 6       S0  0.00000000 1.00000000  3.104968e-11\n## 7        X  0.00100000 0.01662228  6.595537e-03\n## 8    Alpha  0.01116157 1.68112917  6.670534e-01\nsViz <- uchronia::mkDate(2010,12,1)\neViz <- uchronia::mkDate(2011,4,30,23)\n\noneWetSeason <- function(tts) {\n    window(tts, start=sViz, end=eViz) \n}\n\nobsVsCalc <- function(obs, calc, ylab=\"flow (m3/s)\") {\n  obs <- oneWetSeason(obs)\n  calc <- oneWetSeason(calc)\n    joki::plotTwoSeries(obs, calc, ylab=ylab, startTime = start(calc), endTime = end(calc))\n}\n\napplySysConfig(p, simulation)\nexecSimulation(simulation)\nobsVsCalc(flowRateTs, getRecorded(simulation, flowId))\n\n\n\nSet up the error correction model\nlist(getNodeIds(simulation), \ngetNodeNames(simulation))\n## [[1]]\n## [1] \"2\" \"1\"\n## \n## [[2]]\n## [1] \"Outlet\" \"Node_1\"\nerrorModelElementId <- 'node.2';\nsetErrorCorrectionModel(simulation, 'ERRIS', errorModelElementId, length=-1, seed=0)\n\nflowRateTsGapped <- flowRateTs\nflowRateTsGapped['2011-02'] <- NA\n\n# plot(flowRateTsGapped)\n\nplayInput(simulation,flowRateTsGapped,varIds=mkFullDataId(errorModelElementId,\"ec\",\"Observation\"))\nNow, prepare a model with error correction, and set up for generation\necs <- swift::CloneModel_R(simulation)\n\nsetStateValue(ecs,mkFullDataId(nodeId,\"ec\",\"Generating\"),FALSE)\nupdatedFlowVarID <- mkFullDataId(nodeId,\"ec\",\"Updated\")\ninputFlowVarID <- mkFullDataId(nodeId,\"ec\",\"Input\")\nrecordState(ecs,varIds=c(updatedFlowVarID, inputFlowVarID))\n\n\nERRIS calibration in stages\n#termination <- getMaxRuntimeTermination(0.005)\ntermination <- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.05','0.0167'))\nWe could set up a four-stages estimation in one go, but we will instead work in each stages for didactic purposes.\ncensOpt = 0.0\nestimator <- createERRISParameterEstimator (simulation, flowRateTs, errorModelElementId,\n                                            estimationStart = simstart, estimationEnd=simend, censThr=0.0, censOpt,\n                                            termination, restrictionOn=TRUE, weightedLeastSquare=FALSE)\n\nstageOnePset = CalibrateERRISStageOne_R(estimator)\nprint(parameterizerAsDataFrame(stageOnePset))\n##              Name    Min    Max       Value\n## 1         Epsilon  -20.0    0.0   -8.676488\n## 2          Lambda  -30.0    5.0   -1.451523\n## 3               D    0.0    0.0    0.000000\n## 4              Mu    0.0    0.0    0.000000\n## 5             Rho    0.0    0.0    0.000000\n## 6  Sigma1_Falling    0.0    0.0    0.000000\n## 7   Sigma1_Rising    0.0    0.0    0.000000\n## 8  Sigma2_Falling    0.0    0.0    0.000000\n## 9   Sigma2_Rising    0.0    0.0    0.000000\n## 10 Weight_Falling    1.0    1.0    1.000000\n## 11  Weight_Rising    1.0    1.0    1.000000\n## 12        CensThr    0.0    0.0    0.000000\n## 13        CensOpt    0.0    0.0    0.000000\n## 14         MaxObs 1126.3 1126.3 1126.300000\n\nStage 2\nStage two can be logged:\nSetERRISVerboseCalibration_R(estimator, TRUE)\n\nstageTwoPset = CalibrateERRISStageTwo_R(estimator, stageOnePset)\nprint(parameterizerAsDataFrame(stageTwoPset))\n##              Name         Min         Max        Value\n## 1               D    0.000000    2.000000    0.7386187\n## 2              Mu -100.000000  100.000000   -3.5225392\n## 3   Sigma1_Rising   -6.907755    6.907755    0.9109061\n## 4         CensOpt    0.000000    0.000000    0.0000000\n## 5         CensThr    0.000000    0.000000    0.0000000\n## 6         Epsilon   -8.676488   -8.676488   -8.6764883\n## 7          Lambda   -1.451523   -1.451523   -1.4515233\n## 8          MaxObs 1126.300000 1126.300000 1126.3000000\n## 9             Rho    0.000000    0.000000    0.0000000\n## 10 Sigma1_Falling    0.000000    0.000000    0.0000000\n## 11 Sigma2_Falling    0.000000    0.000000    0.0000000\n## 12  Sigma2_Rising    0.000000    0.000000    0.0000000\n## 13 Weight_Falling    1.000000    1.000000    1.0000000\n## 14  Weight_Rising    1.000000    1.000000    1.0000000\nmkEcIds <- function(p) {\n  df <- parameterizerAsDataFrame(p)\n  df$Name <- mkFullDataId(nodeId, 'ec', df$Name)\n  createParameterizer('Generic',df)\n}\n\napplySysConfig(mkEcIds(stageTwoPset), ecs)\nexecSimulation(ecs)\nobsVsCalc(flowRateTsGapped, getRecorded(ecs, updatedFlowVarID))\n\nA helper function to process the calibration log:\nprepOptimLog <- function(estimator, fitnessName = \"Log.likelihood\")\n{\n  optimLog = getLoggerContent(estimator)\n  # head(optimLog)\n  optimLog$PointNumber = 1:nrow(optimLog)   \n  logMh <- mhplot::mkOptimLog(optimLog, fitness = fitnessName, messages = \"Message\", categories = \"Category\") \n  geomOps <- mhplot::subsetByMessage(logMh)\n  d <- list(data=logMh, geomOps=geomOps)\n}\nd <- prepOptimLog(estimator, fitnessName = \"Log.likelihood\")\nprint(mhplot::plotParamEvolution(d$geomOps, 'Sigma1_Rising', c(0, max(d$data@data$Log.likelihood))))\n\n\n\nStage 3\nstageThreePset = CalibrateERRISStageThree_R(estimator, stageTwoPset)\nprint(parameterizerAsDataFrame(stageThreePset))\n##              Name          Min          Max        Value\n## 1             Rho    0.0000000    1.0000000    0.9880881\n## 2   Sigma1_Rising   -6.9077553    6.9077553   -0.9134793\n## 3         CensOpt    0.0000000    0.0000000    0.0000000\n## 4         CensThr    0.0000000    0.0000000    0.0000000\n## 5               D    0.7386187    0.7386187    0.7386187\n## 6         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 7          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 8          MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 9              Mu   -3.5225392   -3.5225392   -3.5225392\n## 10 Sigma1_Falling    0.0000000    0.0000000    0.0000000\n## 11 Sigma2_Falling    0.0000000    0.0000000    0.0000000\n## 12  Sigma2_Rising    0.0000000    0.0000000    0.0000000\n## 13 Weight_Falling    1.0000000    1.0000000    1.0000000\n## 14  Weight_Rising    1.0000000    1.0000000    1.0000000\nd <- prepOptimLog(estimator, fitnessName = \"Log.likelihood\")\nprint(mhplot::plotParamEvolution(d$geomOps, 'Rho', c(0, max(d$data@data$Log.likelihood))))\n\n\n\nStage 3a, generating and fitting M and S if free\nstageThreePsetMS = CalibrateERRISStageThreeMS_R(estimator, stageThreePset)\nprint(parameterizerAsDataFrame(stageThreePsetMS))\n##              Name          Min          Max        Value\n## 1             Rho    0.0000000    1.0000000    0.9880881\n## 2   Sigma1_Rising   -6.9077553    6.9077553   -0.9134793\n## 3         CensOpt    0.0000000    0.0000000    0.0000000\n## 4         CensThr    0.0000000    0.0000000    0.0000000\n## 5               D    0.7386187    0.7386187    0.7386187\n## 6         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 7          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 8          MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 9              Mu   -3.5225392   -3.5225392   -3.5225392\n## 10 Sigma1_Falling    0.0000000    0.0000000    0.0000000\n## 11 Sigma2_Falling    0.0000000    0.0000000    0.0000000\n## 12  Sigma2_Rising    0.0000000    0.0000000    0.0000000\n## 13 Weight_Falling    1.0000000    1.0000000    1.0000000\n## 14  Weight_Rising    1.0000000    1.0000000    1.0000000\n## 15         MNoise -100.0000000  100.0000000   -2.5833943\n## 16         SNoise  -10.0000000   10.0000000    1.8858371\napplySysConfig(mkEcIds(stageThreePsetMS), ecs)\nexecSimulation(ecs)\nobsVsCalc(flowRateTsGapped, getRecorded(ecs, updatedFlowVarID))\n\n\n\nStage 4, rising limb\nstageFourPsetRising = CalibrateERRISStageFour_R(estimator, stageThreePsetMS, useRising = TRUE)\nprint(parameterizerAsDataFrame(stageFourPsetRising))\n##              Name          Min          Max        Value\n## 1   Sigma1_Rising   -6.9077553    6.9077553   -1.0822514\n## 2   Sigma2_Rising   -6.9077553    6.9077553    0.6662236\n## 3   Weight_Rising    0.5000000    1.0000000    0.8770708\n## 4         CensOpt    0.0000000    0.0000000    0.0000000\n## 5         CensThr    0.0000000    0.0000000    0.0000000\n## 6               D    0.7386187    0.7386187    0.7386187\n## 7         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 8          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 9          MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 10             Mu   -3.5225392   -3.5225392   -3.5225392\n## 11            Rho    0.9880881    0.9880881    0.9880881\n## 12 Sigma1_Falling    0.0000000    0.0000000    0.0000000\n## 13 Sigma2_Falling    0.0000000    0.0000000    0.0000000\n## 14 Weight_Falling    1.0000000    1.0000000    1.0000000\nd <- prepOptimLog(estimator, fitnessName = \"Log.likelihood\")\nprint(mhplot::plotParamEvolution(d$geomOps, 'Weight_Rising', c(0, max(d$data@data$Log.likelihood))))\n\napplySysConfig(mkEcIds(stageFourPsetRising), ecs)\nexecSimulation(ecs)\nobsVsCalc(flowRateTsGapped, getRecorded(ecs, updatedFlowVarID))\n\n\n\nStage 4, falling limbs\nstageFourPsetFalling = CalibrateERRISStageFour_R(estimator, stageThreePsetMS, useRising = FALSE)\nprint(parameterizerAsDataFrame(stageFourPsetFalling))\n##              Name          Min          Max        Value\n## 1   Sigma1_Rising   -6.9077553    6.9077553   -3.1021521\n## 2   Sigma2_Rising   -6.9077553    6.9077553   -0.6290198\n## 3   Weight_Rising    0.5000000    1.0000000    0.8131939\n## 4         CensOpt    0.0000000    0.0000000    0.0000000\n## 5         CensThr    0.0000000    0.0000000    0.0000000\n## 6               D    0.7386187    0.7386187    0.7386187\n## 7         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 8          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 9          MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 10             Mu   -3.5225392   -3.5225392   -3.5225392\n## 11            Rho    0.9880881    0.9880881    0.9880881\n## 12 Sigma1_Falling    0.0000000    0.0000000    0.0000000\n## 13 Sigma2_Falling    0.0000000    0.0000000    0.0000000\n## 14 Weight_Falling    1.0000000    1.0000000    1.0000000\nNd <- prepOptimLog(estimator, fitnessName = \"Log.likelihood\")\nprint(mhplot::plotParamEvolution(d$geomOps, 'Weight_Rising', c(0, max(d$data@data$Log.likelihood))))\n\n\n\nFinal consolidated parameter set\nfinalPset = ConcatenateERRISStagesParameters_R(estimator, hydroParams = createParameterizer(), stage1_result =  stageOnePset, stage2_result = stageTwoPset, \n                                   stage3_result = stageThreePsetMS, stage4a_result = stageFourPsetRising, stage4b_result = stageFourPsetFalling, toLongParameterName = FALSE)\n\nprint(parameterizerAsDataFrame(finalPset))\n##              Name          Min          Max        Value\n## 1         CensThr    0.0000000    0.0000000    0.0000000\n## 2         CensOpt    0.0000000    0.0000000    0.0000000\n## 3          MNoise -100.0000000  100.0000000   -2.5833943\n## 4          SNoise  -10.0000000   10.0000000    1.8858371\n## 5          Lambda   -1.4515233   -1.4515233   -1.4515233\n## 6         Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 7              Mu   -3.5225392   -3.5225392   -3.5225392\n## 8               D    0.7386187    0.7386187    0.7386187\n## 9             Rho    0.9880881    0.9880881    0.9880881\n## 10         MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 11  Sigma1_Rising   -6.9077553    6.9077553   -1.0822514\n## 12  Sigma2_Rising   -6.9077553    6.9077553    0.6662236\n## 13  Weight_Rising    0.5000000    1.0000000    0.8770708\n## 14 Sigma1_Falling   -6.9077553    6.9077553   -3.1021521\n## 15 Sigma2_Falling   -6.9077553    6.9077553   -0.6290198\n## 16 Weight_Falling    0.5000000    1.0000000    0.8131939\n\n\n\nLegacy call\nCheck that the previous “one stop shop” call gives the same results.\ndummyDate <- simstart\n\npsetFullEstimate <- estimateERRISParameters(simulation, flowRateTs, errorModelElementId,\n  warmupStart=dummyDate, warmupEnd=dummyDate, warmup=FALSE, estimationStart = simstart, estimationEnd=simend, censThr=0.0,\n censOpt = censOpt, exclusionStart=dummyDate, exclusionEnd=dummyDate, exclusion=FALSE, terminationCondition = termination,\n  hydroParams = NULL, errisParams = NULL, restrictionOn = TRUE,\n  weightedLeastSquare = FALSE)\n\nprint(parameterizerAsDataFrame(psetFullEstimate))\n##                        Name          Min          Max        Value\n## 1         node.2.ec.CensThr    0.0000000    0.0000000    0.0000000\n## 2         node.2.ec.CensOpt    0.0000000    0.0000000    0.0000000\n## 3          node.2.ec.MNoise -100.0000000  100.0000000   -2.5833943\n## 4          node.2.ec.SNoise  -10.0000000   10.0000000    1.8858371\n## 5          node.2.ec.Lambda   -1.4515233   -1.4515233   -1.4515233\n## 6         node.2.ec.Epsilon   -8.6764883   -8.6764883   -8.6764883\n## 7              node.2.ec.Mu   -3.5225392   -3.5225392   -3.5225392\n## 8               node.2.ec.D    0.7386187    0.7386187    0.7386187\n## 9             node.2.ec.Rho    0.9880881    0.9880881    0.9880881\n## 10         node.2.ec.MaxObs 1126.3000000 1126.3000000 1126.3000000\n## 11  node.2.ec.Sigma1_Rising   -6.9077553    6.9077553   -1.0822514\n## 12  node.2.ec.Sigma2_Rising   -6.9077553    6.9077553    0.6662236\n## 13  node.2.ec.Weight_Rising    0.5000000    1.0000000    0.8770708\n## 14 node.2.ec.Sigma1_Falling   -6.9077553    6.9077553   -3.1021521\n## 15 node.2.ec.Sigma2_Falling   -6.9077553    6.9077553   -0.6290198\n## 16 node.2.ec.Weight_Falling    0.5000000    1.0000000    0.8131939"
  },
  {
    "objectID": "doc/samples/R/vignettes/meta_parameters/meta_parameters.html",
    "href": "doc/samples/R/vignettes/meta_parameters/meta_parameters.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Calibrating tied meta parameters\nJean-Michel Perraud 2020-01-28\n\n\nSample code to define meta parameter sets over a catchment\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:54:59. It illustrates how to set up a calibration where a global parameterization is set at the catchment level, with scaled values for each subareas. This method helps to keep the degrees of freedom of an optimisation to a minimum.\n\n\nGetting started\nlibrary(swift)\nWe need to adjust the observed flow, as the SWIFTv1 legacy missing value code is not consistent with default handling in SAK. Note that I think the flow included in the sample data is the Ovens catchment outlet, but I am not sure!\nflow <- sampleSeries('Abbeyard', 'flow')\nflow[which(flow < -1)] <- NA\nWe create a system with areas similar to the real use case, but do note that test catchment structure is just an arbitrary default one, suitable for this example, but probably not a valid model.\nareasKm2 <- c(91.2627, 95.8716, 6.5610, 128.4822, 93.0042)\nmss <- createTestCatchmentStructure(areasKm2 = areasKm2)\nms <- mss$model\nms <- swapModel(ms, 'Muskingum', 'channel_routing')\nWe will run over a few years and calibrate with a warmup of two years.\ne <- end(flow) - lubridate::ddays(2)\nw <- e - lubridate::dyears(10)\ns <- w - lubridate::dyears(2)\nms <- configureTestSimulation(ms, dataId = \"Ovens\", simulStart = s, \n    simulEnd = e, tstep = \"hourly\", \n    varNameRain = \"P\", varNamePet = \"E\",\n    varNameDataRain = 'rain', varNameDataPet = 'evap') \nThe package includes a function that flags possible inconsistencies prior to running a model (inconsistent time steps, etc.)\ncheckSimulation(ms)\n## $errors\n## character(0)\nWe need to adjust a couple of parameters for proper operation on hourly data for the GR4 model structure.\npGr4jHourly <- createGr4jhParameters()\nparameterizerAsDataFrame(pGr4jHourly)\n##         Name  Min  Max Value\n## 1 PercFactor 4.00 4.00  4.00\n## 2 UHExponent 1.25 1.25  1.25\napplySysConfig(pGr4jHourly, ms)\nWe now define a meta parameter set with area scaling applied to x4 and time scaling applied to x2 and x3.\nrefArea <- 250\ntimeSpan <- as.integer(lubridate::dhours(1))\np <- gr4jScaledParameterizer(refArea, timeSpan)\n(pSpecGr4j <- joki::getFreeParams('GR4J'))\n##   Name      Value Min  Max\n## 1   x1 650.488000   1 3000\n## 2   x2  -0.280648 -27   27\n## 3   x3   7.891230   1  660\n## 4   x4  18.917200   1  240\npSpecGr4j$Min <- c(1.0E+00, -2.70E+01, 1.0E+00, 1.0E+00)\npSpecGr4j$Max <- c(5.0E+03,  2.70E+01, 6.6E+02, 2.4E+02)\nsetHyperCube(p, pSpecGr4j)\n\np <- wrapTransform(p)\naddTransform(p, 'log_x4', 'x4', 'log10')\nWe can inspect the values of one of the subareas to check that the parameter values applied are indeed scaled. For instance x4 is scaled based on the area\nparameterizerAsDataFrame(p)\n##     Name Min         Max      Value\n## 1 log_x4   0    2.380211   1.276857\n## 2     x2 -27   27.000000  -0.280648\n## 3     x3   1  660.000000   7.891230\n## 4     x1   1 5000.000000 650.488000\nsubareaIds <- paste('subarea', getSubareaIds(ms), sep='.')\nareas <- getStateValue(ms, paste(subareaIds, 'areaKm2', sep='.') )\nx4ParamIds <- paste(subareaIds, 'x4', sep='.')\ngetStateValue(ms, x4ParamIds)\n## subarea.lnk1.x4 subarea.lnk2.x4 subarea.lnk3.x4 subarea.lnk4.x4 \n##             0.5             0.5             0.5             0.5 \n## subarea.lnk5.x4 \n##             0.5\napplySysConfig(p, ms)\ngetStateValue(ms, x4ParamIds)\n## subarea.lnk1.x4 subarea.lnk2.x4 subarea.lnk3.x4 subarea.lnk4.x4 \n##       11.429665       11.714718        3.064586       13.561519 \n## subarea.lnk5.x4 \n##       11.538202\nBuild the definition of the optimisation task. TODO: improve ways to search for element keys by element names.\noutflowVarname <- \"Catchment.StreamflowRate\"\nrecordState(ms, outflowVarname)\nexecSimulation(ms)\nlibrary(lubridate)\ncalc <- getRecorded(ms, outflowVarname)\njoki::plotTwoSeries(flow, calc, startTime=end(flow)-lubridate::years(3), endTime=end(flow))\n\nobjective <- createObjective(ms, outflowVarname, flow, 'NSE', w, e)\nscore <- getScore(objective, p)\nprint(score)\n## $scores\n##       NSE \n## -3.536795 \n## \n## $sysconfig\n##     Name Min         Max      Value\n## 1 log_x4   0    2.380211   1.276857\n## 2     x2 -27   27.000000  -0.280648\n## 3     x3   1  660.000000   7.891230\n## 4     x1   1 5000.000000 650.488000\nWe have our objectives defined, and the parameter space ‘p’ in which to search. Let’s create an optimizer and we are ready to go. While the optimizer can be created in one line, we show how to choose one custom termination criterion and how to configure the optimizer to capture a detailed log of the process.\nif(Sys.getenv('SWIFT_FULL') != \"\") {\n  maxHours = 0.2 \n} else {\n  maxHours = 0.02\n}\n# term <- getMarginalTermination(tolerance = 1e-05, cutoffNoImprovement = 30, maxHours = maxHours) \nterm <- getMaxRuntimeTermination(maxHours = maxHours) \nsceParams <- getDefaultSceParameters()\nurs <- createParameterSampler(0, p, 'urs')\noptimizer <- createSceOptimSwift(objective, term, SCEpars=sceParams, urs)\ncalibLogger <- setCalibrationLogger(optimizer, '')\nAt this point you may want to specify the maximum number of cores that can be used by the optimizer, for instance if you wish to keep one core free to work in parallel on something else.\n# TODO add an API entry point for SetMaxDegreeOfParallelismHardwareMinus\nstartTime <- lubridate::now()\ncalibResults <- executeOptimization(optimizer)\nendTime <- lubridate::now()\ncalibWallTime <- endTime-startTime\nprint(paste( 'Optimization completed in ', calibWallTime, attr(calibWallTime, 'units')))\n## [1] \"Optimization completed in  1.20513858397802 mins\"\nProcessing the calibration log:\nd <- getLoggerContent(optimizer)\nd$PointNumber = 1:nrow(d)\nlogMh <- mhplot::mkOptimLog(d, fitness = \"NSE\", messages = \"Message\", categories = \"Category\") \ngeomOps <- mhplot::subsetByMessage(logMh)\nstr(geomOps@data)\n## 'data.frame':    1198 obs. of  9 variables:\n##  $ Category      : Factor w/ 7 levels \"Complex No 0\",..: 7 7 7 7 7 7 7 7 7 7 ...\n##  $ CurrentShuffle: Factor w/ 16 levels \"\",\"0\",\"1\",\"10\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Message       : Factor w/ 5 levels \"Adding a random point in hypercube\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ NSE           : num  -1.20e+04 1.31e-01 -1.29e-04 -1.02 -3.22e+03 ...\n##  $ log_x4        : num  2.256 0.414 0.509 1.724 2.08 ...\n##  $ x1            : num  3366 1996 377 4325 1731 ...\n##  $ x2            : num  20.7 -6.55 -6.36 5.99 25.78 ...\n##  $ x3            : num  41.4 469 82.6 614.2 83.7 ...\n##  $ PointNumber   : int  1 2 3 4 5 6 7 8 9 10 ...\nWe can then visualize how the calibration evolved. There are several types of visualisations included in the mhplot package, and numerous customizations possible, but starting with the overall population evolution:\npVarIds <- (parameterizerAsDataFrame(p))$Name\nfor (pVarId in pVarIds) {\n    print(mhplot::plotParamEvolution(geomOps, pVarId, objLims=c(0,1)))\n}\n\nsortedResults <- sortByScore(calibResults, 'NSE')\nbestPset <- getScoreAtIndex(sortedResults, 1)\nbestPset <- GetSystemConfigurationWila_R(bestPset)\nswift can back-transform a parameters to obtain the untransformed parameter set(s):\nuntfPset <- backtransform(bestPset)\n(score <- getScore(objective, bestPset))\n## $scores\n##       NSE \n## 0.6221243 \n## \n## $sysconfig\n##     Name Min         Max      Value\n## 1 log_x4   0    2.380211   1.488763\n## 2     x2 -27   27.000000 -12.748777\n## 3     x3   1  660.000000 147.210803\n## 4     x1   1 5000.000000 548.429824\n(score <- getScore(objective, untfPset))\n## $scores\n##       NSE \n## 0.6221243 \n## \n## $sysconfig\n##   Name Min  Max     Value\n## 1   x2 -27   27 -12.74878\n## 2   x3   1  660 147.21080\n## 3   x4   1  240  30.81507\n## 4   x1   1 5000 548.42982\nFinally, let’s have a visual of the fitted streamflow data at Abbeyard:\napplySysConfig(bestPset, ms)\nexecSimulation(ms)\nmodRunoff <- getRecorded(ms, outflowVarname)\njoki::plotTwoSeries(flow, modRunoff, startTime=end(modRunoff)-lubridate::years(3), endTime=end(modRunoff))"
  },
  {
    "objectID": "doc/samples/R/vignettes/ensemble_forecast_model_runs/ensemble_forecast_model_runs.html",
    "href": "doc/samples/R/vignettes/ensemble_forecast_model_runs/ensemble_forecast_model_runs.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Ensemble Forecasting SWIFT model runs\nJean-Michel Perraud 2020-01-28\n\n\nEnsemble SWIFT model runs\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:52:34.\n\n\nElaboration\nlibrary(swift)\nhas_data <- swift::hasSampleData()\nif (!has_data) {Sys.setenv(SWIFT_SAMPLE_DATA_DIR='c:/data/stsf/documentation')}\nLet’s create a test catchment with a few subareas\nrunoffModel='GR4J'\n\nnodeIds=paste0('n', 1:6)\nlinkIds = paste0('lnk', 1:5)\ndefn <- list(\n    nodeIds=nodeIds,\n    nodeNames = paste0(nodeIds, '_name'),\n    linkIds=linkIds,\n    linkNames = paste0(linkIds, '_name'),\n    fromNode = paste0('n', c(2,5,4,3,1)),\n    toNode = paste0('n', c(6,2,2,4,4)),\n    areasKm2 = c(1.2, 2.3, 4.4, 2.2, 1.5),\n    runoffModel = runoffModel\n)\nsimulation <- createCatchment(defn$nodeIds, defn$nodeNames, defn$linkIds, defn$linkNames, defn$fromNode, defn$toNode, defn$runoffModel, defn$areasKm2)\nthe package uchronia includes facilities to access time series from a “library”, akin to what you would do to manage books.\ndataLibrary <- uchronia::sampleTimeSeriesLibrary('upper murray')\ndataIds <- uchronia::GetEnsembleDatasetDataIdentifiers_R(dataLibrary)\nprint(uchronia::GetEnsembleDatasetDataSummaries_R(dataLibrary))\n## [1] \"variable name: pet_der, identifier: 1, start: 1989-12-31T00:00:00, end: 2012-12-30T00:00:00, time length: 8401, time step: daily\"                            \n## [2] \"variable name: pet_der, identifier: 1, start: 1988-12-31T00:00:00, end: 2012-12-30T00:00:00, time length: 8766, time step: daily\"                            \n## [3] \"variable name: rain_der, identifier: 1, start: 1989-12-31T13:00:00, end: 2012-10-31T12:00:00, time length: 200160, time step: hourly\"                        \n## [4] \"variable name: rain_fcast_ens, identifier: 1, index: 0, start: 2010-08-01T21:00:00, end: 2010-08-06T21:00:00, time length: 5, time step: <not yet supported>\"\nThe sample catchment structure is obviously not the real “Upper Murray”. For the sake of a didactic example, let’s set the same inputs across all the subareas.\nprecipIds <- paste( 'subarea', getSubareaIds(simulation), 'P', sep='.')\nevapIds <- paste( 'subarea', getSubareaIds(simulation), 'E', sep='.')\nplayInputs(simulation, dataLibrary, precipIds, rep('rain_obs', length(precipIds)))\nplayInputs(simulation, dataLibrary, evapIds, rep('pet_obs', length(evapIds)), 'daily_to_hourly')\n## Warning in playInputs(simulation, dataLibrary, evapIds, rep(\"pet_obs\",\n## length(evapIds)), : Reusing argument `resample` to match the length of\n## `modelVarId`\n# And the flow rate we will record\noutflowId <- 'Catchment.StreamflowRate'\nGiven the information from the input data, let’s define a suitable simulation time span. NOTE and TODO: hourly information may not have been shown above yet.\ns <- uchronia::mkDate(2007, 1, 1)\ne <- uchronia::mkDate(2010, 8, 1, 20)\nsHot <- uchronia::mkDate(2010, 8, 1, 21)\neHot <- uchronia::mkDate(2010, 8, 5, 21)\nFirst, before demonstrating ensemble forecasting simulations, let’s demonstrate how we can get a snapshot of the model states at a point in time and restore it later on, hot-starting further simulation.\nsetSimulationSpan(simulation, start=s, end=eHot)\nrecordState(simulation, outflowId)\nexecSimulation(simulation)\nbaseline <- getRecorded(simulation, outflowId)\nintv <- joki::makeTextTimeInterval(sHot,eHot)\nbaseline <- baseline[intv]\n\nsetSimulationSpan(simulation, start=s, end=e)\nexecSimulation(simulation)\nsnapshot <- snapshotState(simulation)\nWe can execute a simulation over the new time span, but requesting model states to NOT be reset. If we compare with a simulation where, as per default, the states are reset before the first time step, we notice a difference:\nsetSimulationSpan(simulation, start=sHot, end=eHot)\nexecSimulation(simulation, resetInitialStates = FALSE)\nnoReset <- getRecorded(simulation, outflowId)\nexecSimulation(simulation, resetInitialStates = TRUE)\nwithReset <- getRecorded(simulation, outflowId)\nx <- merge(noReset,withReset)\nzoo::plot.zoo(x, plot.type='single', col=c('blue','red'), ylab=\"Outflow m3/s\", main=\"Outflows with/without state resets\")\n\nNow let’d ready the simulation to do ensemble forecasts. We define a list inputMap such that keys are the names of ensemble forecast time series found in dataLibrary and the values is one or more of the model properties found in the simulation. In this instance we use the same series for all model precipitation inputs in precipIds\ninputMap <- list(rain_fcast_ens=precipIds)\nresetModelStates(simulation)\nsetStates(simulation, snapshot)\nems <- createEnsembleForecastSimulation(simulation, dataLibrary, start=sHot, end=eHot, inputMap=inputMap, leadTime=as.integer(24*2 + 23), ensembleSize=100, nTimeStepsBetweenForecasts=24)\nGetSimulationSpan_Pkg_R(ems)\n## $Start\n## [1] \"2010-08-01 21:00:00 UTC\"\n## \n## $End\n## [1] \"2010-08-04 21:00:00 UTC\"\n## \n## $TimeStep\n## [1] \"hourly\"\nrecordState(ems, outflowId)\nexecSimulation(ems)\nforecasts <- getRecordedEnsembleForecast(ems, outflowId)\nstrSwiftRef(forecasts)\n## ensemble forecast time series:\n##  2010-08-01 21:00:00 UTC\n##  time step 86400S\n##  size 4\nflowFc <- uchronia::getItem(forecasts, 1)\nuchronia::plotXtsQuantiles(flowFc)"
  },
  {
    "objectID": "doc/samples/R/vignettes/calibration_initial_states/calibration_initial_states.html",
    "href": "doc/samples/R/vignettes/calibration_initial_states/calibration_initial_states.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Calibration with initial model memory states as parameters\nJean-Michel Perraud 2020-01-28\n\n\nCalibration with initial model memory states as parameters\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:52:32. It is a vignette to demonstrate features in SWIFT to calibrate a model with initial model states as a parameter.\n\n\nEssentials of setting up a calibration of initial states\nThis vignette will illustrate how to define two meta-parameters, S0 and R0, controlling the initial level of stores in the GR4J model, as fraction of the store capacities.\nWe’ll load a simple catchment with one subarea only; the feature applies equally to catchment with multiple sub-areas\nlibrary(swift)\n\nmodelId <- 'GR4J'\nms <- createSubareaSimulation(dataId='MMH', simulStart='1990-01-01', simulEnd='2005-12-31', \n    modelId=modelId, tstep='daily', varNameRain='P', varNamePet='E')\nWe define a few model state identifiers, and set them to be recorded to time series.\ngr4jModelVars <- runoffModelVarIds(modelId)\nprint(gr4jModelVars)\n##  [1] \"P\"          \"E\"          \"runoff\"     \"S\"          \"R\"         \n##  [6] \"Ps\"         \"Es\"         \"Pr\"         \"ech1\"       \"ech2\"      \n## [11] \"Perc\"       \"x1\"         \"x2\"         \"x3\"         \"x4\"        \n## [16] \"UHExponent\" \"PercFactor\"\nelementId <- 'subarea.Subarea'\nmkVarId <- function (shortName) { paste0(elementId, '.', shortName) }\nrunoffId <- mkVarId('runoff')\nsVarId <- mkVarId('S')\nrVarId <- mkVarId('R')\nrecordState(ms, c(runoffId, sVarId, rVarId))\nWe’ll set up a short runtime span, so that we illustrate the state initialisation feature.\nobsRunoff <- sampleSeries('MMH', 'flow') #actually, this is a time series of runoff depth, not streamflow rate\nobsRunoff[which(obsRunoff < -1)] <- NA\ns <- start(obsRunoff)\nw <- s\ne <- s + lubridate::days(90)\nsetSimulationSpan(ms, s, e)\nLet’s apply some default model parameters to the model:\n(pSpecGr4j <- joki::getFreeParams(modelId))\n##   Name      Value Min  Max\n## 1   x1 650.488000   1 3000\n## 2   x2  -0.280648 -27   27\n## 3   x3   7.891230   1  660\n## 4   x4  18.917200   1  240\npSpecGr4j$Name <- mkVarId(pSpecGr4j$Name)\n# TODO : a print function for native parameterizers.\np <- createParameterizer(type='Generic', pSpecGr4j)\nparameterizerAsDataFrame(p)\n##                 Name Min  Max      Value\n## 1 subarea.Subarea.x1   1 3000 650.488000\n## 2 subarea.Subarea.x2 -27   27  -0.280648\n## 3 subarea.Subarea.x3   1  660   7.891230\n## 4 subarea.Subarea.x4   1  240  18.917200\napplySysConfig(p, ms)\nWe get a time series of S if we run it at this point; the starting value is zero.\nexecSimulation(ms)\nplot(getRecorded(ms, sVarId), main='GR4J S store. No state initializer')\n\nLet’s define S0 and R0 parameters such that for each GR4J model instance, S = S0 * x1 and R = R0 * x3\npStates <- linearParameterizer(\n                      c(\"S0\",\"R0\"), \n                      c(\"S\",\"R\"), \n                      c(\"x1\",\"x3\"),\n                      c(0.0,0.0), \n                      c(1.0,1.0), \n                      c(0.9,0.9), \n                      'each subarea')\nIf one applies this parameterizer pState to the system, the the S store is set to the expected value relative to x1.\napplySysConfig(pStates, ms)\ngetStateValue(ms, sVarId)\n## subarea.Subarea.S \n##          585.4392\nHowever this is not enough to define a parameterizer as an initial state. If executing the simulation, the time series of S still starts at zero, because the resetting the model overrides the state S:\nexecSimulation(ms)\nplot(getRecorded(ms, sVarId), main='GR4J S store; incomplete store initialization')\n\nYou need to define a new parameterizer, that makes sure that the model is reset to the expected initial value.\ninitParameterizer <- makeStateInitParameterizer(pStates)\napplySysConfig(initParameterizer, ms)\nexecSimulation(ms)\nplot(getRecorded(ms, sVarId), main='GR4J S store, with a proper state initializer')\n\nThere is logic in keeping the two previous steps in defining a parameterizer as separate, hence this present vignette emphasizes the importance of these two steps.\nOnce you have defined this parameterizer using {r eval=FALSE} makeStateInitParameterizer, you can define a calibration objective the usual way. This vignette does not include calibration steps; please refer to other vignettes.\np <- concatenateParameterizers(p, initParameterizer)\nparameterizerAsDataFrame(p)\n##                 Name Min  Max      Value\n## 1 subarea.Subarea.x1   1 3000 650.488000\n## 2 subarea.Subarea.x2 -27   27  -0.280648\n## 3 subarea.Subarea.x3   1  660   7.891230\n## 4 subarea.Subarea.x4   1  240  18.917200\n## 5                 R0   0    1   0.900000\n## 6                 S0   0    1   0.900000\nobjective <- createObjective(ms, runoffId, obsRunoff, 'NSE', w, e)\nscore <- getScore(objective, p)\nprint(score)\n## $scores\n##       NSE \n## -5.894663 \n## \n## $sysconfig\n##                 Name Min  Max      Value\n## 1 subarea.Subarea.x1   1 3000 650.488000\n## 2 subarea.Subarea.x2 -27   27  -0.280648\n## 3 subarea.Subarea.x3   1  660   7.891230\n## 4 subarea.Subarea.x4   1  240  18.917200\n## 5                 R0   0    1   0.900000\n## 6                 S0   0    1   0.900000"
  },
  {
    "objectID": "doc/samples/R/vignettes/calibrate_multisite/calibrate_multisite.html",
    "href": "doc/samples/R/vignettes/calibrate_multisite/calibrate_multisite.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Calibration of a catchment using multisite multiobjective composition\nJean-Michel Perraud 2020-01-28\n\n\nCalibration of a catchment using multisite multiobjective composition\n\n\nUse case\nThis vignette demonstrates how one can calibrate a catchment using multiple gauging points available within this catchment.\nThis is a joint calibration weighing multiple objectives, possibly sourced at different modelling elements, thus a whole-of-catchment calibration technique. Staged, cascading calibration is supported and described in another vignette.\n\n\nData\nThe sample data that comes with the package contains a model definition for the South Esk catchment, including a short subset of the climate and flow record data.\n# swiftr_dev()   \n\nlibrary(swift)\n\nmodelId <- 'GR4J'\nsiteId <- 'South_Esk'\nsimulation <- sampleCatchmentModel(siteId=siteId, configId='catchment')\n# simulation <- swapModel(simulation, 'MuskingumNonLinear', 'channel_routing')\nsimulation <- swapModel(simulation, 'LagAndRoute', 'channel_routing')\nseClimate <- sampleSeries(siteId=siteId, varName='climate')\nseFlows <- sampleSeries(siteId=siteId, varName='flow')\nThe names of the climate series is already set to the climate input identifiers of the model simulation, so setting them as inputs is easy:\nplayInput(simulation, seClimate)\nsetSimulationSpan(simulation, start(seClimate), end(seClimate))\nsetSimulationTimeStep(simulation, 'hourly')\nMoving on to define the parameters, free or fixed. We will use (for now - may change) the package calibragem, companion to SWIFT.\nconfigureHourlyGr4j(simulation)\nWe define a function creating a realistic feasible parameter space. This is not the main object of this vignette, so we do not describe in details.\ncreateMetaParameterizer <- function(simulation, refArea=250, timeSpan=3600L) {\n  timeSpan <- as.integer(timeSpan)\n  parameterizer <- defineGr4jScaledParameter(refArea, timeSpan)\n  \n  # Let's define _S0_ and _R0_ parameters such that for each GR4J model instance, _S = S0 * x1_ and _R = R0 * x3_\n  pStates <- linearParameterizer(\n                      paramName=c(\"S0\",\"R0\"), \n                      stateName=c(\"S\",\"R\"), \n                      scalingVarName=c(\"x1\",\"x3\"),\n                      minPval=c(0.0,0.0), \n                      maxPval=c(1.0,1.0), \n                      value=c(0.9,0.9), \n                      selectorType='each subarea')\n  \n  initParameterizer <- makeStateInitParameterizer(pStates)\n  parameterizer <- concatenateParameterizers(parameterizer, initParameterizer)\n  \n  lagAndRouteParameterizer <- function() {\n    p <- data.frame(Name = c('alpha', 'inverse_velocity'),\n        Value = c(1, 1),\n        Min = c(1e-3, 1e-3),\n        Max = c(1e2, 1e2),\n        stringsAsFactors = FALSE)\n    p <- createParameterizer('Generic links', p)\n    return(p)\n  }\n\n  # Lag and route has several discrete storage type modes. One way to set up the modeP\n  setupStorageType <- function(simulation) {\n    p <- data.frame(Name = c('storage_type'), \n        Value = 1, \n        Min = 1,\n        Max = 1,\n        stringsAsFactors = FALSE)\n    p <- createParameterizer('Generic links', p)\n    applySysConfig(p, simulation)\n  }\n  setupStorageType(simulation)\n\n  # transfer reach lengths to the Lag and route model\n  linkIds <- getLinkIds(simulation)\n  reachLengths <- getStateValue(simulation, paste0('link.', linkIds, '.Length'))\n  setStateValue(simulation, paste0('link.', linkIds, '.reach_length'), reachLengths) \n\n  lnrp <- lagAndRouteParameterizer()\n  parameterizer <- concatenateParameterizers(parameterizer, lnrp)  \n  return(parameterizer)\n}\nparameterizer <- createMetaParameterizer(simulation)\nsetParameterValue(parameterizer, 'asinh_x2', 0)\napplySysConfig(parameterizer, simulation)\nexecSimulation(simulation)\nWe are now ready to enter the main topic of this vignette, setting up a weighted multi-objective for calibration purposes.\nThe sample gauge data flow contains identifiers that are of course distinct from the network node identifiers. We create a map between them (note - this information used to be in the NodeLink file in swiftv1).\ngauges <- as.character(c( 92106, 592002, 18311, 93044,    25,   181))\nnodeIds <- paste0('node.', as.character( c(7,   12,   25,   30,   40,   43) ))   \nnames(gauges) <- nodeIds\nFirst, let us try the Nash Sutcliffe efficiency, for simplicity (no additional parameters needed). We will set up NSE calculations at two points (nodes) in the catchments. Any model state from a link, node or subarea could be a point for statistics calculation.\ns <- GetSimulationSpan_Pkg_R(simulation)\n\ncalibrationPoints <- nodeIds[1:2]\nmvids <- mkFullDataId(calibrationPoints, 'OutflowRate')\n\nw <- c(1.0, 2.0) # weights (internally normalised to a total of 1.0)\nnames(w) <- mvids\n\nstatspec <- multiStatisticDefinition( \n  modelVarIds = mvids, \n  statisticIds = rep('nse', 2), \n  objectiveIds=calibrationPoints, \n  objectiveNames = paste0(\"NSE-\", calibrationPoints), \n  starts = rep(s$Start, 2), \n  ends = rep(s$End, 2) )\n\nobservations <- list(\n  seFlows[,gauges[1]],\n  seFlows[,gauges[2]]\n)\n\nmoe <- createMultisiteObjective(simulation, statspec, observations, w)\ngetScore(moe, parameterizer)\n## $scores\n## MultisiteObjectives \n##          0.03281224 \n## \n## $sysconfig\n##               Name       Min        Max     Value\n## 1           log_x4  0.000000   2.380211 0.3054223\n## 2           log_x1  0.000000   3.778151 0.5066903\n## 3           log_x3  0.000000   3.000000 0.3154245\n## 4         asinh_x2 -3.989327   3.989327 0.0000000\n## 5               R0  0.000000   1.000000 0.9000000\n## 6               S0  0.000000   1.000000 0.9000000\n## 7            alpha  0.001000 100.000000 1.0000000\n## 8 inverse_velocity  0.001000 100.000000 1.0000000\nWe now build a parameterizer that is suited to multisite objective functions. Note that for now there is no effect changing the result - just scaffloding and structural test.\np <- createParameterizer('no apply')\nfuncParam <- list(p, clone(p)) # for the two calib points, NSE has no param here\ncatchmentPzer <- parameterizer\nfp <- createMultisiteObjParameterizer(funcParam, calibrationPoints, prefixes=paste0(calibrationPoints, '.'), mixFuncParam=NULL, hydroParam=catchmentPzer)\n(parameterizerAsDataFrame(fp))\n##               Name       Min        Max     Value\n## 1           log_x4  0.000000   2.380211 0.3054223\n## 2           log_x1  0.000000   3.778151 0.5066903\n## 3           log_x3  0.000000   3.000000 0.3154245\n## 4         asinh_x2 -3.989327   3.989327 0.0000000\n## 5               R0  0.000000   1.000000 0.9000000\n## 6               S0  0.000000   1.000000 0.9000000\n## 7            alpha  0.001000 100.000000 1.0000000\n## 8 inverse_velocity  0.001000 100.000000 1.0000000\nEvaluateScoreForParameters(moe, fp)\n## $scores\n## MultisiteObjectives \n##          0.03281224 \n## \n## $sysconfig\n##               Name       Min        Max     Value\n## 1           log_x4  0.000000   2.380211 0.3054223\n## 2           log_x1  0.000000   3.778151 0.5066903\n## 3           log_x3  0.000000   3.000000 0.3154245\n## 4         asinh_x2 -3.989327   3.989327 0.0000000\n## 5               R0  0.000000   1.000000 0.9000000\n## 6               S0  0.000000   1.000000 0.9000000\n## 7            alpha  0.001000 100.000000 1.0000000\n## 8 inverse_velocity  0.001000 100.000000 1.0000000\nWe can get the value of each objective. The two NSE scores below are negative. Note above that the composite objective is positive, i.e. the opposite of the weighted average. This is because the composite objective is always minimisable (as of writing anyway this is a design choice.)\nEvaluateScoresForParametersWila_Pkg_R(moe, fp)\n##     node.12      node.7 \n## -0.03819707 -0.02204260\n\nlog-likelihood multiple objective\nNow, let’s move on to use log-likelihood instead of NSE.\nstatspec <- multiStatisticDefinition(  \n  modelVarIds = mvids, \n  statisticIds = rep('log-likelihood', 2), \n  objectiveIds=calibrationPoints, \n  objectiveNames = paste0(\"LL-\", calibrationPoints), \n  starts = rep(s$Start, 2), \n  ends = rep(s$End, 2) )\n\nobj <- createMultisiteObjective(simulation, statspec, observations, w)\nFor this to work we need to include parameters\nmaxobs <- max(observations[[1]], na.rm=TRUE)\ncensorThreshold <- 0.01\ncensopt <- 0.0\ncalcMAndS <- 1.0 # i.e. TRUE\n\n#       const string LogLikelihoodKeys::KeyAugmentSimulation = \"augment_simulation\";\n#       const string LogLikelihoodKeys::KeyExcludeAtTMinusOne = \"exclude_t_min_one\";\n#       const string LogLikelihoodKeys::KeyCalculateModelledMAndS = \"calc_mod_m_s\";\n#       const string LogLikelihoodKeys::KeyMParMod = \"m_par_mod\";\n#       const string LogLikelihoodKeys::KeySParMod = \"s_par_mod\";\n\np <- createParameterizer('no apply')\n# Note: exampleParameterizer is also available\naddToHyperCube(p, \n          data.frame( Name=c('b','m','s','a','maxobs','ct', 'censopt', 'calc_mod_m_s'),\n          Min   = c(-30, 0, 1,    -30, maxobs, censorThreshold, censopt, calcMAndS),\n          Max   = c(0,   0, 1000, 1, maxobs, censorThreshold, censopt, calcMAndS),\n          Value = c(-7,  0, 100,  -10, maxobs, censorThreshold, censopt, calcMAndS),\n          stringsAsFactors=FALSE) )\n\nfuncParam <- list(p, clone(p)) # for the two calib points, NSE has no param here\ncatchmentPzer <- parameterizer\nfp <- createMultisiteObjParameterizer(funcParam, calibrationPoints, prefixes=paste0(calibrationPoints, '.'), mixFuncParam=NULL, hydroParam=catchmentPzer)\n(parameterizerAsDataFrame(fp))\n##                    Name        Min         Max       Value\n## 1                log_x4   0.000000    2.380211   0.3054223\n## 2                log_x1   0.000000    3.778151   0.5066903\n## 3                log_x3   0.000000    3.000000   0.3154245\n## 4              asinh_x2  -3.989327    3.989327   0.0000000\n## 5                    R0   0.000000    1.000000   0.9000000\n## 6                    S0   0.000000    1.000000   0.9000000\n## 7                 alpha   0.001000  100.000000   1.0000000\n## 8      inverse_velocity   0.001000  100.000000   1.0000000\n## 9              node.7.b -30.000000    0.000000  -7.0000000\n## 10             node.7.m   0.000000    0.000000   0.0000000\n## 11             node.7.s   1.000000 1000.000000 100.0000000\n## 12             node.7.a -30.000000    1.000000 -10.0000000\n## 13        node.7.maxobs  22.000000   22.000000  22.0000000\n## 14            node.7.ct   0.010000    0.010000   0.0100000\n## 15       node.7.censopt   0.000000    0.000000   0.0000000\n## 16  node.7.calc_mod_m_s   1.000000    1.000000   1.0000000\n## 17            node.12.b -30.000000    0.000000  -7.0000000\n## 18            node.12.m   0.000000    0.000000   0.0000000\n## 19            node.12.s   1.000000 1000.000000 100.0000000\n## 20            node.12.a -30.000000    1.000000 -10.0000000\n## 21       node.12.maxobs  22.000000   22.000000  22.0000000\n## 22           node.12.ct   0.010000    0.010000   0.0100000\n## 23      node.12.censopt   0.000000    0.000000   0.0000000\n## 24 node.12.calc_mod_m_s   1.000000    1.000000   1.0000000\nmoe <- obj\ngetScore(moe, fp)\n## $scores\n## MultisiteObjectives \n##             44779.5 \n## \n## $sysconfig\n##                    Name        Min         Max       Value\n## 1                log_x4   0.000000    2.380211   0.3054223\n## 2                log_x1   0.000000    3.778151   0.5066903\n## 3                log_x3   0.000000    3.000000   0.3154245\n## 4              asinh_x2  -3.989327    3.989327   0.0000000\n## 5                    R0   0.000000    1.000000   0.9000000\n## 6                    S0   0.000000    1.000000   0.9000000\n## 7                 alpha   0.001000  100.000000   1.0000000\n## 8      inverse_velocity   0.001000  100.000000   1.0000000\n## 9              node.7.b -30.000000    0.000000  -7.0000000\n## 10             node.7.m   0.000000    0.000000   0.0000000\n## 11             node.7.s   1.000000 1000.000000 100.0000000\n## 12             node.7.a -30.000000    1.000000 -10.0000000\n## 13        node.7.maxobs  22.000000   22.000000  22.0000000\n## 14            node.7.ct   0.010000    0.010000   0.0100000\n## 15       node.7.censopt   0.000000    0.000000   0.0000000\n## 16  node.7.calc_mod_m_s   1.000000    1.000000   1.0000000\n## 17            node.12.b -30.000000    0.000000  -7.0000000\n## 18            node.12.m   0.000000    0.000000   0.0000000\n## 19            node.12.s   1.000000 1000.000000 100.0000000\n## 20            node.12.a -30.000000    1.000000 -10.0000000\n## 21       node.12.maxobs  22.000000   22.000000  22.0000000\n## 22           node.12.ct   0.010000    0.010000   0.0100000\n## 23      node.12.censopt   0.000000    0.000000   0.0000000\n## 24 node.12.calc_mod_m_s   1.000000    1.000000   1.0000000\nEvaluateScoresForParametersWila_Pkg_R(moe, fp)\n##   node.12    node.7 \n## -44617.76 -45102.99"
  },
  {
    "objectID": "doc/samples/R/vignettes/getting_started/getting_started.html",
    "href": "doc/samples/R/vignettes/getting_started/getting_started.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Getting started with the swift R package\nJean-Michel Perraud 2020-01-28\n\n\nGetting started with the SWIFT R package\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:53:51. It is the introduction ‘vignette’ to an R package for interacting with SWIFT.\nIt shows one of the most basic usage, running a single model simulation. While basic, it is realistic and uses data from a study catchment.\n\n\nGetting started\nLoad the R package the usual way…\nlibrary(swift)\nIt includes a fair level of documentation, that should be accessible using the ‘?’ command, shortcut for help(swift).\n?swift\n?getRecorded\nThe package contains some sample data for a few Australian catchments. Note that these sample data are for documentation only and not to be used for real world applications.\ndata('swift_sample_data')\nnames(swiftSampleData)\n## [1] \"MMH\"       \"Ovens\"     \"Abbeyard\"  \"07378500\"  \"South_Esk\" \"Adelaide\"\nswift now has some functions to create a single subarea simulation for testing purposes. While is it perfectly possible to manually build your own model simulation from scratch, for the sake of getting started quickly let’s use pre-defined functions to get a model simulation ready to run. The parameters of the function should be fairly self-explanatory to you:\nms <- createSubareaSimulation(dataId='MMH', simulStart='1990-01-01', simulEnd='2005-12-31', \n    modelId='GR4J', tstep='daily', varNameRain='P', varNamePet='E')\nstr(ms)\n## Formal class 'ExternalObjRef' [package \"cinterop\"] with 2 slots\n##   ..@ obj :<externalptr> \n##   ..@ type: chr \"MODEL_SIMULATION_PTR\"\nThe R object ‘ms’ may appear unusual to many R users. This is basically a handle to the SWIFT simulation written in C++. It is passed as an argument to R functions, and you do not need to know more details than this.\nThe simulation we just created is ready to execute, which means it already has some input data defined (site ‘MMH’). We can inspect it for instance with:\nhead(getPlayed(ms))\n## Warning: timezone of object (UTC) is different than current timezone ().\n\n##            subcatchment.Subarea.E subcatchment.Subarea.P\n## 1990-01-01               5.542243               0.000000\n## 1990-01-02               5.552225               0.000000\n## 1990-01-03               5.562208               0.000000\n## 1990-01-04               5.572191               0.000000\n## 1990-01-05               5.582174               0.000000\n## 1990-01-06               5.592156               0.526757\nzoo::plot.zoo(getPlayed(ms), main='MMH daily climate inputs')\n\nThe simulation has no output to record to time series defined yet. SWIFT is designed to record model variables on demand in a highly flexible manner. First, we can query the system to find out known models, and the model variable names that we can record.\ngetRecordedVarnames(ms)\n## character(0)\nrunoffModelIds()\n##  [1] \"NetRainfall\"   \"GR4J\"          \"GR4J_SG\"       \"GR5H\"         \n##  [5] \"GR6J\"          \"GR5J\"          \"PDM\"           \"AWBM\"         \n##  [9] \"SACSMA\"        \"const_outflow\" \"external\"      \"GRKAL\"\ngr4jModelVars <- runoffModelVarIds('GR4J')\nprint(gr4jModelVars)\n##  [1] \"P\"          \"E\"          \"runoff\"     \"S\"          \"R\"         \n##  [6] \"Ps\"         \"Es\"         \"Pr\"         \"ech1\"       \"ech2\"      \n## [11] \"Perc\"       \"x1\"         \"x2\"         \"x3\"         \"x4\"        \n## [16] \"UHExponent\" \"PercFactor\"\nThese are the variable names for a single GR4J model instance; since SWIFT is for semi-distributed models, we need to use a hierarchical naming scheme to uniquely identify model variables (even when in this case we do have only one subarea). Using unique keys allow to inspect the model states in great details if needed.\ngetSubareaIds(ms)\n## [1] \"Subarea\"\ngetStateValue(ms, 'subarea.Subarea.x4')\n## subarea.Subarea.x4 \n##                0.5\ntoRecord <- c('runoff', 'S', 'R', 'Ps', 'Es', 'Pr', 'ech1', 'ech2', 'Perc')\nstopifnot(all( toRecord %in% gr4jModelVars)) # just a check in case something changes in the future\nLet’s record to time series all the storage and flux states of GR4J (no need to record model parameters which will be flat lines here)\nrecordState(ms, paste0('subarea.Subarea.', toRecord))\ngetRecordedVarnames(ms)\n## [1] \"subarea.Subarea.Es\"     \"subarea.Subarea.Perc\"  \n## [3] \"subarea.Subarea.Pr\"     \"subarea.Subarea.Ps\"    \n## [5] \"subarea.Subarea.R\"      \"subarea.Subarea.S\"     \n## [7] \"subarea.Subarea.ech1\"   \"subarea.Subarea.ech2\"  \n## [9] \"subarea.Subarea.runoff\"\nAnd let’s run\nexecSimulation(ms)\nWe do a bit of text preprocessing to shorten the default names of the time series (‘xts’ objects in R), but otherwise default time series plots are straightforward.\nvarSeries = getRecorded(ms)\nstr(varSeries)\n## An 'xts' object on 1990-01-01/2005-12-31 containing:\n##   Data: num [1:5844, 1:9] 0 0 0 0 0 ...\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : NULL\n##   ..$ : chr [1:9] \"subarea.Subarea.Es\" \"subarea.Subarea.Perc\" \"subarea.Subarea.Pr\" \"subarea.Subarea.Ps\" ...\n##   Indexed by objects of class: [POSIXct,POSIXt] TZ: UTC\n##   xts Attributes:  \n##  NULL\nnames(varSeries) <- gsub(names(varSeries), pattern=\"subarea\\\\.Subarea\\\\.\", replacement='')\nzoo::plot.zoo(varSeries, main = 'Default GR4J output on MMH data')\n\nLet’s look at a shorter period of the output; we can demonstrate the use of the lubridate package for convenient date-time arithmetic, and the window function. We define a couple of functions to slice and plot the last three years of the time series.\nlibrary(lubridate)\n\nlastThreeYears <- function(tts) {\n    window(tts, start=end(tts)-lubridate::years(3), end=end(tts)) \n}\nobsVsCalc <- function(obs, calc, ylab=\"runoff (mm)\") {\n    obs <- lastThreeYears(obs)\n    joki::plotTwoSeries(obs, calc, ylab=ylab, startTime = start(obs), endTime = end(obs))\n}\nzoo::plot.zoo(lastThreeYears(varSeries), main = 'Default GR4J output on MMH data')\n\n\n\nExploring the model interactively\nAs mentioned earlier, it is possibly to define the model simulation definition directly and interactively. The following shows how a to assign another input time series, with a somewhat contrived example of a scaled up precipitation input series.\nprecipId <- 'subcatchment.Subarea.P'\nrunoffId <- 'subarea.Subarea.runoff'\nprecip <- getPlayed(ms, precipId)\nbaselineRunoff <- getRecorded(ms, runoffId)\nprecipScaled <- precip * 1.1\nplayInput(ms, precipScaled, precipId)\nexecSimulation(ms)\nrunoffDiff <- getRecorded(ms, runoffId) - baselineRunoff\nzoo::plot.zoo(lastThreeYears(runoffDiff), main = 'Additional runoff with scaled up precipitation', ylab='runoff depth change (mm)')\n\nLet’s get back to the initial input settings, and demonstrate one way to change parameters interactively. Note that this is not necessarily the recommended way to handle model parameterisation, as will be made clear later in this document when setting up calibration. You can already see how the code required for just one parameter is more length.\nplayInput(ms, precip, precipId)\nx4Id <- 'subarea.Subarea.x4'\n(x4 <- getStateValue(ms, x4Id))\n## subarea.Subarea.x4 \n##                0.5\nsetStateValue(ms, x4Id, x4*1.1)\nexecSimulation(ms)\nrunoffDiff <- getRecorded(ms, runoffId) - baselineRunoff\nzoo::plot.zoo(lastThreeYears(runoffDiff), main = 'Change in runoff with x4 scaled up by 10%', ylab='runoff depth change')\n\nsetStateValue(ms, x4Id, x4)\n\n\nCalibration\nLet’s st up a calibration against the observed runoff depth for ‘MMH’, included as sample data in the package, and view it along the current default model runoff output.\nobsRunoff <- sampleSeries('MMH', 'flow') #actually, runoff depth\nobsRunoff[which(obsRunoff < -1)] <- NA\nobsVsCalc(obsRunoff, baselineRunoff)\n\nFirst let’s define the objective of the calibration, the Nash-Sutcliffe efficiency (NSE) for the runoff depth. We’ll use two years of data as warmup.\ns <- start(obsRunoff)\nw <- s + lubridate::years(2)\ne <- end(obsRunoff)\nsetSimulationSpan(ms, s, e)\nobjective <- createObjective(ms, runoffId, obsRunoff, 'NSE', w, e)\nWhile we can tweak model parameters directly from R one by one as shown in the previous paragraph, there are tools available in the core SWIFT library for catchment parameterization. These are more scalable to catchments with a large number of sub-areas. In this section we’ll use a generic parameterizer. Other forms available are parameterizers applying scaled “meta-parameters” to subareas, based for instance on the surface of each subarea.\nStarting from an arbitrary template data frame for GR4J parameters:\n(pSpecGr4j <- joki::getFreeParams('GR4J'))\n##   Name      Value Min  Max\n## 1   x1 650.488000   1 3000\n## 2   x2  -0.280648 -27   27\n## 3   x3   7.891230   1  660\n## 4   x4  18.917200   1  240\npSpecGr4j$Value <- c(542.1981111,  -0.4127542,   7.7403390 ,  1.2388548)\npSpecGr4j$Min <- c(1,-30, 1,1)\npSpecGr4j$Max <- c(1000, 30, 1000, 240)\nThe template is using the short names of the GR4J parameters; our SWIFT parameterizer here is generic and needs the full hierarchical name of the model variable identifiers\npSpecGr4j$Name <- paste0('subarea.Subarea.', pSpecGr4j$Name)\np <- createParameterizer(type='Generic', pSpecGr4j)\nparameterizerAsDataFrame(p)\n##                 Name Min  Max       Value\n## 1 subarea.Subarea.x1   1 1000 542.1981111\n## 2 subarea.Subarea.x2 -30   30  -0.4127542\n## 3 subarea.Subarea.x3   1 1000   7.7403390\n## 4 subarea.Subarea.x4   1  240   1.2388548\nLet’s see the default goodness of fit for these parameters. Unsurprisingly, fairly poor. The purpose of the following lines is to show how to get manually objective score values.\nscore <- getScore(objective, p)\nprint(score)\n## $scores\n##       NSE \n## -2.338177 \n## \n## $sysconfig\n##                 Name Min  Max       Value\n## 1 subarea.Subarea.x1   1 1000 542.1981111\n## 2 subarea.Subarea.x2 -30   30  -0.4127542\n## 3 subarea.Subarea.x3   1 1000   7.7403390\n## 4 subarea.Subarea.x4   1  240   1.2388548\nWe have our objectives defined, and the parameter space ‘p’ in which to search. Let’s create an optimizer and we are ready to go. While the optimizer can be created in one line, we show how to choose one custom termination criterion and how to configure the optimizer to capture a detailed log of the process.\nThere are several options for defining a calibration termination criterion\nterm <- getMarginalTermination(tolerance = 1e-06, cutoffNoImprovement = 100, maxHours = 0.05) \nterm <- getMaxRuntimeTermination(maxHours = 0.0015) \nFor this vignette we will use a criterion using the maximum standard deviation of parameters in a population (of 0.2%). Note that most termination criteria have a maximum wallclock runtime as a fallback to set an upper bound for reasonable runtime.\nterm <- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.002','0.0167'))\n\nsceParams <- getDefaultSceParameters()\nurs <- createParameterSampler(0, p, 'urs')\noptimizer <- createSceOptimSwift(objective, term, SCEpars=sceParams, urs)\n\ncalibLogger <- setCalibrationLogger(optimizer, '')\nstartTime <- lubridate::now()\ncalibResults <- executeOptimization(optimizer)\nendTime <- lubridate::now()\ncalibWallTime <- endTime-startTime\nprint(paste( 'Optimization completed in ', calibWallTime, attr(calibWallTime, 'units')))\n## [1] \"Optimization completed in  2.04213500022888 secs\"\nswift uses optimization tools that will parallelize model simulation runs if possible (i.e. if supported by the model).\nThere are high level functions in the packages swift and mhplot to import the optimisation log information into R data structures\noptLog <- extractOptimizationLog(optimizer, fitnessName = \"NSE\")\ngeomOps <- optLog$geomOps \nstr(geomOps@data)\n## 'data.frame':    1848 obs. of  9 variables:\n##  $ Category          : Factor w/ 7 levels \"Complex No 0\",..: 7 7 7 7 7 7 7 7 7 7 ...\n##  $ CurrentShuffle    : Factor w/ 29 levels \"\",\"0\",\"1\",\"10\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Message           : Factor w/ 5 levels \"Adding a random point in hypercube\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ NSE               : num  -1339.286 -0.891 -0.327 -2.367 -913.047 ...\n##  $ subarea.Subarea.x1: num  948 175 215 725 874 ...\n##  $ subarea.Subarea.x2: num  23.01 -7.28 -7.07 6.66 28.64 ...\n##  $ subarea.Subarea.x3: num  62.2 710.5 124.6 930.6 126.4 ...\n##  $ subarea.Subarea.x4: num  161.9 96.4 19 207.7 83.7 ...\n##  $ PointNumber       : int  1 2 3 4 5 6 7 8 9 10 ...\nWe can see that at least one of the parameters settled at one of its boundaries:\npVarIds <- parameterizerAsDataFrame(p)$Name\npVarIds\n## [1] \"subarea.Subarea.x1\" \"subarea.Subarea.x2\" \"subarea.Subarea.x3\"\n## [4] \"subarea.Subarea.x4\"\nprint(mhplot::plotParamEvolution(geomOps, pVarIds[1], objLims=c(0,1)))\n\nNote that the parameter x4 also seems to have settled at its lower bound. x4 influences the unit hydrograph, and the meaning of this parameter depends on the time step of the input series. It may be justified in this case to go below 1 for its lower bound.\nprint(mhplot::plotParamEvolution(geomOps, pVarIds[4], objLims=c(0,1)))\n\nSo let’s restart, with a larger upper bound for the x1 parameter:\npSpecGr4j$Max <- c(2500, 30, 1000, 240)\npSpecGr4j$Min <- c(1,-30, 1,0.2)\np <- createParameterizer(type='Generic', pSpecGr4j)\nurs <- createParameterSampler(0, p, 'urs')\noptimizer <- createSceOptimSwift(objective, term, SCEpars=sceParams, urs)\ncalibLogger <- setCalibrationLogger(optimizer, '')\ncalibResults <- executeOptimization(optimizer)\noptLog <- extractOptimizationLog(optimizer, fitnessName = \"NSE\")\ngeomOps <- optLog$geomOps \nLet’s check that the parameter does not settle at the boundary anymore:\nd <- mhplot::plotParamEvolution(geomOps, pVarIds[1], objLims=c(0,1))\nprint(d)\n\nNote that there are further options in mhplot and ggplot2 to assess the behavior of the optimization process. Explore these packages’ documentation to find these options.\nlibrary(ggplot2)\nd + facet_wrap( as.formula(paste(\"~\", geomOps@messages, sep=' ')) )\n\nLet’s retrieve the parameter set with the best NSE, and see the resulting runoff time series.\n(bestPset <- getBestScore(calibResults, 'NSE'))\n## An object of class \"ExternalObjRef\"\n## Slot \"obj\":\n## <pointer: 0x0000000016f07c10>\n## \n## Slot \"type\":\n## [1] \"OBJECTIVE_SCORES_WILA_PTR\"\nTo get a visual on the information of this external pointer, we can use:\nasRStructure(bestPset)\n## $scores\n##       NSE \n## 0.7832226 \n## \n## $sysconfig\n##                 Name   Min  Max       Value\n## 1 subarea.Subarea.x1   1.0 2500 1141.326083\n## 2 subarea.Subarea.x2 -30.0   30   -5.443462\n## 3 subarea.Subarea.x3   1.0 1000   97.374236\n## 4 subarea.Subarea.x4   0.2  240    0.430444\nNote, as an aside, that we see only the last 3 years of time series, while the NSE score is calculated over several more years. As it happens, the runoff prediction has a systematic negative bias.\napplySysConfig(bestPset, ms)\nexecSimulation(ms)\nobsVsCalc(obsRunoff, getRecorded(ms, runoffId))"
  },
  {
    "objectID": "doc/samples/R/vignettes/calibrate_subcatchments/calibrate_subcatchments.html",
    "href": "doc/samples/R/vignettes/calibrate_subcatchments/calibrate_subcatchments.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Jean-Michel Perraud 2020-01-28"
  },
  {
    "objectID": "doc/samples/R/vignettes/calibrate_subcatchments/calibrate_subcatchments.html#whole-of-catchment-calibration-combining-point-gauges",
    "href": "doc/samples/R/vignettes/calibrate_subcatchments/calibrate_subcatchments.html#whole-of-catchment-calibration-combining-point-gauges",
    "title": "Streamflow Forecasting Onboarding",
    "section": "Whole of catchment calibration combining point gauges",
    "text": "Whole of catchment calibration combining point gauges\ngauges <- as.character(c( 92106, 592002, 18311, 93044,    25,   181))\nnames(gauges) <- paste0('node.', as.character( c(7,   12,   25,   30,   40,   43) ))\ncalibNodes <- paste0('node.', as.character( c(7,   12) ))\n\nelementId <- names(subCats)[1]\n\ngaugeId <- gauges[calibNodes]\ngaugeFlow <- seFlows[,gaugeId]\n\nvarId <- paste0(calibNodes, '.OutflowRate')\nrecordState(simulation,varId)\nobjectiveId <- 'NSE'\nobjective <- createObjective(simulation, varId[1], observation=gaugeFlow[,1], objectiveId, start(seFlows), end(seFlows))\n\naddToCompositeObjective <- function(compositeObj, objective, weight, name) {\n  obj <- objective\n  if (cinterop::isExternalObjRef(objective, 'OBJECTIVE_EVALUATOR_WILA_PTR')) {\n    obj <- UnwrapObjectiveEvaluatorWila_R(objective)\n  }\n  AddSingleObservationObjectiveEvaluator_R(compositeObj, obj, weight, name)\n}\n\nco <- CreateEmptyCompositeObjectiveEvaluator_R()\naddToCompositeObjective(co, objective, 1.0, varId[1])\nobjective <- createObjective(simulation, varId[2], observation=gaugeFlow[,2], objectiveId, start(seFlows), end(seFlows))\naddToCompositeObjective(co, objective, 1.0, varId[2])\n\nco <- swift::WrapObjectiveEvaluatorWila_R(co, clone=TRUE)\n\nscore <- getScore(co,parameterizer) \n# scoresAsDataFrame(score)"
  },
  {
    "objectID": "doc/samples/R/vignettes/log_likelihood/log_likelihood.html",
    "href": "doc/samples/R/vignettes/log_likelihood/log_likelihood.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Sample code for log-likelihood calibration\nJean-Michel Perraud 2020-01-28\n\n\nSample code for log-likelihood calibration\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:54:01. It illustrates how to set up a calibration with a log-likelihood objective.\n\n\nSetting up a calibration on daily data\nWe will use some sample data from (MMH - Minamata?) included in the package\nlibrary(lubridate)\nlibrary(swift)\ndata(swift_sample_data)\nsSpan <- '1990-01-01/2005-12-31'\nrain <- sampleSeries('MMH', 'rain')[sSpan]\nevap <- sampleSeries('MMH', 'evap')[sSpan]\nflow <- sampleSeries('MMH', 'flow')[sSpan]\nWe need to adjust the observed flow, as the SWIFTv1 legacy missing value code is not consistent with default handling in SAK.\nflow[flow<0] <- NA\nLet’s create a single catchment setup, using daily data. We need so specify the simulation time step to be consistent with the daily input data.\nms <- createSubarea('GR4J', 1.0)\ns <- start(rain)\ne <- end(rain)\nsetSimulationSpan(ms, s, e)\nsetSimulationTimeStep(ms, 'daily')\nAssign input time series\nsubAreaName <- getSubareaNames(ms)[1]\nplaySubareaInput(ms, input=rain, subAreaName, \"P\")\nplaySubareaInput(ms, input=evap, subAreaName, \"E\")\nModel variables identifiers are hierarchical, with separators ‘.’ and ‘|’ supported. The “dot” notation should now be preferred, as some R functions producing data frames may change the variable names and replace some characters with ‘.’.\nsubareaId <- paste0(\"subarea.\", subAreaName)\nrootId <- paste0(subareaId, \".\")\nprint(getVariableIds(ms, subareaId))\n##  [1] \"areaKm2\"       \"P\"             \"E\"             \"runoff\"       \n##  [5] \"S\"             \"R\"             \"Ps\"            \"Es\"           \n##  [9] \"Pr\"            \"ech1\"          \"ech2\"          \"Perc\"         \n## [13] \"x1\"            \"x2\"            \"x3\"            \"x4\"           \n## [17] \"UHExponent\"    \"PercFactor\"    \"OutflowVolume\" \"OutflowRate\"\ngr4StateNames <- paste0(rootId, c('runoff', 'S', 'R', 'Perc'))\nfor (name in gr4StateNames) { recordState(ms, name) }\nLet’s check that one simulation runs fine, before we build a calibration definition.\nexecSimulation(ms)\nsState <- getRecorded(ms, gr4StateNames[2])\nnames(sState) <- shortVarId(names(sState))\nzoo::plot.zoo(sState)\n\nLet’s build the objective calculator that will guide the calibration process:\nw <- uchronia::mkDate(1992, 01, 01)\nrunoffDepthVarname <- gr4StateNames[1]\nmodRunoff <- getRecorded(ms, runoffDepthVarname)\nzoo::index(flow) <- zoo::index(modRunoff)\nobjective <- createObjective(ms, runoffDepthVarname, flow, 'log-likelihood', w, e)\nDefine the feasible parameter space, using a generic parameter set for the model parameters. This is ‘wrapped’ by a log-likelihood parameter set with the extra parameters used in the log likelihood calculation, but which exposes all the parameters as 8 independent degrees of freedom to the optimizer.\n(pSpecGr4j <- joki::getFreeParams('GR4J'))\n##   Name      Value Min  Max\n## 1   x1 650.488000   1 3000\n## 2   x2  -0.280648 -27   27\n## 3   x3   7.891230   1  660\n## 4   x4  18.917200   1  240\npSpecGr4j$Value <- c(542.1981111, -0.4127542, 7.7403390, 1.2388548)\npSpecGr4j$Min <- c(1,-30, 1,1)\npSpecGr4j$Max <- c(3000, 30, 1000, 240)\npSpecGr4j$Name <- paste0(rootId, pSpecGr4j$Name)\n\n\nmaxobs <- max(flow, na.rm=TRUE)\np <- createParameterizer(type='Generic', specs=pSpecGr4j)\nsetLogLikParamKeys(a='a', b='b', m='m', s='s', ct=\"ct\", censopt='censopt')\ncensorThreshold <- maxobs / 100 # TBC\ncensopt <- 0.0\n\nloglik <- createParameterizer(type='no apply')\naddToHyperCube(loglik, \n          data.frame( Name=c('b','m','s','a','maxobs','ct', 'censopt'),\n          Min   = c(-30, 0, 1,    -30, maxobs, censorThreshold, censopt),\n          Max   = c(0,   0, 1000, 1, maxobs, censorThreshold, censopt),\n          Value = c(-7,  0, 100,  -10, maxobs, censorThreshold, censopt),\n          stringsAsFactors=FALSE) )\np <- concatenateParameterizers(p, loglik)\nparameterizerAsDataFrame(p)\n##                  Name         Min          Max       Value\n## 1  subarea.Subarea.x1   1.0000000 3000.0000000 542.1981111\n## 2  subarea.Subarea.x2 -30.0000000   30.0000000  -0.4127542\n## 3  subarea.Subarea.x3   1.0000000 1000.0000000   7.7403390\n## 4  subarea.Subarea.x4   1.0000000  240.0000000   1.2388548\n## 5                   b -30.0000000    0.0000000  -7.0000000\n## 6                   m   0.0000000    0.0000000   0.0000000\n## 7                   s   1.0000000 1000.0000000 100.0000000\n## 8                   a -30.0000000    1.0000000 -10.0000000\n## 9              maxobs  17.2211304   17.2211304  17.2211304\n## 10                 ct   0.1722113    0.1722113   0.1722113\n## 11            censopt   0.0000000    0.0000000   0.0000000\nCheck that the objective calculator works, at least with the default values in the feasible parameter space:\nscore <- getScore(objective, p)\nprint(score)\n## $scores\n## Log-likelihood \n##      -463759.9 \n## \n## $sysconfig\n##                  Name         Min          Max       Value\n## 1  subarea.Subarea.x1   1.0000000 3000.0000000 542.1981111\n## 2  subarea.Subarea.x2 -30.0000000   30.0000000  -0.4127542\n## 3  subarea.Subarea.x3   1.0000000 1000.0000000   7.7403390\n## 4  subarea.Subarea.x4   1.0000000  240.0000000   1.2388548\n## 5                   b -30.0000000    0.0000000  -7.0000000\n## 6                   m   0.0000000    0.0000000   0.0000000\n## 7                   s   1.0000000 1000.0000000 100.0000000\n## 8                   a -30.0000000    1.0000000 -10.0000000\n## 9              maxobs  17.2211304   17.2211304  17.2211304\n## 10                 ct   0.1722113    0.1722113   0.1722113\n## 11            censopt   0.0000000    0.0000000   0.0000000\nmodRunoff <- getRecorded(ms, runoffDepthVarname)\njoki::plotTwoSeries(flow, modRunoff, ylab=\"obs/mod runoff\", startTime = start(flow), endTime = end(flow))\n\nBuild the optimiser definition, instrument with a logger.\n# term <- getMaxRuntimeTermination(maxHours = 0.3/60)  # ~20 second appears enough with SWIFT binaries in Release mode\n# term <- getMarginalTermination(tolerance = 1e-06, cutoffNoImprovement = 10, maxHours = 0.3/60) \nterm <- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.05','0.0167'))\n\nsceParams <- getDefaultSceParameters()\nurs <- createParameterSampler(0, p, 'urs')\noptimizer <- createSceOptimSwift(objective, term, SCEpars=sceParams, urs)\ncalibLogger <- setCalibrationLogger(optimizer, '')\nstartTime <- lubridate::now()\ncalibResults <- executeOptimization(optimizer)\nendTime <- lubridate::now()\ncalibWallTime <- endTime-startTime\nprint(paste( 'Optimization completed in ', calibWallTime, attr(calibWallTime, 'units')))\n## [1] \"Optimization completed in  50.4119968414307 secs\"\nd <- getLoggerContent(optimizer)\nd$PointNumber = 1:nrow(d)\nlogMh <- mhplot::mkOptimLog(d, fitness = 'Log.likelihood', messages = \"Message\", categories = \"Category\") \ngeomOps <- mhplot::subsetByMessage(logMh)\nstr(geomOps@data)\n## 'data.frame':    2575 obs. of  16 variables:\n##  $ Category          : Factor w/ 7 levels \"Complex No 0\",..: 7 7 7 7 7 7 7 7 7 7 ...\n##  $ CurrentShuffle    : Factor w/ 37 levels \"\",\"0\",\"1\",\"10\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ Message           : Factor w/ 6 levels \"Adding a random point in hypercube\",..: 4 4 4 4 4 4 4 4 4 4 ...\n##  $ Log.likelihood    : num  -1.00e+20 -1.00e+20 -1.00e+20 -7.71e+05 -1.00e+20 ...\n##  $ a                 : num  -17.63 -26.11 -3.56 -28.56 -14.85 ...\n##  $ b                 : num  -24.79 -4.05 -5.19 -4.51 -7.93 ...\n##  $ censopt           : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ ct                : num  0.172 0.172 0.172 0.172 0.172 ...\n##  $ m                 : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ maxobs            : num  17.2 17.2 17.2 17.2 17.2 ...\n##  $ s                 : num  710 977 784 196 354 ...\n##  $ subarea.Subarea.x1: num  2843 227 1651 2993 224 ...\n##  $ subarea.Subarea.x2: num  23 13.5 -13.6 26.2 -26.1 ...\n##  $ subarea.Subarea.x3: num  62.2 611.4 425.9 246.5 658.5 ...\n##  $ subarea.Subarea.x4: num  161.9 223.4 120 190.2 65.7 ...\n##  $ PointNumber       : int  1 2 3 4 5 6 7 8 9 10 ...\npVarIds <- (parameterizerAsDataFrame(p))$Name\nfor (pVar in pVarIds) {\n    print(mhplot::plotParamEvolution(geomOps, pVar))\n}\n\nFinally, get a visual of the runoff time series with the best known parameter set (the penultimate entry in the data frame with the log of the calibration process).\nsortedResults <- sortByScore(calibResults, 'Log-likelihood')\nhead(scoresAsDataFrame(sortedResults))\n##   Log.likelihood subarea.Subarea.x1 subarea.Subarea.x2 subarea.Subarea.x3\n## 1       2924.671           266.1925          -29.60785           773.6771\n## 2       2923.107           191.3387          -29.28017           824.4254\n## 3       2920.818           176.7221          -27.53492           866.5888\n## 4       2911.518           177.2337          -28.48846           789.2143\n## 5       2911.046           280.0083          -28.70505           771.3981\n## 6       2902.444           231.8093          -26.57367           809.7063\n##   subarea.Subarea.x4         b m        s         a   maxobs        ct\n## 1           1.033368 -1.850320 0 1.099840 -4.939078 17.22113 0.1722113\n## 2           1.147338 -2.099422 0 1.242173 -5.096023 17.22113 0.1722113\n## 3           1.032679 -2.056026 0 1.189237 -5.143367 17.22113 0.1722113\n## 4           1.165675 -2.045539 0 1.146694 -4.928836 17.22113 0.1722113\n## 5           1.082458 -2.118719 0 1.143031 -4.991464 17.22113 0.1722113\n## 6           1.035202 -2.024382 0 1.165068 -5.007636 17.22113 0.1722113\n##   censopt\n## 1       0\n## 2       0\n## 3       0\n## 4       0\n## 5       0\n## 6       0\nbestPset <- getScoreAtIndex(sortedResults, 1)\napplySysConfig(bestPset, ms)\nexecSimulation(ms)\nmodRunoff <- getRecorded(ms, runoffDepthVarname)\njoki::plotTwoSeries(flow, modRunoff, ylab=\"obs/mod runoff\", startTime = start(flow), endTime = end(flow))"
  },
  {
    "objectID": "doc/samples/R/vignettes/ensemble_model_runs/ensemble_model_runs.html",
    "href": "doc/samples/R/vignettes/ensemble_model_runs/ensemble_model_runs.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Ensemble SWIFT model runs\nJean-Michel Perraud 2020-01-28\n\n\nEnsemble SWIFT model runs\n\n\nAbout this document\nThis document was generated from an R markdown file on 2020-01-28 10:52:36.\n\n\nElaboration\nlibrary(swift)\nhas_data <- swift::hasSampleData()\nLet’s create a test catchment with a few subareas\nrunoffModel='GR4J'\n\nnodeIds=paste0('n', 1:6)\nlinkIds = paste0('lnk', 1:5)\ndefn <- list(\n    nodeIds=nodeIds,\n    nodeNames = paste0(nodeIds, '_name'),\n    linkIds=linkIds,\n    linkNames = paste0(linkIds, '_name'),\n    fromNode = paste0('n', c(2,5,4,3,1)),\n    toNode = paste0('n', c(6,2,2,4,4)),\n    areasKm2 = c(1.2, 2.3, 4.4, 2.2, 1.5),\n    runoffModel = runoffModel\n)\nsimulation <- createCatchment(defn$nodeIds, defn$nodeNames, defn$linkIds, defn$linkNames, defn$fromNode, defn$toNode, defn$runoffModel, defn$areasKm2)\nthe package uchronia includes facilities to access time series from a “library”, akin to what you would do to manage books.\ndataLibrary <- uchronia::sampleTimeSeriesLibrary('upper murray')\ndataIds <- uchronia::GetEnsembleDatasetDataIdentifiers_R(dataLibrary)\nprint(uchronia::GetEnsembleDatasetDataSummaries_R(dataLibrary))\n## [1] \"variable name: pet_der, identifier: 1, start: 1989-12-31T00:00:00, end: 2012-12-30T00:00:00, time length: 8401, time step: daily\"                            \n## [2] \"variable name: pet_der, identifier: 1, start: 1988-12-31T00:00:00, end: 2012-12-30T00:00:00, time length: 8766, time step: daily\"                            \n## [3] \"variable name: rain_der, identifier: 1, start: 1989-12-31T13:00:00, end: 2012-10-31T12:00:00, time length: 200160, time step: hourly\"                        \n## [4] \"variable name: rain_fcast_ens, identifier: 1, index: 0, start: 2010-08-01T21:00:00, end: 2010-08-06T21:00:00, time length: 5, time step: <not yet supported>\"\nThe sample catchment structure is obviously not the real “Upper Murray”. For the sake of a didactic example, let’s set the same inputs across all the subareas.\nprecipIds <- paste( 'subarea', getSubareaIds(simulation), 'P', sep='.')\nevapIds <- paste( 'subarea', getSubareaIds(simulation), 'E', sep='.')\nplayInputs(simulation, dataLibrary, precipIds, rep('rain_obs', length(precipIds)))\nplayInputs(simulation, dataLibrary, evapIds, rep('pet_obs', length(evapIds)), 'daily_to_hourly')\n## Warning in playInputs(simulation, dataLibrary, evapIds, rep(\"pet_obs\",\n## length(evapIds)), : Reusing argument `resample` to match the length of\n## `modelVarId`\n# And the flow rate we will record\noutflowId <- 'Catchment.StreamflowRate'\nGiven the information from the input data, let’s define a suitable simulation time span. NOTE and TODO: hourly information may not have been shown above yet.\ns <- joki::asPOSIXct('2007-01-01')\ne <- joki::asPOSIXct('2010-08-01 20')\nsHot <- joki::asPOSIXct('2010-08-01 21')\neHot <- joki::asPOSIXct('2010-08-05 21')\nFirst, before demonstrating ensemble forecasting simulations, let’s demonstrate how we can get a snapshot of the model states at a point in time and restore it later on, hot-starting further simulation.\nsetSimulationSpan(simulation, start=s, end=eHot)\nrecordState(simulation, outflowId)\nexecSimulation(simulation)\nbaseline <- getRecorded(simulation, outflowId)\nintv <- joki::makeTextTimeInterval(sHot,eHot)\nbaseline <- baseline[intv]\n\nsetSimulationSpan(simulation, start=s, end=e)\nexecSimulation(simulation)\nsnapshot <- snapshotState(simulation)\nWe can execute a simulation over the new time span, but requesting model states to NOT be reset. If we compare with a simulation where, as per default, the states are reset before the first time step, we notice a difference:\nsetSimulationSpan(simulation, start=sHot, end=eHot)\nexecSimulation(simulation, resetInitialStates = FALSE)\nnoReset <- getRecorded(simulation, outflowId)\nexecSimulation(simulation, resetInitialStates = TRUE)\nwithReset <- getRecorded(simulation, outflowId)\nx <- merge(noReset,withReset)\nzoo::plot.zoo(x, plot.type='single', col=c('blue','red'), ylab=\"Outflow m3/s\", main=\"Outflows with/without state resets\")\n\nNow let’d ready the simulation to do ensemble forecasts. We define a list inputMap such that keys are the names of ensemble forecast time series found in dataLibrary and the values is one or more of the model properties found in the simulation. In this instance we use the same series for all model precipitation inputs in precipIds\ninputMap <- list(rain_fcast_ens=precipIds)\nresetModelStates(simulation)\nsetStates(simulation, snapshot)\nems <- createEnsembleForecastSimulation(simulation, dataLibrary, start=sHot, end=eHot, inputMap=inputMap, leadTime=as.integer(24*2 + 23), ensembleSize=100, nTimeStepsBetweenForecasts=24)\nGetSimulationSpan_Pkg_R(ems)\n## $Start\n## [1] \"2010-08-01 21:00:00 UTC\"\n## \n## $End\n## [1] \"2010-08-04 21:00:00 UTC\"\n## \n## $TimeStep\n## [1] \"hourly\"\nrecordState(ems, outflowId)\nexecSimulation(ems)\nforecasts <- getRecordedEnsembleForecast(ems, outflowId)\nstrSwiftRef(forecasts)\n## ensemble forecast time series:\n##  2010-08-01 21:00:00 UTC\n##  time step 86400S\n##  size 4\nflowFc <- uchronia::getItem(forecasts, 1)\nuchronia::plotXtsQuantiles(flowFc)"
  },
  {
    "objectID": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html",
    "href": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Linear Muskingum channel routing model - constrained subcatchment calibration ================ Jean-Michel Perraud 2020-01-28"
  },
  {
    "objectID": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#setting-up-calibration",
    "href": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#setting-up-calibration",
    "title": "Streamflow Forecasting Onboarding",
    "section": "Setting up calibration",
    "text": "Setting up calibration\npSpecMaxBounds <- data.frame(\n  Name =  c('X',     'Alpha'),\n  Value = c(1.0E-6, akbounds['alpha_for_max_x']), # IMPORTANT to use these values.\n  Min=    c(1.0E-6, akbounds['min_alpha']),   \n  Max =   c(akbounds['max_x'], 1e6), # Alpha_max can get very large. \n  stringsAsFactors=FALSE\n)\n\npp(pzc(subsim, pSpecMaxBounds))\n##    Name        Min          Max     Value\n## 1     X 0.00000100 3.738204e-01 0.0000010\n## 2 Alpha 0.08143331 4.861449e+04 0.1300477\nIf we were to use another (X, Alpha) point e.g. X=0.1869102, the feasible bounds for Alpha change drastically. If an optimizer samples this for an initial population of points (SCE), this is unnecessarily restrictive for Alpha. Many hydrological calibration schemes were designed without consideration on feasible space that are not hypercubes.\npp(pzc(subsim, pSpecMusk))\n##    Name       Min       Max     Value\n## 1     X 0.0000010 0.3738204 0.1869102\n## 2 Alpha 0.1001528 0.2600954 0.1300477\nWhile calibrating in the (Alpha,X) space is possible, perhaps preferable in some cases, (1/Alpha,X) has a triangular shaped feasibility region that may be easier to handle for optimizers that work with geometric transformation in the parameter space (SCE). Swift can add this on top of the constrained calibration:\n# (X, 1/Alpha) parametrizer with dynamically constrained min/max bounds.\npzer_inv <- function(simulation, pSpecs=pSpecMusk) {\n  p_musk_c <- pzc(simulation, pSpecs)\n  p_musk_inv_a <- wrapTransform(p_musk_c)\n  addTransform(p_musk_inv_a, 'inv_alpha', 'Alpha', '1/x')\n  p_musk_inv_a\n}\n\np <- pzer_inv(subsim, pSpecMaxBounds)\npp(p)\n##        Name       Min        Max    Value\n## 1 inv_alpha 2.057e-05 12.2799877 7.689486\n## 2         X 1.000e-06  0.3738204 0.000001\nWe check that backtransforming to (Alpha-X) works:\npp(backtransform(p))\n##    Name        Min          Max     Value\n## 1     X 0.00000100 3.738204e-01 0.0000010\n## 2 Alpha 0.08143331 4.861449e+04 0.1300477\nobjectiveId <- 'NSE'\nobjective <- createObjective(subsim, varId, observation=someFlow, objectiveId, start(someFlow), end(someFlow))\nscore <- getScore(objective,p)  \nscore\n## $scores\n##       NSE \n## 0.9997748 \n## \n## $sysconfig\n##        Name       Min        Max    Value\n## 1 inv_alpha 2.057e-05 12.2799877 7.689486\n## 2         X 1.000e-06  0.3738204 0.000001\n#termination <- swift::CreateSceMaxRuntimeTerminationWila_R(1/60)\ntermination <- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.001','0.0167'))\nsceParams <- getDefaultSceParameters()\nnpars <- 2\nsceParams <- SCEParameters(npars)\noptimizer <- createSceOptimSwift(objective,terminationCriterion = termination, populationInitializer = p,SCEpars = sceParams)\ncalibLogger <- setCalibrationLogger(optimizer,\"dummy\")\n\noptimStartTime <- lubridate::now();\ncalibResults <- executeOptimization(optimizer)\noptimEndTime <- lubridate::now();\noptimWallClock <- lubridate::as.duration(lubridate::interval(optimStartTime, optimEndTime))\n\noptimWallClock\n## [1] \"0.209973812103271s\"\noptLog <- extractOptimizationLog(optimizer, fitnessName = \"NSE\")\ngeomOps <- optLog$geomOps \n\nshuffleLogs <- mhplot::subsetByCategory(optLog$data, pattern = \"Initial.*|Shuffling.*\") \nmhplot::plotShuffles(shuffleLogs, 'X', 'inv_alpha', objLims = (0:1))\n\nsortedResults <- sortByScore(calibResults, 'NSE')\nhead(scoresAsDataFrame(sortedResults))\n##   NSE inv_alpha         X\n## 1   1  7.688320 0.1871205\n## 2   1  7.687345 0.1867482\n## 3   1  7.686741 0.1869934\n## 4   1  7.693171 0.1868937\n## 5   1  7.693687 0.1868359\n## 6   1  7.693877 0.1869150\nq <- getBestScore(calibResults, 'NSE', FALSE)\nq <- GetSystemConfigurationWila_R(q)\npp(q)\n##        Name      Min       Max     Value\n## 1 inv_alpha 3.849069 9.9821603 7.6883204\n## 2         X 0.000001 0.3737638 0.1871205\npp(backtransform(q))\n##    Name       Min       Max     Value\n## 1     X 0.0000010 0.3737638 0.1871205\n## 2 Alpha 0.1001787 0.2598031 0.1300674"
  },
  {
    "objectID": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#seeding-the-optimisation-point-population-with-restrictive-constraint-bounds",
    "href": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#seeding-the-optimisation-point-population-with-restrictive-constraint-bounds",
    "title": "Streamflow Forecasting Onboarding",
    "section": "Seeding the optimisation point population with restrictive constraint bounds",
    "text": "Seeding the optimisation point population with restrictive constraint bounds\nThis section is a counter-example. Do not do this.\nSay, instead of seeding with alpha set to alpha_for_x_max (0.37382040) we instead use a value cloase to its global minimum, 0.083:\npSpecRestrictiveBounds <- pSpecMaxBounds\npSpecRestrictiveBounds$Value[2] <- 0.083\npSpecRestrictiveBounds\n##                  Name   Value        Min          Max\n##                     X 1.0e-06 0.00000100 3.738204e-01\n## alpha_for_max_x Alpha 8.3e-02 0.08143322 1.000000e+06\np <- pzer_inv(subsim, pSpecRestrictiveBounds)\npp(p)\n##        Name       Min         Max     Value\n## 1 inv_alpha 2.057e-05 12.27998772 12.048193\n## 2         X 1.000e-06  0.01887681  0.000001\nX is now much more constrained in its feasible range, and initializing a population fails to cover large sections of the feasible triangle. If used in the optimizer (uniform random sampling)\ntermination <- swift::CreateSceTerminationWila_Pkg_R('relative standard deviation', c('0.001','0.0167'))\nsceParams <- getDefaultSceParameters()\nnpars <- 2\nsceParams <- SCEParameters(npars)\noptimizer <- createSceOptimSwift(objective,terminationCriterion = termination, populationInitializer = p,SCEpars = sceParams)\ncalibLogger <- setCalibrationLogger(optimizer,\"dummy\")\ncalibResults <- executeOptimization(optimizer)\nlibrary(mhplot)\noptLog <- extractOptimizationLog(optimizer, fitnessName = \"NSE\")\ngeomOps <- optLog$geomOps \n\nshuffleLogs <- mhplot::subsetByCategory(optLog$data, pattern = \"Initial.*|Shuffling.*\") \nmhplot::plotShuffles(shuffleLogs, 'X', 'inv_alpha', objLims = (0:1))\n SCE does manage to converge towards the optimum, but it takes a larger number of iterations. Anecdotally, we observed cases where the calibration does fail to go near the optimum, when interplaying with a convergence criterion configured for “leniency”."
  },
  {
    "objectID": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#provenance",
    "href": "doc/samples/R/vignettes/muskingum_multilink_calibration/muskingum_multilink_calibration.html#provenance",
    "title": "Streamflow Forecasting Onboarding",
    "section": "Provenance",
    "text": "Provenance\nThis document was generated from an R markdown file on 2020-01-28 10:56:27\nsessionInfo()\n## R version 3.6.1 (2019-07-05)\n## Platform: x86_64-w64-mingw32/x64 (64-bit)\n## Running under: Windows 10 x64 (build 17763)\n## \n## Matrix products: default\n## \n## locale:\n## [1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252   \n## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C                      \n## [5] LC_TIME=English_Australia.1252    \n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] mhplot_0.5-1     stringr_1.4.0    readr_1.3.1      dplyr_0.8.3     \n##  [5] tidyr_1.0.0      ggplot2_3.2.1    lubridate_1.7.4  DiagrammeR_1.0.1\n##  [9] swift_2.1.2      Rcpp_1.0.2       knitr_1.25       rmarkdown_1.16  \n## \n## loaded via a namespace (and not attached):\n##  [1] mvtnorm_1.0-11     lattice_0.20-38    visNetwork_2.0.8  \n##  [4] msvs_0.3.1         zoo_1.8-6          utf8_1.1.4        \n##  [7] assertthat_0.2.1   zeallot_0.1.0      digest_0.6.21     \n## [10] R6_2.4.0           backports_1.1.4    evaluate_0.14     \n## [13] pillar_1.4.2       rlang_0.4.0        lazyeval_0.2.2    \n## [16] rstudioapi_0.10    labeling_0.3       webshot_0.5.1     \n## [19] downloader_0.4     htmlwidgets_1.3    igraph_1.2.4.1    \n## [22] munsell_0.5.0      compiler_3.6.1     influenceR_0.1.0  \n## [25] rgexf_0.15.3       xfun_0.9           pkgconfig_2.0.2   \n## [28] htmltools_0.3.6    tidyselect_0.2.5   tibble_2.1.3      \n## [31] gridExtra_2.3      XML_3.98-1.20      fansi_0.4.0       \n## [34] joki_0.4.0         viridisLite_0.3.0  crayon_1.3.4      \n## [37] withr_2.1.2        grid_3.6.1         jsonlite_1.6      \n## [40] gtable_0.3.0       lifecycle_0.1.0    magrittr_1.5      \n## [43] scales_1.0.0       cli_1.1.0          stringi_1.4.3     \n## [46] viridis_0.5.1      calibragem_0.8-0   ellipsis_0.3.0    \n## [49] brew_1.0-6         xts_0.11-2         vctrs_0.2.0       \n## [52] RColorBrewer_1.1-2 tools_3.6.1        glue_1.3.1        \n## [55] purrr_0.3.2        hms_0.5.1          Rook_1.1-1        \n## [58] yaml_2.2.0         colorspace_1.4-1   cinterop_0.3.0    \n## [61] uchronia_2.1.2"
  },
  {
    "objectID": "doc/samples/sample_workflows_r.html",
    "href": "doc/samples/sample_workflows_r.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Sample modelling workflows\nThe following documents are extracted from the vignettes in the R package swift\n\nGetting started with the swift R package\nEnsemble SWIFT model runs\nEnsemble forecast model runs\nCalibrating tied meta parameters\nMultisite multiobjective calibration\nCalibration of subcatchments defined by multiple gauges in a catchment\nCalibration with initial model memory states as parameters\n\nError correction models - Elaboration\nLinear Muskingum channel routing model - constrained subcatchment calibration"
  },
  {
    "objectID": "doc/roadmap.html",
    "href": "doc/roadmap.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "As of Jan 2019 the python package pyswift uses CFFI for interop. uchronia started down the same path but while trying to get things at parity with the R packages, we looked at pybind11 which is a serious candidate for our python/c++/c interop needs. Even more so since we are looking at xtensor for multidimensional data handling and xtensor-python also uses pybind11. pybind11 could play a role very similar to Rcpp and indeed is a close conceptual equivalent.\n\nq1 2019 Set up a Testbed with a uchronia_p11 package"
  },
  {
    "objectID": "doc/installation/install_macosx.html",
    "href": "doc/installation/install_macosx.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "Installing ensemble streamflow forecasting tools on Linux\nPlaceholder - As of Jan 2023, Homebrew binary packages can be built and installed, but this remains experimental."
  },
  {
    "objectID": "doc/installation/install_linux.html",
    "href": "doc/installation/install_linux.html",
    "title": "Installation: Debian-based Linux",
    "section": "",
    "text": "As of Jan 2020, Debian binary packages can be built and installed.\nsudo dpkg -i libmoirai_1.0-1_amd64.deb\nsudo dpkg -i libuchronia_2.3-7_amd64.deb\nsudo dpkg -i libswift_2.3-7_amd64.deb\nsudo dpkg -i libqppcore_2.3-7_amd64.deb\nsudo dpkg -i libqpp_2.3-7_amd64.deb\nlibboost-threadpool-dev_0.2-6_amd64.deb\nlibcinterop-dev_1.1-1_amd64.deb\nlibwila-dev_0.7-2_amd64.deb\nlibmoirai-dev_1.0-1_amd64.deb\nlibqppcore-dev_2.3-7_amd64.deb\nlibqpp-dev_2.3-7_amd64.deb\nlibsfsl-dev_2.3-1_amd64.deb\nlibswift-dev_2.3-7_amd64.deb\nlibuchronia-dev_2.3-7_amd64.deb\nlibmoirai-dbgsym_1.0-1_amd64.deb\nlibqppcore-dbgsym_2.3-7_amd64.deb\nlibqpp-dbgsym_2.3-7_amd64.deb\nlibuchronia-dbgsym_2.3-7_amd64.deb\nlibswift-dbgsym_2.3-7_amd64.deb"
  },
  {
    "objectID": "doc/installation/install_docker.html",
    "href": "doc/installation/install_docker.html",
    "title": "Installation: Docker",
    "section": "",
    "text": "Placeholder: a docker container with python bindings and Jupyter."
  },
  {
    "objectID": "doc/installation/install_windows.html",
    "href": "doc/installation/install_windows.html",
    "title": "Streamflow Forecasting Onboarding",
    "section": "",
    "text": "You will have been given one or more links to the location of C++ compiled libraries (windows DLLs), and R and/or Python packages."
  },
  {
    "objectID": "doc/installation/install_windows.html#adding-environment-variable-library_path",
    "href": "doc/installation/install_windows.html#adding-environment-variable-library_path",
    "title": "Streamflow Forecasting Onboarding",
    "section": "Adding environment variable LIBRARY_PATH",
    "text": "Adding environment variable LIBRARY_PATH\nThe packages in R, python, (etc.) need a way to search for these native libraries. To do so, you should set up a User- or System-level environmental variable LIBRARY_PATH. The approach borrows from what is customary (albeit variable) on most Linux system for dynamic library resolution.\nGo to the Control Panel, System and Security, Search for the string “environment”, and it should give you the link to Edit environment variables for your account.\n\nfor the field “Variable name:” use LIBRARY_PATH\nfor the field “Variable value:” use c:(if you unzipped libs.7z into c:in the previous section)"
  },
  {
    "objectID": "doc/installation/install_windows.html#r-packages",
    "href": "doc/installation/install_windows.html#r-packages",
    "title": "Streamflow Forecasting Onboarding",
    "section": "R packages",
    "text": "R packages\nAt the time of writing, instructions should work for R versions 4.2. It should be possible to install in ulterior versions but changes may be required. On Windows binary packages are version-specific, so by default packages for older versions may not be available, but could be built upon request.\nMost users will not (and should not) write to the default R library. You may already have installed additional packages, in which case R would have proposed to create a personal library instead. You can check this with: .libPaths()\n[1] \"C:/Users/xxxyyy/AppData/Local/R/win-library/4.2\"\n[2] \"C:/Program Files/R/R-4.2.1/library\"             \nAn alternate possibility is to specify a custom library directory via the R_LIBS environment variable.\n\nAdding an R_LIBS environment variable\nThis step is not compulsory.\nWhile not required, you can set up an additional R library location, specified via an environment variable R_LIBS at the machine or user level. This can facilitate access to R packages for all users and upgrades to newer version of R down the track. you can install the packages in the other library folders that R creates (user-specific if you do not have admin rights)\n\n\nInstalling an R package\nYou should have been given pre-compiled R packages. If you have for instance a USB drive mapped to E:, the following command should install swift, and all its depencency packages. This may take some time to set up once, but is worth it - most of these packages are publicly, popular R packages providing powerful statistical and visualisation tools to complement SWIFT2.\nswift is the main modelling package, uchronia is a closely related package for multi-dimensional time series handling.\ninstall.packages(c('uchronia','swift'), repos=c('file:///E:/software/R_pkgs', 'https://cran.csiro.au'), type='win.binary')\nOther related packages but less mature or less often used are available:\ninstall.packages(c('rpp','calibragem'), repos=c('file:///E:/software/R_pkgs', 'https://cran.csiro.au'), type='win.binary')"
  },
  {
    "objectID": "doc/installation/install_windows.html#python-packages",
    "href": "doc/installation/install_windows.html#python-packages",
    "title": "Streamflow Forecasting Onboarding",
    "section": "Python packages",
    "text": "Python packages\n\nPython environments\nWe strongly advise you to use python virtual environments to execute these tools via python.\nThere are various methods for these, but typically venv (virtual environments) or conda\nThis section documents the process using conda.\nWe recommend you use miniconda3, but you may have already it installed on your Windows box by e.g. your IT department installation management software.\nThis document assumes you start from the base environment, for instance:\n(base) C:\\Users\\xxxyyy>conda env list\n# conda environments:\n#\nbase                  *  C:\\Users\\xxxyyy\\Miniconda3\nWe recommend you use the mamba package, a newer and faster drop-in replacement for conda. This is optional.\nconda install -c conda-forge mamba\n\n\nInstalling hydroforecast packages\nBelow remember to replace mamba by conda if you have not installed mamba.\nset env_name=\"hydrofc\"\nmamba create -n %env_name% -c conda-forge python=3.9 xarray cffi pandas numpy matplotlib ipykernel jsonpickle netcdf4 seaborn\nREM note to self seaborn used in swift2 sample notebooks, so somewhat optional.\nRegister the new conda environment as a “kernel” for jupyter notebooks\nconda activate %env_name%\npython -m ipykernel install --user --name %env_name% --display-name \"HFC\"\nFrom here on all commands are done from within this new conda environment\nYou may already have jupyter-lab installed in another conda environment. You may use it to run ‘hydrofc’ notebooks. If not, install jupyter-lab in this new environment with:\nmamba install -c conda-forge jupyterlab\nYou may need to install some additional conda packages depending on the notebooks you are using.\nWe can now install “our” packages. we can install from ‘wheels’, or from source code (for developers)\nTwo dependencies refcount and cinterop are on pypi, but should also be in the zip archive as wheels as well; prefer the latter.\nTo install “wheels”, which you may have gotten from a zip file:\ncd  C:\\tmp\\sf\n7z x python.7z\n\n:: Adapt the following to the versions you have.\npip install --force-reinstall --no-deps refcount-0.9.3-py2.py3-none-any.whl\npip install --force-reinstall --no-deps cinterop-0.9.0-py2.py3-none-any.whl\npip install --force-reinstall --no-deps uchronia-2.3.7-py2.py3-none-any.whl\npip install --force-reinstall --no-deps swift2-2.3.7-py2.py3-none-any.whl\npip install --force-reinstall --no-deps fogss-0.3-py2.py3-none-any.whl\n\n\nInstalling hydroforecast packages in dev mode\nTo install instead in development mode, for some or all of the 4 packages:\nset GITHUB_REPOS=c:\\src\nset CSIRO_BITBUCKET=c:\\src\ncd %GITHUB_REPOS%\\pyrefcount\npython setup.py develop\ncd %GITHUB_REPOS%\\c-interop\\bindings\\python\\cinterop\npython setup.py develop\n\ncd %CSIRO_BITBUCKET%\\datatypes\\bindings\\python\\uchronia\\\npython setup.py develop\ncd %CSIRO_BITBUCKET%\\swift\\bindings\\python\\swift2\\\npython setup.py develop\n:: fogss...\nNow to exploresample notebooks\nmamba install -c conda-forge seaborn\ncd %userprofile%\\Documents\nmkdir notebooks\nxcopy %CSIRO_BITBUCKET%\\swift\\bindings\\python\\swift2\\notebooks\\* .\\\n\njupyter-lab .\nstart with getting_started.ipynb\nTODO: recommend using nbstripout and jupytext"
  },
  {
    "objectID": "doc/installation/install_windows.html#matlab-functions",
    "href": "doc/installation/install_windows.html#matlab-functions",
    "title": "Streamflow Forecasting Onboarding",
    "section": "Matlab functions",
    "text": "Matlab functions\nThis section is a placeholder"
  }
]